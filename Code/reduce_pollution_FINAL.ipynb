{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194a44ce-a5bb-4e81-bb58-0c584f2160cd",
   "metadata": {},
   "source": [
    "# Light pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bbb478-20f6-4c39-99c4-759e8dd52909",
   "metadata": {},
   "source": [
    "For the raw light pollution files, data were downloaded from [here](https://doi.pangaea.de/10.1594/PANGAEA.969081) as 11 regional netcdf files per each month in 2019 (corresponding to 132 netcdf files total). The files for each month were merged together with `xarray` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb0c86-9e62-407d-8cc5-ad880c92a59c",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a41e33-2872-4190-b6fc-d8c0582f450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "# Open all ALAN light pollution data for 2019 as an xarray dataset\n",
    "combined = xr.open_mfdataset(\"../Data/ALAN/global_month_*.nc\")\n",
    "\n",
    "# Create a risk_level boolean based on what's considered a 'High' threat level (critical depth > 10)\n",
    "combined['risk_level'] = (combined['z_thresh'] >= 10) #.astype(int)\n",
    "\n",
    "# Drop the 'z_thresh' variable from the dataset\n",
    "combined = combined.drop_vars('z_thresh')\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428328bc-c20c-4fb1-9f20-a3c69b746689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Open EEZ shapefiles\n",
    "eez = gpd.read_file('../Data/World_EEZ_v12_20231025/eez_v12.shp')\n",
    "eez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e960fd4-467a-40bb-b7da-fb79d817f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "\n",
    "# Open the biodiversity priority areas based on Zhao et al. 2020 (https://www.sciencedirect.com/science/article/abs/pii/S0006320719312182?via%3Dihub)\n",
    "masked_data = rioxarray.open_rasterio('masked_top_30_percent_over_water.tif')\n",
    "\n",
    "# Set the CRS for masked_data if it's not already set\n",
    "if 'crs' not in masked_data.attrs:\n",
    "    masked_data.rio.write_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "# Iterate over each country's EEZ and calculate the area where light pollution exceeds the 'High' threat level for at least one month in 2019\n",
    "area_light_data = []\n",
    "for i, row in eez.iterrows():\n",
    "    try:\n",
    "        print(i)\n",
    "        country_name = row['TERRITORY1']\n",
    "        geom = row['geometry']\n",
    "        ISO_TER1 = row['ISO_TER1']\n",
    "    \n",
    "        # Rename dimensions if necessary\n",
    "        if 'x' not in combined.dims or 'y' not in combined.dims:\n",
    "            combined = combined.rename({'lon': 'x', 'lat': 'y'})\n",
    "            combined.rio.write_crs('EPSG:4326', inplace=True)\n",
    "        \n",
    "        # Mask light pollution data with the selected EEZ geometry\n",
    "        masked_light = combined.rio.clip([geom], drop=True)\n",
    "    \n",
    "        # Interpolate biodiversity priority areas to the same resolution as the light pollution data\n",
    "        masked_data_interp = masked_data.interp(\n",
    "            x=combined['x'],\n",
    "            y=combined['y'],\n",
    "            method='nearest'\n",
    "        )\n",
    "    \n",
    "        lat = masked_light['y'].values\n",
    "        lon = masked_light['x'].values\n",
    "        \n",
    "        # Calculate grid cell area\n",
    "        lat_rad = np.deg2rad(lat)\n",
    "        lon_rad = np.deg2rad(lon)\n",
    "        \n",
    "        R = 6371  # Earth radius in kilometers\n",
    "        dlat = np.gradient(lat_rad)\n",
    "        dlon = np.gradient(lon_rad)\n",
    "        \n",
    "        # Create a numpy array with approximate area calculation\n",
    "        cell_areas = (R**2 * np.outer(np.sin(dlat), dlon)) * np.cos(lat_rad[:, None])\n",
    "    \n",
    "        # Ensure that cell_areas corresponds to the valid area in masked_light\n",
    "        valid_cell_areas = np.where(masked_light.values, cell_areas, 0)\n",
    "    \n",
    "        # Compute the area impacted by 'High' threat light pollution\n",
    "        area_light = (masked_light.max(dim='time')['risk_level'] * valid_cell_areas).sum().values\n",
    "        \n",
    "        # Calculate total masked area within the selected EEZ\n",
    "        area_eez = valid_cell_areas.sum()\n",
    "    \n",
    "        # Create a mask for non-NaN values in masked_data_interp\n",
    "        valid_mask_data = masked_data_interp\n",
    "    \n",
    "        # Compute the total area where both light pollution is under 'High' threat and it's within a priority area for a given EEZ\n",
    "        area_biodiversity = ((masked_light.max(dim='time')['risk_level'] * valid_cell_areas) * valid_mask_data).sum().values\n",
    "        \n",
    "        # Store the result\n",
    "        area_light_data.append({\n",
    "            'Country': country_name,\n",
    "            'ISO_TER1': ISO_TER1,\n",
    "            'geometry': geom,\n",
    "            'Light_Area': row['AREA_KM2']*area_light/area_eez,  # Convert to a standard Python float\n",
    "            'EEZ_Area': row['AREA_KM2'],\n",
    "            'Biodiversity_Area': row['AREA_KM2']*area_biodiversity/area_eez\n",
    "        })\n",
    "        print(country_name,'Light_Area:', row['AREA_KM2']*area_light/area_eez,'EEZ_Area:', row['AREA_KM2'],'Biodiversity_Area:', row['AREA_KM2']*area_biodiversity/area_eez)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Convert the results to a GeoDataFrame for easy viewing\n",
    "area_light_gdf = gpd.GeoDataFrame(area_light_data, crs=eez.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3997c0-dba9-4356-8ef6-c2e4ad5fdeaf",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c4f69b-1afb-456d-b573-573619e91df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the GeoDataFrame to a CSV file\n",
    "area_light_gdf[['Country', 'ISO_TER1', 'Light_Area', 'EEZ_Area', 'Biodiversity_Area']].to_csv(\"../Data/pollution_3_light.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8e3482-b524-469a-aadc-6e2067e743db",
   "metadata": {},
   "source": [
    "# Plastic pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b960f68-0847-4003-9f36-6d74b4e5b551",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a201137-4fcf-4ddb-a6eb-450ce5513c8f",
   "metadata": {},
   "source": [
    "Data were obtained from OECD (2022) – processed by Our World in Data and can be found [here](https://ourworldindata.org/grapher/plastic-waste-accumulated-in-oceans)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ab387-28d0-423e-b5f9-f9673754b97b",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf8dd32-7f65-4ede-bf0f-54eda7d017ce",
   "metadata": {},
   "source": [
    "Data were obtained from OECD (2022) – processed by Our World in Data and can be found [here](https://ourworldindata.org/grapher/plastic-production-by-sector)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e818f10-a933-4a34-9a2f-df97cb87bed8",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a85bc-e4fe-49ef-b896-b5e9ac79e151",
   "metadata": {},
   "source": [
    "Data were obtained from Meijer et al. (2021). More than 1000 rivers account for 80% of global riverine plastic emissions into the ocean. Science Advances. – processed by Our World in Data and can be found [here](https://ourworldindata.org/grapher/plastic-waste-emitted-to-the-ocean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b622f38-39b3-4216-85b2-cbefed938621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Read plastic data and continents shapefile\n",
    "plastic_df = pd.read_csv(\"../Data/pollution_3_plastic.csv\")\n",
    "continents_df = gpd.read_file(\"../Data/continents/continents.json\")\n",
    "continents_df['Entity'] = ['Asia', 'Africa', 'Europe', 'North America', 'Oceania', 'South America']\n",
    "\n",
    "# Merge plastic data with continents\n",
    "merged_df = plastic_df.merge(continents_df[['Entity', 'geometry']], on='Entity')\n",
    "\n",
    "# Save the result as a GeoJSON\n",
    "merged_gdf = gpd.GeoDataFrame(merged_df, geometry='geometry')\n",
    "merged_gdf.to_file(\"../Data/pollution_3_plastic.geojson\", driver=\"GeoJSON\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95404576-13c7-49ad-902d-9015517817ef",
   "metadata": {},
   "source": [
    "# CO<sub>2</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d747872-1de0-4475-a794-da54549ec873",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15512f54-5111-4e53-86cc-bd259b34719d",
   "metadata": {},
   "source": [
    "Coming soon..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a3e6b0-c185-486b-9734-6a7f0edd4861",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ea37f-d94f-45f8-944c-46b62ff7fe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils functions and globals \n",
    "\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import box\n",
    "import rioxarray\n",
    "import re\n",
    "\n",
    "from rasterio.features import geometry_mask\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Open the biodiversity priority areas based on Zhao et al. 2020 (https://www.sciencedirect.com/science/article/abs/pii/S0006320719312182?via%3Dihub)\n",
    "masked_data = rioxarray.open_rasterio('masked_top_30_percent_over_water.tif')\n",
    "\n",
    "# Set the CRS for masked_data if it's not already set\n",
    "if 'crs' not in masked_data.attrs:\n",
    "    masked_data.rio.write_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "# Load SST dataset and EEZ shapefile\n",
    "seas_shapefile_path = '../Data/World_Seas_IHO_v3/World_Seas_IHO_v3.shp'\n",
    "SEAS_DF = gpd.read_file(seas_shapefile_path)\n",
    "\n",
    "# Calculate linear trend and p-value for each grid point\n",
    "def calculate_trend_and_significance(x):\n",
    "    if np.isnan(x).all():\n",
    "        return np.nan, np.nan, np.nan\n",
    "    else:\n",
    "        slope, intercept, _, p_value, _ = stats.linregress(range(len(x)), x)\n",
    "        return slope, intercept, p_value\n",
    "\n",
    "# Calculate the trend and significance of the trend at each pixel in an xarray dataset\n",
    "def calculate_trend_df(climate_df):\n",
    "    df_mean = climate_df.groupby('time.year').mean()\n",
    "    \n",
    "    # Apply the trend and p-value calculation to the entire dataset\n",
    "    results = xr.apply_ufunc(\n",
    "        calculate_trend_and_significance,\n",
    "        df_mean,\n",
    "        input_core_dims=[['year']],\n",
    "        vectorize=True,\n",
    "        output_core_dims=[[], [], []],\n",
    "        output_dtypes=[float, float, float]\n",
    "    )\n",
    "    \n",
    "    # Extract the trend and p-value into separate DataArrays\n",
    "    trends_da = xr.DataArray(results[0], coords=df_mean.isel(year=0).coords, name='trend')\n",
    "    pvalues_da = xr.DataArray(results[2], coords=df_mean.isel(year=0).coords, name='p_value')\n",
    "    \n",
    "    # Create a significance mask where p-value < 0.05\n",
    "    significant_da = xr.DataArray((pvalues_da < 0.05), coords=pvalues_da.coords, name='significant')\n",
    "    \n",
    "    # Combine trend, p-value, and significance mask into a single dataset\n",
    "    trend_significance_ds = xr.Dataset({\n",
    "        'trend': trends_da,\n",
    "        'p_value': pvalues_da,\n",
    "        'significant': significant_da\n",
    "    })\n",
    "    \n",
    "    # Set the CRS for the trends dataset to match the EEZ CRS\n",
    "    trend_significance_ds = trend_significance_ds.rio.write_crs(\"epsg:4326\")\n",
    "    return trend_significance_ds\n",
    "\n",
    "# Calculate area-weighted trend, significance for each sea/ocean area\n",
    "def area_trend(trend_significance_ds, SEAS_DF=SEAS_DF):\n",
    "    # Iterate over each sea/ocean area and calculate the area-weighted trend and significant area percentage\n",
    "    area_weighted_trends = []\n",
    "    \n",
    "    # Check if 'lat' and 'lon' are in the dataset, otherwise check for 'latitude' and 'longitude'\n",
    "    if 'lat' in trend_significance_ds.dims and 'lon' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'lat': 'y', 'lon': 'x'})\n",
    "    elif 'latitude' in trend_significance_ds.dims and 'longitude' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "\n",
    "\n",
    "    # Interpolate biodiversity priority areas to the same resolution as the climate data\n",
    "    masked_data_interp = masked_data.interp(\n",
    "        x=trend_significance_ds['x'],\n",
    "        y=trend_significance_ds['y'],\n",
    "        method='nearest'\n",
    "    )\n",
    "\n",
    "    # Calculate the area for each grid cell (assumes lat/lon grid)\n",
    "    lat = trend_significance_ds['y'].values\n",
    "    lon = trend_significance_ds['x'].values\n",
    "    \n",
    "    # Calculate grid cell area using Haversine formula or by approximation\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    \n",
    "    # Earth radius in kilometers\n",
    "    R = 6371\n",
    "    dlat = np.gradient(lat_rad)\n",
    "    dlon = np.gradient(lon_rad)\n",
    "    \n",
    "    # Approximate area calculation\n",
    "    cell_areas = (R**2 * np.outer(np.sin(dlat), dlon)) * np.cos(lat_rad[:, None])\n",
    "    \n",
    "    for i, row in SEAS_DF.iterrows():\n",
    "        try:\n",
    "            region_name = row['NAME']\n",
    "            area = row['area']\n",
    "            geom = row['geometry']\n",
    "    \n",
    "            # Mask SST trends with the sea geometry\n",
    "            masked_trends = trend_significance_ds['trend'].rio.clip([geom], drop=True)\n",
    "            masked_significance = trend_significance_ds['significant'].rio.clip([geom], drop=True)\n",
    "    \n",
    "            # Clip cell_areas to the same extent as masked_trends\n",
    "            cell_areas_clipped = xr.DataArray(\n",
    "                cell_areas, \n",
    "                dims=['y', 'x'], \n",
    "                coords={'y': trend_significance_ds['y'], 'x': trend_significance_ds['x']}\n",
    "            )\n",
    "            \n",
    "            # Set CRS for cell_areas_clipped to match the CRS of trend_significance_ds\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.write_crs('EPSG:4326')\n",
    "    \n",
    "            # Clip cell_areas to the same geometry\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.clip([geom], drop=True)\n",
    "        \n",
    "            # Compute the area-weighted trend\n",
    "            weighted_trend = (masked_trends * cell_areas_clipped).sum(dim=('y', 'x')) / cell_areas_clipped.sum()\n",
    "    \n",
    "            # Compute the total area that is significant\n",
    "            significant_masked_areas = (masked_significance * cell_areas_clipped).where(masked_significance, 0)\n",
    "            total_significant_area = significant_masked_areas.sum(dim=('y', 'x')).item()\n",
    "    \n",
    "            # Calculate the percentage of the area that is significant\n",
    "            total_area = cell_areas_clipped.sum()\n",
    "            significant_area_percent = (total_significant_area / total_area) * 100\n",
    "    \n",
    "            # Calculate the area for biodiversity based on the mask\n",
    "            area_biodiversity = ((masked_significance * cell_areas_clipped) * masked_data_interp).sum(dim=['x', 'y']).values\n",
    "    \n",
    "            # Store the result\n",
    "            area_weighted_trends.append({\n",
    "                'Region_Name': region_name,\n",
    "                'geometry': geom,\n",
    "                'Weighted_Trend': weighted_trend.item(),\n",
    "                'Sea_Area': total_area.item(),\n",
    "                'Significant_Area': total_significant_area,\n",
    "                'Significant_Area_Percent': significant_area_percent.item(),\n",
    "                'Biodiversity_Area': area_biodiversity[0]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Convert the results to a GeoDataFrame for easy viewing\n",
    "    area_weighted_trends_gdf = gpd.GeoDataFrame(area_weighted_trends, crs=SEAS_DF.crs)\n",
    "    return area_weighted_trends_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305da6a0-ca39-4c2e-b522-d6b1387c978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(co2_df['spco2'])\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/pollution_3_co2.geojson\",driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625568a3-08c9-4c5a-a88f-844f0508dced",
   "metadata": {},
   "source": [
    "# Nutrient Pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e48a732-1eda-420e-9cb6-a5b18dcbe266",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136034d-5109-4226-81ab-106767202fcb",
   "metadata": {},
   "source": [
    "Coming soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d4a70-1ef3-40cf-b95d-b1afce32654f",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daced229-0ae6-449b-97d2-fc47bd525b9e",
   "metadata": {},
   "source": [
    "Coming soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5912e1f-cc4e-439c-909d-63a52e6da6f8",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e4c31b-4fb3-47a7-83e1-f1fbc19b71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import box\n",
    "import rioxarray\n",
    "import re\n",
    "\n",
    "from rasterio.features import geometry_mask\n",
    "from scipy.stats import linregress\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Open the biodiversity priority areas based on Zhao et al. 2020 (https://www.sciencedirect.com/science/article/abs/pii/S0006320719312182?via%3Dihub)\n",
    "masked_data = rioxarray.open_rasterio('masked_top_30_percent_over_water.tif')\n",
    "\n",
    "# Set the CRS for masked_data if it's not already set\n",
    "if 'crs' not in masked_data.attrs:\n",
    "    masked_data.rio.write_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "# Load SST dataset and EEZ shapefile\n",
    "seas_shapefile_path = '../Data/World_Seas_IHO_v3/World_Seas_IHO_v3.shp'\n",
    "SEAS_DF = gpd.read_file(seas_shapefile_path)\n",
    "\n",
    "def area_dead_zone(o2_df, SEAS_DF=SEAS_DF):\n",
    "    area_deadzone = []\n",
    "\n",
    "    # Set CRS and rename dimensions and coordinates\n",
    "    o2_df = o2_df.rio.write_crs(\"epsg:4326\")\n",
    "    o2_df = o2_df.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "\n",
    "    # Interpolate biodiversity priority areas to the same resolution as the climate data\n",
    "    masked_data_interp = masked_data.interp(\n",
    "        x=o2_df['x'],\n",
    "        y=o2_df['y'],\n",
    "        method='nearest'\n",
    "    )\n",
    "\n",
    "    # Calculate the area for each grid cell (assumes lat/lon grid)\n",
    "    lat = o2_df['y'].values\n",
    "    lon = o2_df['x'].values\n",
    "    \n",
    "    # Calculate grid cell area using Haversine formula or by approximation\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    \n",
    "    # Earth radius in kilometers\n",
    "    R = 6371\n",
    "    dlat = np.gradient(lat_rad)\n",
    "    dlon = np.gradient(lon_rad)\n",
    "    \n",
    "    # Approximate area calculation\n",
    "    cell_areas = (R**2 * np.outer(np.sin(dlat), dlon)) * np.cos(lat_rad[:, None])\n",
    "    \n",
    "    # Use tqdm to track progress through SEAS_DF.iterrows()\n",
    "    for i, row in tqdm(SEAS_DF.iterrows(), total=len(SEAS_DF), desc=\"Processing Sea Areas\"):\n",
    "        try:\n",
    "            region_name = row['NAME']\n",
    "            geom = row['geometry']\n",
    "    \n",
    "            # Mask SST trends with the sea geometry\n",
    "            masked_df = o2_df.rio.clip([geom], drop=True)\n",
    "    \n",
    "            # Clip cell_areas to the same extent as masked_df\n",
    "            cell_areas_clipped = xr.DataArray(\n",
    "                cell_areas, \n",
    "                dims=['y', 'x'], \n",
    "                coords={'y': o2_df['y'], 'x': o2_df['x']}\n",
    "            )\n",
    "            \n",
    "            # Set CRS for cell_areas_clipped to match the CRS of trend_significance_ds\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.write_crs('EPSG:4326')\n",
    "    \n",
    "            # Clip cell_areas to the same geometry\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.clip([geom], drop=True)\n",
    "        \n",
    "            # Compute the total area that is impacted by dead zones\n",
    "            deadzone_area = (masked_df * cell_areas_clipped).sum(dim=('y', 'x')).compute()  # Compute to convert from Dask array\n",
    "    \n",
    "            # Calculate the area for biodiversity based on the mask\n",
    "            area_biodiversity = ((masked_df * cell_areas_clipped) * masked_data_interp).sum(dim=['x', 'y']).compute()\n",
    "\n",
    "            total_area = cell_areas_clipped.sum(dim=('y', 'x')).compute()  # Ensure computation\n",
    "    \n",
    "            # Extract values after computing\n",
    "            deadzone_area_value = deadzone_area.item() if deadzone_area.size == 1 else deadzone_area.values[0]\n",
    "            total_area_value = total_area.item() if total_area.size == 1 else total_area.values[0]\n",
    "            area_biodiversity = area_biodiversity.item() if area_biodiversity.size == 1 else area_biodiversity.values[0]\n",
    "    \n",
    "            # Store the result\n",
    "            area_deadzone.append({\n",
    "                'Region_Name': region_name,\n",
    "                'geometry': geom,\n",
    "                'Deadzone_Area': deadzone_area_value,\n",
    "                'Sea_Area': total_area_value,\n",
    "                'Biodiversity_Area': area_biodiversity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {region_name}: {e}\")\n",
    "\n",
    "    # Convert the results to a GeoDataFrame for easy viewing\n",
    "    area_deadzone_gdf = gpd.GeoDataFrame(area_deadzone, crs=SEAS_DF.crs)\n",
    "    return area_deadzone_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe5e77-b6b8-4392-ab8b-269573d7034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "o2_df = xr.open_mfdataset(\"../Data/dead_zone/*\")\n",
    "\n",
    "# Conversion factor from mmol O2/m³ to mg/L\n",
    "o2_conversion_factor = 32 / 1000\n",
    "\n",
    "# Convert oxygen concentration from mmol O2/m³ to mg/L\n",
    "o2_mg_per_l = o2_df['o2'] * o2_conversion_factor\n",
    "\n",
    "# Define a depth range for dead zone analysis (e.g., upper 200 meters)\n",
    "shallow_depth_mask = o2_df['depth'] <= 200\n",
    "\n",
    "# Apply the mask to limit analysis to shallow depths\n",
    "o2_mg_per_l_shallow = o2_mg_per_l.where(shallow_depth_mask, drop=True)\n",
    "\n",
    "# Create a mask where oxygen concentration is less than 2 mg/L at any shallow depth or time\n",
    "low_oxygen_mask_shallow = (o2_mg_per_l_shallow < 2).max(dim=['depth', 'time']).astype(int)\n",
    "\n",
    "area_df = area_dead_zone(low_oxygen_mask_shallow)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/pollution_3_nutrients.geojson\",driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac4270e-e612-4f32-b48a-faec5eeb8e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel",
   "language": "python",
   "name": "claymodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
