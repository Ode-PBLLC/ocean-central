{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a68d863-6d59-4533-a5a1-4f616f8e6808",
   "metadata": {},
   "source": [
    "# Mitigate Climate Change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe067026-4fd8-4760-b32a-46c11dfd2b63",
   "metadata": {},
   "source": [
    "This notebook outlines the general workflow for the data within the [Mitigate Climate Change](https://oceancentral.org/track/mitigate-climate-change) page of the Ocean Central website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c137a64-8efa-4766-a781-d5f48faa57d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Utils functions and globals used for making all figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeacb4e-82f3-4b15-9fec-3551c390abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import box\n",
    "import rioxarray\n",
    "import re\n",
    "\n",
    "from rasterio.features import geometry_mask\n",
    "from scipy.stats import linregress\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Open the biodiversity priority areas based on Zhao et al. 2020 (https://www.sciencedirect.com/science/article/abs/pii/S0006320719312182?via%3Dihub)\n",
    "masked_data = rioxarray.open_rasterio('../Data/masked_top_30_percent_over_water.tif')\n",
    "\n",
    "# Set the CRS for masked_data if it's not already set\n",
    "if 'crs' not in masked_data.attrs:\n",
    "    masked_data.rio.write_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "# Load SST dataset and EEZ shapefile\n",
    "seas_shapefile_path = '../Data/World_Seas_IHO_v3/World_Seas_IHO_v3.shp'\n",
    "SEAS_DF = gpd.read_file(seas_shapefile_path)\n",
    "\n",
    "# Calculate linear trend and p-value for each grid point\n",
    "def calculate_trend_and_significance(x):\n",
    "    if np.isnan(x).all():\n",
    "        return np.nan, np.nan, np.nan\n",
    "    else:\n",
    "        slope, intercept, _, p_value, _ = stats.linregress(range(len(x)), x)\n",
    "        return slope, intercept, p_value\n",
    "\n",
    "# Calculate the trend and significance of the trend at each pixel in an xarray dataset\n",
    "def calculate_trend_df(climate_df):\n",
    "    df_mean = climate_df.groupby('time.year').mean()\n",
    "    \n",
    "    # Apply the trend and p-value calculation to the entire dataset\n",
    "    results = xr.apply_ufunc(\n",
    "        calculate_trend_and_significance,\n",
    "        df_mean,\n",
    "        input_core_dims=[['year']],\n",
    "        vectorize=True,\n",
    "        output_core_dims=[[], [], []],\n",
    "        output_dtypes=[float, float, float]\n",
    "    )\n",
    "    \n",
    "    # Extract the trend and p-value into separate DataArrays\n",
    "    trends_da = xr.DataArray(results[0], coords=df_mean.isel(year=0).coords, name='trend')\n",
    "    pvalues_da = xr.DataArray(results[2], coords=df_mean.isel(year=0).coords, name='p_value')\n",
    "    \n",
    "    # Create a significance mask where p-value < 0.05\n",
    "    significant_da = xr.DataArray((pvalues_da < 0.05), coords=pvalues_da.coords, name='significant')\n",
    "    \n",
    "    # Combine trend, p-value, and significance mask into a single dataset\n",
    "    trend_significance_ds = xr.Dataset({\n",
    "        'trend': trends_da,\n",
    "        'p_value': pvalues_da,\n",
    "        'significant': significant_da\n",
    "    })\n",
    "    \n",
    "    # Set the CRS for the trends dataset to match the EEZ CRS\n",
    "    trend_significance_ds = trend_significance_ds.rio.write_crs(\"epsg:4326\")\n",
    "    return trend_significance_ds\n",
    "\n",
    "# Calculate area-weighted trend, significance for each sea/ocean area\n",
    "def area_trend(trend_significance_ds, SEAS_DF=SEAS_DF):\n",
    "    # Iterate over each sea/ocean area and calculate the area-weighted trend and significant area percentage\n",
    "    area_weighted_trends = []\n",
    "    \n",
    "    # Check if 'lat' and 'lon' are in the dataset, otherwise check for 'latitude' and 'longitude'\n",
    "    if 'lat' in trend_significance_ds.dims and 'lon' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'lat': 'y', 'lon': 'x'})\n",
    "    elif 'latitude' in trend_significance_ds.dims and 'longitude' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "\n",
    "    # Interpolate biodiversity priority areas to the same resolution as the climate data\n",
    "    masked_data_interp = masked_data.interp(\n",
    "        x=trend_significance_ds['x'],\n",
    "        y=trend_significance_ds['y'],\n",
    "        method='nearest'\n",
    "    )\n",
    "\n",
    "    # Calculate the area for each grid cell (assumes lat/lon grid)\n",
    "    lat = trend_significance_ds['y'].values\n",
    "    lon = trend_significance_ds['x'].values\n",
    "    \n",
    "    # Calculate grid cell area using Haversine formula or by approximation\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    \n",
    "    # Earth radius in kilometers\n",
    "    R = 6371\n",
    "    dlat = np.gradient(lat_rad)\n",
    "    dlon = np.gradient(lon_rad)\n",
    "    \n",
    "    # Approximate area calculation\n",
    "    cell_areas = (R**2 * np.outer(np.sin(dlat), dlon)) * np.cos(lat_rad[:, None])\n",
    "    \n",
    "    for i, row in SEAS_DF.iterrows():\n",
    "        try:\n",
    "            region_name = row['NAME']\n",
    "            area = row['area']\n",
    "            geom = row['geometry']\n",
    "    \n",
    "            # Mask SST trends with the sea geometry\n",
    "            masked_trends = trend_significance_ds['trend'].rio.clip([geom], drop=True)\n",
    "            masked_significance = trend_significance_ds['significant'].rio.clip([geom], drop=True)\n",
    "    \n",
    "            # Clip cell_areas to the same extent as masked_trends\n",
    "            cell_areas_clipped = xr.DataArray(\n",
    "                cell_areas, \n",
    "                dims=['y', 'x'], \n",
    "                coords={'y': trend_significance_ds['y'], 'x': trend_significance_ds['x']}\n",
    "            )\n",
    "            \n",
    "            # Set CRS for cell_areas_clipped to match the CRS of trend_significance_ds\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.write_crs('EPSG:4326')\n",
    "    \n",
    "            # Clip cell_areas to the same geometry\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.clip([geom], drop=True)\n",
    "        \n",
    "            # Compute the area-weighted trend\n",
    "            weighted_trend = (masked_trends * cell_areas_clipped).sum(dim=('y', 'x')) / cell_areas_clipped.sum()\n",
    "    \n",
    "            # Compute the total area that is significant\n",
    "            significant_masked_areas = (masked_significance * cell_areas_clipped).where(masked_significance, 0)\n",
    "            total_significant_area = significant_masked_areas.sum(dim=('y', 'x')).item()\n",
    "    \n",
    "            # Calculate the percentage of the area that is significant\n",
    "            total_area = cell_areas_clipped.sum()\n",
    "            significant_area_percent = (total_significant_area / total_area) * 100\n",
    "    \n",
    "            # Calculate the area for biodiversity based on the mask\n",
    "            area_biodiversity = ((masked_significance * cell_areas_clipped) * masked_data_interp).sum(dim=['x', 'y']).values\n",
    "    \n",
    "            # Store the result\n",
    "            area_weighted_trends.append({\n",
    "                'Region_Name': region_name,\n",
    "                'geometry': geom,\n",
    "                'Weighted_Trend': weighted_trend.item(),\n",
    "                'Sea_Area': area,\n",
    "                'Significant_Area': area*total_significant_area/total_area.item(),\n",
    "                'Significant_Area_Percent': 100*total_significant_area/total_area.item(),\n",
    "                'Biodiversity_Area': area*area_biodiversity[0]/total_area.item(),\n",
    "                'Biodiversity_Area_Percent': 100*area_biodiversity[0]/total_area.item(),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Convert the results to a GeoDataFrame for easy viewing\n",
    "    area_weighted_trends_gdf = gpd.GeoDataFrame(area_weighted_trends, crs=SEAS_DF.crs)\n",
    "    return area_weighted_trends_gdf\n",
    "\n",
    "def area_heatwave(temp_df, SEAS_DF=SEAS_DF):\n",
    "    area_heatwave = []\n",
    "\n",
    "    # Set CRS and rename dimensions and coordinates\n",
    "    temp_df = temp_df.rio.write_crs(\"epsg:4326\")\n",
    "    temp_df = temp_df.rename({'latdim': 'y', 'londim': 'x'}).rename({'lat': 'y', 'lon': 'x'})  # Adjust based on your dimensions\n",
    "\n",
    "    # Select heatwave categories >= 3 and aggregate over time\n",
    "    temp_df = (temp_df['heatwave_category'] >= 3).any(dim='time')\n",
    "\n",
    "    # Interpolate biodiversity priority areas to the same resolution as the climate data\n",
    "    masked_data_interp = masked_data.interp(\n",
    "        x=temp_df['x'],\n",
    "        y=temp_df['y'],\n",
    "        method='nearest'\n",
    "    )\n",
    "\n",
    "    # Calculate the area for each grid cell (assumes lat/lon grid)\n",
    "    lat = temp_df['y'].values\n",
    "    lon = temp_df['x'].values\n",
    "    \n",
    "    # Calculate grid cell area using Haversine formula or by approximation\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    \n",
    "    # Earth radius in kilometers\n",
    "    R = 6371\n",
    "    dlat = np.gradient(lat_rad)\n",
    "    dlon = np.gradient(lon_rad)\n",
    "    \n",
    "    # Approximate area calculation\n",
    "    cell_areas = (R**2 * np.outer(np.sin(dlat), dlon)) * np.cos(lat_rad[:, None])\n",
    "    \n",
    "    # Use tqdm to track progress through SEAS_DF.iterrows()\n",
    "    for i, row in tqdm(SEAS_DF.iterrows(), total=len(SEAS_DF), desc=\"Processing Sea Areas\"):\n",
    "        try:\n",
    "            region_name = row['NAME']\n",
    "            area = row['area']\n",
    "            geom = row['geometry']\n",
    "    \n",
    "            # Mask SST trends with the sea geometry\n",
    "            masked_df = temp_df.rio.clip([geom], drop=True)\n",
    "    \n",
    "            # Clip cell_areas to the same extent as masked_df\n",
    "            cell_areas_clipped = xr.DataArray(\n",
    "                cell_areas, \n",
    "                dims=['y', 'x'], \n",
    "                coords={'y': temp_df['y'], 'x': temp_df['x']}\n",
    "            )\n",
    "            \n",
    "            # Set CRS for cell_areas_clipped to match the CRS of trend_significance_ds\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.write_crs('EPSG:4326')\n",
    "    \n",
    "            # Clip cell_areas to the same geometry\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.clip([geom], drop=True)\n",
    "        \n",
    "            # Compute the total area that is impacted by a severe heatwave\n",
    "            heatwave_area = (masked_df * cell_areas_clipped).sum(dim=('y', 'x')).compute()  # Compute to convert from Dask array\n",
    "    \n",
    "            # Calculate the area for biodiversity based on the mask\n",
    "            area_biodiversity = ((masked_df * cell_areas_clipped) * masked_data_interp).sum(dim=['x', 'y']).compute()\n",
    "\n",
    "            total_area = cell_areas_clipped.sum(dim=('y', 'x')).compute()  # Ensure computation\n",
    "    \n",
    "            # Extract values after computing\n",
    "            heatwave_value = heatwave_area.item() if heatwave_area.size == 1 else heatwave_area.values[0]\n",
    "            total_area_value = total_area.item() if total_area.size == 1 else total_area.values[0]\n",
    "            area_biodiversity = area_biodiversity.item() if area_biodiversity.size == 1 else area_biodiversity.values[0]\n",
    "    \n",
    "            # Store the result\n",
    "            area_heatwave.append({\n",
    "                'Region_Name': region_name,\n",
    "                'geometry': geom,\n",
    "                'Heatwave_Area': area*heatwave_value/total_area_value,\n",
    "                'Heatwave_Area_Percent': 100*heatwave_value/total_area_value,\n",
    "                'Sea_Area': area,\n",
    "                'Biodiversity_Area': area*area_biodiversity/total_area_value,\n",
    "                'Biodiversity_Area_Percent': 100*area_biodiversity/total_area_value,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {region_name}: {e}\")\n",
    "\n",
    "    # Convert the results to a GeoDataFrame for easy viewing\n",
    "    area_heatwave_gdf = gpd.GeoDataFrame(area_heatwave, crs=SEAS_DF.crs)\n",
    "    return area_heatwave_gdf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5164ba-c570-424c-a810-d7f172f3de88",
   "metadata": {},
   "source": [
    "# Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c00d4-77cc-4782-a181-9f13bf4e6a9f",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca8ca4-21b0-469a-aefe-266348bb6091",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_1.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c41e63-e685-4d74-9e18-70249b5c02b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_data = pd.read_csv(\"../Data/GISTEMP_SST.csv\") # Data downloaded from GISS Surface Temperature Analysis (v4)\n",
    "gmst_data = pd.read_csv(\"../Data/GMST_GISTEMP4.csv\") # Data downloaded from GISS Surface Temperature Analysis (v4)\n",
    "\n",
    "ocean_data['Ocean_Annual'] = ocean_data['Lowess(5)']\n",
    "gmst_data['GMST_Annual'] = gmst_data['Lowess(5)']\n",
    "\n",
    "temp_data = ocean_data.merge(gmst_data,on='Year')\n",
    "\n",
    "# Calculate the mean of the 'No_Smoothing' column for the period 1880-1900\n",
    "base_period = temp_data[(temp_data['Year'] >= 1880) & (temp_data['Year'] <= 1900)]\n",
    "mean_sst_base_period = base_period['Ocean_Annual'].mean()\n",
    "mean_gmst_base_period = base_period['GMST_Annual'].mean()\n",
    "\n",
    "# Update the 'No_Smoothing' column to be anomalies relative to the period 1880-1900\n",
    "temp_data['Ocean_Annual'] = temp_data['Ocean_Annual'] - mean_sst_base_period\n",
    "temp_data['GMST_Annual'] = temp_data['GMST_Annual'] - mean_gmst_base_period\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(temp_data['Year'], temp_data['Ocean_Annual'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "temp_data['ocean_trend'] = intercept + slope * temp_data['Year']\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(temp_data['Year'], temp_data['GMST_Annual'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "temp_data['gmst_trend'] = intercept + slope * temp_data['Year']\n",
    "temp_data['paris_goal'] = 1.5\n",
    "\n",
    "# Save out as JSON\n",
    "temp_data[['Year','Ocean_Annual','ocean_trend','GMST_Annual','gmst_trend','paris_goal']].to_json(\n",
    "    \"../Data/Figure_1_temperature.json\", \n",
    "    orient=\"records\",  # list of dicts (one per row)\n",
    "    indent=2           # pretty print\n",
    ")\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "temp_data[['Year','Ocean_Annual','ocean_trend','GMST_Annual','gmst_trend','paris_goal']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126ca5c-8d7f-47e6-bf6e-d56ffaa63fe4",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea8702-867b-43ed-b85c-f0289d80a964",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_2.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5f7f6-5fd2-4431-a9ba-f0866c389e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "trends_df = xr.open_dataset(\"../Data/global_omi_tempsal_sst_trend_19932021_P20220331.nc\")\n",
    "\n",
    "# --------------------\n",
    "# CONFIG\n",
    "# --------------------\n",
    "OUTPUT_TIF_8BIT   = \"../Data/Figure_2_temperature.tif\"\n",
    "OUTPUT_TIF_FLOAT  = \"../Data/sst_trend_float32.tif\"\n",
    "USE_PERCENTILES   = True\n",
    "P_LOW, P_HIGH     = 2, 98\n",
    "VMIN_FIXED, VMAX_FIXED = -0.5, 0.5  # if USE_PERCENTILES=False\n",
    "\n",
    "# 1) Select the dataarray\n",
    "da = trends_df[\"sst_trends\"]\n",
    "if \"time\" in da.dims:\n",
    "    da = da.mean(dim=\"time\")\n",
    "\n",
    "# 2) Rename to x/y if needed\n",
    "rename_map = {}\n",
    "if \"lat\" in da.dims or \"lat\" in da.coords: rename_map[\"lat\"] = \"y\"\n",
    "if \"latitude\" in da.dims or \"latitude\" in da.coords: rename_map[\"latitude\"] = \"y\"\n",
    "if \"lon\" in da.dims or \"lon\" in da.coords: rename_map[\"lon\"] = \"x\"\n",
    "if \"longitude\" in da.dims or \"longitude\" in da.coords: rename_map[\"longitude\"] = \"x\"\n",
    "if rename_map:\n",
    "    da = da.rename(rename_map)\n",
    "\n",
    "# 3) Ensure y is north→south (descending)\n",
    "if da[\"y\"].values[0] < da[\"y\"].values[-1]:\n",
    "    da = da.sortby(\"y\", ascending=False)\n",
    "\n",
    "# 4) Register spatial dims & CRS\n",
    "da = da.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "if da.rio.crs is None:\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "\n",
    "# 5) Prepare data and scaling\n",
    "data = da.data.astype(np.float32)\n",
    "valid = np.isfinite(data)\n",
    "\n",
    "if USE_PERCENTILES:\n",
    "    vmin = float(np.nanpercentile(data, P_LOW))\n",
    "    vmax = float(np.nanpercentile(data, P_HIGH))\n",
    "else:\n",
    "    vmin, vmax = float(VMIN_FIXED), float(VMAX_FIXED)\n",
    "\n",
    "if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "    raise ValueError(f\"Bad scaling range: vmin={vmin}, vmax={vmax}\")\n",
    "\n",
    "# 6) Build the 8-bit layer (reserve 0 for NoData)\n",
    "scaled = np.zeros_like(data, dtype=np.uint8)  # 0 = NoData\n",
    "scaled_valid = (np.clip((data[valid] - vmin) / (vmax - vmin), 0.0, 1.0) * 254 + 1).astype(np.uint8)\n",
    "scaled[valid] = scaled_valid\n",
    "\n",
    "da8 = da.copy(data=scaled)\n",
    "\n",
    "# --- CRITICAL: clear CF encodings that carry a massive _FillValue ---\n",
    "da8.encoding.pop(\"_FillValue\", None)\n",
    "da8.encoding.pop(\"missing_value\", None)\n",
    "# (optional) also clear scale/offset if present\n",
    "da8.encoding.pop(\"scale_factor\", None)\n",
    "da8.encoding.pop(\"add_offset\", None)\n",
    "\n",
    "# Set nodata appropriate for uint8 (0)\n",
    "da8 = da8.rio.write_nodata(0, encoded=False, inplace=False)\n",
    "\n",
    "# 7a) Write 8-bit GeoTIFF\n",
    "da8.rio.to_raster(\n",
    "    OUTPUT_TIF_8BIT,\n",
    "    dtype=\"uint8\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "# 7b) Optional: write float32 with native values\n",
    "daf = da.where(np.isfinite(da))\n",
    "# Clear encodings here too\n",
    "daf.encoding.pop(\"_FillValue\", None)\n",
    "daf.encoding.pop(\"missing_value\", None)\n",
    "daf.encoding.pop(\"scale_factor\", None)\n",
    "daf.encoding.pop(\"add_offset\", None)\n",
    "\n",
    "# Use NaN as nodata for float32 (supported by rasterio/GTiff)\n",
    "daf = daf.rio.write_nodata(np.nan, encoded=False, inplace=False)\n",
    "\n",
    "daf.rio.to_raster(\n",
    "    OUTPUT_TIF_FLOAT,\n",
    "    dtype=\"float32\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Wrote {OUTPUT_TIF_8BIT} (uint8; 0=NoData, 1–255=data) and {OUTPUT_TIF_FLOAT} (float32). \"\n",
    "    f\"Scale for 8-bit: vmin={vmin:.4f}, vmax={vmax:.4f} (units of sst_trends).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a6928-80c1-41f0-8931-f136ed10e05f",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a55a1-632f-44d0-8e46-1897367aefa7",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_3.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c1715-7776-4190-9862-0da292a3576f",
   "metadata": {},
   "source": [
    "**This figure calculates the total area for each major sea region globally. It also calculates 1) the area for each sea region impacted by a severe marine heatwave in the year 2023 as well as 2) the area for each sea that both has priority biodiversity areas AND is impacted by a severe marine heatwave.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7329b-be8f-44d0-ba51-8efb83291da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2024 Heatwave data downloaded from NOAA's Coral Reef Watch https://coralreefwatch.noaa.gov/product/marine_heatwave/\n",
    "import xarray as xr\n",
    "import glob\n",
    "\n",
    "files = sorted(glob.glob(\"../Data/2023/*.nc\"))\n",
    "\n",
    "temp_df = xr.open_mfdataset(\"../Data/2023/*.nc\")\n",
    "\n",
    "area_df = area_heatwave(temp_df)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/Figure_3_temperature.geojson\",driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8102994c-1985-4949-a649-f91af99fdaad",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d2692-6a48-4d4e-bb91-66f9922df628",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_4.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f751c1c-b9d9-4d33-8808-06e75751cd34",
   "metadata": {},
   "source": [
    "$CO_2$ data retrieved from [Lan, X., P. Tans, & K.W. Thoning (2025). Trends in globally-averaged CO₂ determined from NOAA Global Monitoring Laboratory measurements (Version 2025-11) NOAA Global Monitoring Laboratory. https://doi.org/10.15138/9N0H-ZH07](https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.txt). Temperature data retrieved from GISS Surface Temperature Analysis (v4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ec2a1-6206-4554-82a1-3481eba4ff91",
   "metadata": {},
   "source": [
    "## Figure 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94da805-3110-4b81-983f-81d7d337f510",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_5.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc173db-e178-4e15-adf9-e93113e8bd23",
   "metadata": {},
   "source": [
    "Data were retrieved from [Hughes, T. P., et al. (2018). Spatial and temporal patterns of mass bleaching of corals in the Anthropocene. Science. – processed by Our World in Data](https://ourworldindata.org/grapher/coral-bleaching-events)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e83a16-7adb-4cc2-9e13-c146d714186c",
   "metadata": {},
   "source": [
    "## Figure 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c497b-be92-4549-b3b6-54eb7d8a4112",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_6.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b62bdc-ee90-488c-9957-35ea1cb62707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import xarray as xr\n",
    "import rioxarray  # pip install rioxarray rasterio\n",
    "\n",
    "# --- Load and compute heatwave days ---\n",
    "ds = xr.open_mfdataset(\"../Data/2023/*.nc\", combine=\"by_coords\")\n",
    "\n",
    "# If you already have temp_df, you can instead do:\n",
    "# hw_count = temp_df\n",
    "\n",
    "hw_count = (ds[\"heatwave_category\"] >= 1).sum(dim=\"time\")\n",
    "hw_count = hw_count.astype(\"float32\")\n",
    "\n",
    "# --- Rechunk along spatial dims for quantile ---\n",
    "# Identify spatial dimensions (everything except 'time')\n",
    "spatial_dims = [d for d in hw_count.dims if d != \"time\"]\n",
    "\n",
    "# Make each spatial dimension a single chunk\n",
    "hw_q = hw_count.chunk({d: -1 for d in spatial_dims})\n",
    "\n",
    "# --- Compute 2nd and 98th percentiles over space ---\n",
    "q = hw_q.quantile([0.02, 0.98], dim=spatial_dims, skipna=True).compute()\n",
    "\n",
    "p2 = float(q.sel(quantile=0.02).values)\n",
    "p98 = float(q.sel(quantile=0.98).values)\n",
    "\n",
    "print(f\"2nd percentile: {p2}, 98th percentile: {p98}\")\n",
    "\n",
    "if p98 <= p2:\n",
    "    raise ValueError(\"98th percentile is not greater than 2nd percentile; cannot scale safely.\")\n",
    "\n",
    "# --- Scale to 0–255 using the 2nd and 98th percentiles ---\n",
    "scaled = (hw_count - p2) / (p98 - p2) * 255.0\n",
    "scaled = scaled.clip(0, 255)\n",
    "scaled = scaled.fillna(0)\n",
    "\n",
    "# Round and cast to uint8\n",
    "scaled_uint8 = scaled.round().astype(\"uint8\")\n",
    "\n",
    "# --- Set spatial dims and CRS for Mapbox ---\n",
    "# Try to guess your x/y dimension names\n",
    "cands_x = [\"lon\", \"longitude\", \"x\", \"londim\"]\n",
    "cands_y = [\"lat\", \"latitude\", \"y\", \"latdim\"]\n",
    "\n",
    "x_dim = next(d for d in cands_x if d in scaled_uint8.dims)\n",
    "y_dim = next(d for d in cands_y if d in scaled_uint8.dims)\n",
    "\n",
    "scaled_uint8 = scaled_uint8.rio.set_spatial_dims(x_dim=x_dim, y_dim=y_dim, inplace=False)\n",
    "\n",
    "# Ensure WGS84\n",
    "if not scaled_uint8.rio.crs:\n",
    "    scaled_uint8 = scaled_uint8.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "# Optional: use 0 as nodata for transparency in Mapbox\n",
    "scaled_uint8 = scaled_uint8.rio.write_nodata(0)\n",
    "\n",
    "# --- Save as GeoTIFF ---\n",
    "out_path = \"Figure_6_temperature.tif\"\n",
    "scaled_uint8.rio.to_raster(out_path, dtype=\"uint8\")\n",
    "\n",
    "print(f\"Saved GeoTIFF: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54339b03-3b65-417a-b87d-2347c4a693f7",
   "metadata": {},
   "source": [
    "## Figure 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a29b4b-602e-477f-a151-d1c6f0b88748",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_7.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ffe68-b87c-4c87-b449-d6d27da859aa",
   "metadata": {},
   "source": [
    "Data were retrieved from [Jones et al. (2024) – with major processing by Our World in Data](https://ourworldindata.org/co2-and-greenhouse-gas-emissions#explore-data-on-co2-and-greenhouse-gas-emissions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e24ffbc-d1f7-4940-a42d-67c0f0cb4a2b",
   "metadata": {},
   "source": [
    "# Salinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3161f5a-816c-4739-b7bf-d37e80b3e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"../Data/Figure_3_temperature.geojson\")\n",
    "\n",
    "(df['Biodiversity_Area'].sum() / df['Sea_Area'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5b29e-d916-4d88-a01b-9cb7f4a3a7fc",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9090c02-0d5c-4ed5-bc90-30deb056defe",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_salinity_1.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc03ef6-b653-46bf-9a5d-01ba19a94020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "salt_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "salt_df = salt_df['salinity'].mean(dim=['lat','lon']).resample(time='Y').mean()\n",
    "\n",
    "final_subset = salt_df.sel(time=slice('1994-01-01', None))\n",
    "\n",
    "# Create a pandas DataFrame with these columns\n",
    "df = pd.DataFrame({\n",
    "    'time': final_subset['time'].values,\n",
    "    'salinity': final_subset.values,\n",
    "})\n",
    "\n",
    "# Convert 'time' to datetime\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Convert datetime to a numerical value for linear regression (using ordinal format)\n",
    "df['time_ordinal'] = df['time'].map(pd.Timestamp.toordinal)\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(df['time_ordinal'], df['salinity'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "df['linear_trend'] = intercept + slope * df['time_ordinal']\n",
    "\n",
    "df[['time','salinity','linear_trend']].to_csv(\"../Data/Figure_1_salinity.csv\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df[['time','salinity','linear_trend']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af9036-4fe4-4c9a-83c7-215181777c26",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f90f8-5b07-4765-adf3-c00f41db7f1b",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_salinity_2.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ee663-1312-494c-b02d-5f8c0127bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "salt_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(salt_df['salinity'])\n",
    "\n",
    "# --------------------\n",
    "# CONFIG\n",
    "# --------------------\n",
    "OUTPUT_TIF_8BIT   = \"./Figure_2_salinity.tif\"\n",
    "OUTPUT_TIF_FLOAT  = \"./salinity_trend_float32.tif\"\n",
    "USE_PERCENTILES   = True\n",
    "P_LOW, P_HIGH     = 2, 98\n",
    "VMIN_FIXED, VMAX_FIXED = -0.5, 0.5  # if USE_PERCENTILES=False\n",
    "\n",
    "# 1) Select the dataarray: salinity trend\n",
    "da = trend_significance_ds['trend']\n",
    "\n",
    "# 2) Rename to x/y if needed\n",
    "rename_map = {}\n",
    "if \"lat\" in da.dims or \"lat\" in da.coords: rename_map[\"lat\"] = \"y\"\n",
    "if \"latitude\" in da.dims or \"latitude\" in da.coords: rename_map[\"latitude\"] = \"y\"\n",
    "if \"lon\" in da.dims or \"lon\" in da.coords: rename_map[\"lon\"] = \"x\"\n",
    "if \"longitude\" in da.dims or \"longitude\" in da.coords: rename_map[\"longitude\"] = \"x\"\n",
    "if rename_map:\n",
    "    da = da.rename(rename_map)\n",
    "\n",
    "# 3) Ensure y is north→south (descending)\n",
    "if da[\"y\"].values[0] < da[\"y\"].values[-1]:\n",
    "    da = da.sortby(\"y\", ascending=False)\n",
    "# ensure (y, x) order\n",
    "if tuple(da.dims) != (\"y\", \"x\"):\n",
    "    da = da.transpose(\"y\", \"x\")\n",
    "\n",
    "# 4) Register spatial dims & force EPSG:4326\n",
    "da = da.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "\n",
    "# If CRS is missing, assume geographic lon/lat; then force-reproject to EPSG:4326\n",
    "if da.rio.crs is None:\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "\n",
    "# If CRS is not 4326, reproject it so the saved rasters are truly EPSG:4326\n",
    "if str(da.rio.crs) != \"EPSG:4326\":\n",
    "    # Use nearest for categorical-like; bilinear is typical for continuous SLA\n",
    "    da = da.rio.reproject(\"EPSG:4326\", resampling=rioxarray.rio.reproject.Resampling.bilinear)\n",
    "\n",
    "# Reassert dims order after any reprojection (just in case)\n",
    "if tuple(da.dims) != (\"y\", \"x\"):\n",
    "    da = da.transpose(\"y\", \"x\")\n",
    "\n",
    "\n",
    "# 5) Compute scaling range (Dask-safe)\n",
    "import dask.array as dsa\n",
    "\n",
    "def compute_percentiles_safe(da, p_low, p_high):\n",
    "    \"\"\"\n",
    "    Try dask.array.nanpercentile; if that fails (older dask/xarray),\n",
    "    fall back to a coarse sample to keep memory in check.\n",
    "    \"\"\"\n",
    "    if getattr(da, \"chunks\", None):\n",
    "        try:\n",
    "            vmin = float(dsa.nanpercentile(da.data, p_low).compute())\n",
    "            vmax = float(dsa.nanpercentile(da.data, p_high).compute())\n",
    "            return vmin, vmax\n",
    "        except Exception:\n",
    "            pass  # fall back to sampled approach\n",
    "\n",
    "    # Fallback: sample every Nth pixel (keeps memory tiny)\n",
    "    step_y = max(int(len(da.y) // 512), 1) if \"y\" in da.dims else 4\n",
    "    step_x = max(int(len(da.x) // 512), 1) if \"x\" in da.dims else 4\n",
    "    das = da.isel(\n",
    "        y=slice(0, None, step_y) if \"y\" in da.dims else slice(None),\n",
    "        x=slice(0, None, step_x) if \"x\" in da.dims else slice(None),\n",
    "    ).load()  # small enough to load\n",
    "    vmin = float(np.nanpercentile(das.values, p_low))\n",
    "    vmax = float(np.nanpercentile(das.values, p_high))\n",
    "    return vmin, vmax\n",
    "\n",
    "if USE_PERCENTILES:\n",
    "    vmin, vmax = compute_percentiles_safe(da, P_LOW, P_HIGH)\n",
    "else:\n",
    "    vmin, vmax = float(VMIN_FIXED), float(VMAX_FIXED)\n",
    "\n",
    "if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "    raise ValueError(f\"Bad scaling range: vmin={vmin}, vmax={vmax}\")\n",
    "\n",
    "\n",
    "# 6) Build the 8-bit layer with xr.where (no boolean indexing)\n",
    "# Reserve 0 for NoData; valid cells map to [1,255]\n",
    "norm = ((da - vmin) / (vmax - vmin)).clip(0, 1)\n",
    "scaled_da = xr.where(np.isfinite(da), norm * 254.0 + 1.0, 0.0).astype(\"uint8\")\n",
    "\n",
    "# Clear troublesome encodings on the uint8 view\n",
    "for k in (\"_FillValue\", \"missing_value\", \"scale_factor\", \"add_offset\"):\n",
    "    scaled_da.encoding.pop(k, None)\n",
    "\n",
    "# Set nodata appropriate for uint8 (0)\n",
    "scaled_da = scaled_da.rio.write_nodata(0, encoded=False, inplace=False)\n",
    "\n",
    "# 7a) Write 8-bit GeoTIFF (works with Dask; will stream-chunk if array is chunked)\n",
    "scaled_da.rio.to_raster(\n",
    "    OUTPUT_TIF_8BIT,\n",
    "    dtype=\"uint8\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "# 7b) Write float32 GeoTIFF with native values\n",
    "daf = da.where(np.isfinite(da)).astype(\"float32\")\n",
    "for k in (\"_FillValue\", \"missing_value\", \"scale_factor\", \"add_offset\"):\n",
    "    daf.encoding.pop(k, None)\n",
    "# Use NaN as nodata for float32\n",
    "daf = daf.rio.write_nodata(np.nan, encoded=False, inplace=False)\n",
    "\n",
    "daf.rio.to_raster(\n",
    "    OUTPUT_TIF_FLOAT,\n",
    "    dtype=\"float32\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Wrote {OUTPUT_TIF_8BIT} (uint8; 0=NoData, 1–255=data) and {OUTPUT_TIF_FLOAT} (float32). \"\n",
    "    f\"8-bit scale: vmin={vmin:.4f}, vmax={vmax:.4f} (native SLA units).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09894f6-aa65-4b23-b39a-f45e51671fe3",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40a8ae-1476-4a1e-b6f5-1146e1f66587",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_salinity_3.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c66766-7acd-46ad-bbbc-a57a0d54344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(salt_df['salinity'])\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/Figure_3_salinity.geojson\",driver=\"GeoJSON\")\n",
    "\n",
    "del salt_df, area_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e07c51-729b-4c85-84b7-dbf550e3014a",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdaa8d3-49c1-4d5c-8a75-e6f5ad41d77a",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_salinity_4.png\" style=\"width:50%;\">\n",
    "</p>\n",
    "\n",
    "**This figure reproduces Figure 3A from [Delworth et al. (2022)](https://www.pnas.org/doi/10.1073/pnas.2116655119). It plots an index of the Atlantic Meridional Overturning Circulation (AMOC) and how it changes over time. There are 6 scenarios --- 2 of which have historical data (\"HISTORICAL\" (representing a historical evolution of anthropogenic emissions) and \"NATURAL\" (no changes to anthropogenic emissions)), and 5 have projections (\"NATURAL\" again, as well as different Shared Socioeconomic Pathway (SSP) scenarios for how emissions will change in the future). Each scenario is run several times, which provides the measures of uncertainty presented in the figure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0806b72-69d8-4cd6-abb5-910daf23d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the file with more flexibility in handling its format to try and correctly parse it.\n",
    "with open('../Data/TAR_FIGURE_3_AMOC_45N', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Adjust the column naming logic to include the full scenario name as requested\n",
    "scenario_data = {}\n",
    "current_scenario = None\n",
    "time_range = range(1921, 2100)\n",
    "\n",
    "for line in lines:\n",
    "    # Check if line indicates a new scenario/ensemble using the SPEAR_c192_o1 pattern\n",
    "    scenario_match = re.search(r'SPEAR_c192_o1_(.+)_ENS_(\\d+)', line)\n",
    "    if scenario_match:\n",
    "        # Preserve the full scenario name (like HIST_SSP585_ALLForc) and ensemble number\n",
    "        scenario_name = scenario_match.group(1) + \"_\" + scenario_match.group(2)\n",
    "        current_scenario = scenario_name\n",
    "        scenario_data[current_scenario] = []\n",
    "    else:\n",
    "        # If the line contains numerical data, extract and add it to the current scenario\n",
    "        match = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line.strip())\n",
    "        if match and current_scenario:\n",
    "            scenario_data[current_scenario].extend([float(value) for value in match])\n",
    "\n",
    "# Creating a DataFrame where the first column is the time and each subsequent column is a scenario/ensemble\n",
    "df = pd.DataFrame({'Year': list(time_range)})\n",
    "\n",
    "for scenario, values in scenario_data.items():\n",
    "    df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
    "\n",
    "# Define scenario prefixes to filter the columns\n",
    "scenarios = ['SSP119', 'SSP245', 'SSP534', 'SSP585', 'NATURAL']\n",
    "\n",
    "# Initialize a result DataFrame\n",
    "result = pd.DataFrame()\n",
    "result['Year'] = df['Year']\n",
    "\n",
    "# Loop through each scenario to calculate the mean and confidence intervals\n",
    "for scenario in scenarios:\n",
    "    # Identify relevant columns for the current scenario\n",
    "    scenario_columns = [col for col in df.columns if scenario in col]\n",
    "    \n",
    "    if scenario_columns:  # Ensure there are columns for this scenario\n",
    "        # Calculate mean and confidence intervals\n",
    "        result[f'{scenario}_Mean'] = df[scenario_columns].mean(axis=1)\n",
    "        result[f'{scenario}_CI_Lower'] = df[scenario_columns].quantile(0.05, axis=1)\n",
    "        result[f'{scenario}_CI_Upper'] = df[scenario_columns].quantile(0.95, axis=1)\n",
    "\n",
    "# Create historical scenario columns by aggregating the SSP scenarios up to the year 2014\n",
    "historical_columns = ['SSP119', 'SSP245', 'SSP534', 'SSP585']\n",
    "\n",
    "# Create a mask for years up to 2014\n",
    "mask_historical = result['Year'] <= 2014\n",
    "\n",
    "# Calculate mean and confidence intervals for historical scenarios\n",
    "result['Historical_Mean'] = result.loc[mask_historical, [f'{scenario}_Mean' for scenario in historical_columns]].mean(axis=1)\n",
    "result['Historical_CI_Lower'] = result.loc[mask_historical, [f'{scenario}_CI_Lower' for scenario in historical_columns]].mean(axis=1)\n",
    "result['Historical_CI_Upper'] = result.loc[mask_historical, [f'{scenario}_CI_Upper' for scenario in historical_columns]].mean(axis=1)\n",
    "\n",
    "# Set SSP columns to NaN for years up to 2014\n",
    "for scenario in historical_columns:\n",
    "    result.loc[mask_historical, f'{scenario}_Mean'] = np.nan\n",
    "    result.loc[mask_historical, f'{scenario}_CI_Lower'] = np.nan\n",
    "    result.loc[mask_historical, f'{scenario}_CI_Upper'] = np.nan\n",
    "\n",
    "# Set Historical columns to NaN for years after 2014\n",
    "result.loc[~mask_historical, 'Historical_Mean'] = np.nan\n",
    "result.loc[~mask_historical, 'Historical_CI_Lower'] = np.nan\n",
    "result.loc[~mask_historical, 'Historical_CI_Upper'] = np.nan\n",
    "\n",
    "result.to_csv(\"../Data/Figure_4_salinity.csv\")\n",
    "\n",
    "result.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb5a62-976a-4316-8ffa-53ef38a66b74",
   "metadata": {},
   "source": [
    "## Figure 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b1d659-ccd7-42ae-bc0c-e83f9f551250",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_salinity_5.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864918e9-969e-4945-b468-e6cb9559f981",
   "metadata": {},
   "source": [
    "Monthly data were retrieved from CMEMS [Mercator Ocean International / Copernicus Marine Service (2023). Global Ocean Physics Reanalysis (GLOBAL_MULTIYEAR_PHY_001_030) [Data set]. Copernicus Marine Service. https://doi.org/10.48670/moi-00021](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe02b21-1e1f-40fd-862b-cbede65687df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, json, datetime as dt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from affine import Affine\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "IN_PATH = \"../Data/cmems_mod_glo_phy_my_0.083deg_P1M-m_1760469819927.nc\"\n",
    "OUT_DIR = \"../Data/Figure_5_salinity\"\n",
    "VAR_NAME_HINTS = [\"so\", \"salinity\"]\n",
    "WRITE_FLOAT32 = False   # set True to write Float32 GeoTIFFs instead of 8-bit\n",
    "LOW_Q, HIGH_Q = 0.02, 0.98   # robust percentiles for 8-bit scaling\n",
    "COMPRESS = \"DEFLATE\"    # GeoTIFF compression\n",
    "NODATA_UINT8 = 0        # 0 reserved as nodata in 8-bit output\n",
    "OPEN_WITH_CHUNKS = True # try to enable dask-friendly quantiles\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def pick_var(ds: xr.Dataset) -> xr.DataArray:\n",
    "    for name in VAR_NAME_HINTS:\n",
    "        if name in ds.data_vars:\n",
    "            return ds[name]\n",
    "    return next(iter(ds.data_vars.values()))\n",
    "\n",
    "def ensure_xy(ds_like: xr.DataArray) -> xr.DataArray:\n",
    "    da = ds_like\n",
    "    rename_map = {}\n",
    "    if \"lon\" in da.dims: rename_map[\"lon\"] = \"x\"\n",
    "    if \"longitude\" in da.dims: rename_map[\"longitude\"] = \"x\"\n",
    "    if \"lat\" in da.dims: rename_map[\"lat\"] = \"y\"\n",
    "    if \"latitude\" in da.dims: rename_map[\"latitude\"] = \"y\"\n",
    "    if rename_map:\n",
    "        da = da.rename(rename_map)\n",
    "    if \"x\" not in da.dims or \"y\" not in da.dims:\n",
    "        raise ValueError(f\"Expected 'x' and 'y' dims, found {list(da.dims)}\")\n",
    "    if np.all(np.diff(da[\"y\"].values) > 0):\n",
    "        da = da.sortby(\"y\", ascending=False)\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "    xv, yv = da[\"x\"].values, da[\"y\"].values\n",
    "    dx = float(np.mean(np.diff(xv)))\n",
    "    dy = float(np.mean(np.diff(yv)))\n",
    "    x0 = xv.min() - dx / 2.0\n",
    "    y0 = yv.max() + dy / 2.0\n",
    "    transform = Affine(dx, 0.0, x0, 0.0, -dy, y0)\n",
    "    da = da.rio.write_transform(transform, inplace=False)\n",
    "    return da\n",
    "\n",
    "def compute_global_bounds(var: xr.DataArray, low_q=LOW_Q, high_q=HIGH_Q):\n",
    "    \"\"\"\n",
    "    Compute global robust quantiles across all time/y/x and return (vmin, vmax).\n",
    "    Tries xarray.quantile (dask-friendly), falls back to numpy if needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        q = var.quantile([low_q, high_q], dim=[d for d in var.dims if d in (\"time\",\"y\",\"x\",\"lat\",\"lon\",\"latitude\",\"longitude\")], skipna=True)\n",
    "        vmin = float(q.sel(quantile=low_q).values)\n",
    "        vmax = float(q.sel(quantile=high_q).values)\n",
    "    except Exception:\n",
    "        arr = var.values.astype(np.float32)  # may be large!\n",
    "        vmin, vmax = np.nanquantile(arr, [low_q, high_q])\n",
    "        vmin, vmax = float(vmin), float(vmax)\n",
    "    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin == vmax:\n",
    "        # very flat / missing data; fallback to global min/max\n",
    "        try:\n",
    "            vmin = float(var.min(skipna=True).values)\n",
    "            vmax = float(var.max(skipna=True).values)\n",
    "        except Exception:\n",
    "            arr = var.values.astype(np.float32)\n",
    "            vmin, vmax = float(np.nanmin(arr)), float(np.nanmax(arr))\n",
    "    return vmin, vmax\n",
    "\n",
    "def scale_to_uint8_fixed(da: xr.DataArray, vmin: float, vmax: float, nodata_val=NODATA_UINT8):\n",
    "    \"\"\"\n",
    "    Scale with fixed vmin/vmax to 8-bit, reserving 0 as nodata.\n",
    "    \"\"\"\n",
    "    arr = da.values.astype(np.float32)\n",
    "    mask = ~np.isfinite(arr)\n",
    "    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin == vmax:\n",
    "        # avoid divide-by-zero\n",
    "        vmin = np.nanmin(arr)\n",
    "        vmax = np.nanmax(arr)\n",
    "        if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin == vmax:\n",
    "            scaled = np.full(arr.shape, nodata_val, dtype=np.uint8)\n",
    "            return scaled\n",
    "    scaled = (arr - vmin) / (vmax - vmin + 1e-12)\n",
    "    scaled = np.clip(scaled, 0.0, 1.0)\n",
    "    scaled = (scaled * 254.0 + 1.0).astype(np.uint8)\n",
    "    scaled[mask] = nodata_val\n",
    "    return scaled\n",
    "\n",
    "def main():\n",
    "    open_kwargs = {\"chunks\": \"auto\"} if OPEN_WITH_CHUNKS else {}\n",
    "    ds = xr.open_dataset(IN_PATH, **open_kwargs)\n",
    "    var = pick_var(ds)\n",
    "    if \"time\" not in var.dims:\n",
    "        raise ValueError(\"Expected a 'time' dimension in salinity DataArray.\")\n",
    "\n",
    "    # --- Compute one pair of robust bounds across ALL timesteps ---\n",
    "    global_vmin, global_vmax = compute_global_bounds(var, LOW_Q, HIGH_Q)\n",
    "    print(f\"Global scaling bounds ({LOW_Q:.2f}-{HIGH_Q:.2f} quantiles): vmin={global_vmin:.4f}, vmax={global_vmax:.4f}\")\n",
    "\n",
    "    scaling_meta = {\n",
    "        \"variable\": str(var.name),\n",
    "        \"quantiles\": [LOW_Q, HIGH_Q],\n",
    "        \"global_vmin\": float(global_vmin),\n",
    "        \"global_vmax\": float(global_vmax),\n",
    "        \"nodata_uint8\": NODATA_UINT8,\n",
    "        \"files\": []\n",
    "    }\n",
    "\n",
    "    for i in range(var.sizes[\"time\"]):\n",
    "        slice_t = var.isel(time=i).squeeze(drop=True)\n",
    "        da = ensure_xy(slice_t)\n",
    "\n",
    "        if \"time\" in slice_t.coords:\n",
    "            tlabel = np.datetime_as_string(slice_t[\"time\"].values, unit=\"D\").replace(\"-\", \"\")[:6]  # YYYYMM\n",
    "        else:\n",
    "            tlabel = f\"t{i:04d}\"\n",
    "\n",
    "        if WRITE_FLOAT32:\n",
    "            out_path = os.path.join(OUT_DIR, f\"salinity_{tlabel}_float32.tif\")\n",
    "            da = da.rio.write_nodata(np.nan, inplace=False)\n",
    "            da.rio.to_raster(out_path, dtype=\"float32\", tiled=True, compress=COMPRESS)\n",
    "            print(f\"Wrote {out_path}\")\n",
    "            scaling_meta[\"files\"].append({\"file\": os.path.basename(out_path), \"tindex\": i, \"tlabel\": tlabel})\n",
    "        else:\n",
    "            scaled = scale_to_uint8_fixed(da, global_vmin, global_vmax, NODATA_UINT8)\n",
    "            out_path = os.path.join(OUT_DIR, f\"salinity_{tlabel}.tif\")\n",
    "            da8 = xr.DataArray(\n",
    "                scaled, dims=(\"y\",\"x\"), coords={\"y\": da[\"y\"], \"x\": da[\"x\"]},\n",
    "                name=\"salinity_uint8\",\n",
    "                attrs={\"long_name\": \"Sea Water Practical Salinity (scaled 8-bit, global bounds)\",\n",
    "                       \"units\": \"unitless (0=nodata)\",\n",
    "                       \"scale_vmin\": global_vmin, \"scale_vmax\": global_vmax}\n",
    "            ).rio.write_crs(da.rio.crs).rio.write_transform(da.rio.transform())\n",
    "            da8 = da8.rio.write_nodata(NODATA_UINT8, inplace=False)\n",
    "            da8.rio.to_raster(out_path, dtype=\"uint8\", tiled=True, compress=COMPRESS)\n",
    "            scaling_meta[\"files\"].append({\"file\": os.path.basename(out_path), \"tindex\": i, \"tlabel\": tlabel})\n",
    "            print(f\"Wrote {out_path} using global scaling [{global_vmin:.4f}, {global_vmax:.4f}]\")\n",
    "\n",
    "    if not WRITE_FLOAT32:\n",
    "        meta_path = os.path.join(OUT_DIR, \"salinity_scaling_metadata.json\")\n",
    "        with open(meta_path, \"w\") as f:\n",
    "            json.dump(scaling_meta, f, indent=2)\n",
    "        print(f\"Saved {meta_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b17496-faf5-4fae-9292-2d99ef0eb966",
   "metadata": {},
   "source": [
    "## Figure 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a153f-9582-4422-8c70-4b8e93663f16",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_salinity_6.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31103fc6-c7d8-4cdf-84d6-9898a9bfde03",
   "metadata": {},
   "source": [
    "Monthly data were retrieved from CMEMS [Mercator Ocean International / Copernicus Marine Service (2023). Global Ocean Physics Reanalysis (GLOBAL_MULTIYEAR_PHY_001_030) [Data set]. Copernicus Marine Service. https://doi.org/10.48670/moi-00021](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0620df74-a8ab-466f-b826-abf1f30ba924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from affine import Affine\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "U_PATH = \"../Data/cmems_mod_glo_phy_my_0.083deg_P1M-m_1760473504514.nc\"  # uo\n",
    "V_PATH = \"../Data/cmems_mod_glo_phy_my_0.083deg_P1M-m_1760473531668.nc\"  # vo\n",
    "OUT_DIR = \"../Data/Figure_6_salinity\"\n",
    "WRITE_FLOAT32 = False        # True -> Float32 GeoTIFFs; False -> 8-bit scaled GeoTIFFs\n",
    "LOW_Q, HIGH_Q = 0.02, 0.98   # robust percentile bounds used globally across ALL timesteps\n",
    "COMPRESS = \"DEFLATE\"         # GeoTIFF compression\n",
    "NODATA_UINT8 = 0             # 0 reserved as nodata in 8-bit output\n",
    "OPEN_WITH_CHUNKS = True      # dask-friendly\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# helpers\n",
    "# ---------------------------\n",
    "def _ensure_xy(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"Rename lon/lat to x/y if needed, set CRS, ensure y desc, attach affine transform.\"\"\"\n",
    "    rename_map = {}\n",
    "    if \"lon\" in da.dims: rename_map[\"lon\"] = \"x\"\n",
    "    if \"longitude\" in da.dims: rename_map[\"longitude\"] = \"x\"\n",
    "    if \"lat\" in da.dims: rename_map[\"lat\"] = \"y\"\n",
    "    if \"latitude\" in da.dims: rename_map[\"latitude\"] = \"y\"\n",
    "    if rename_map:\n",
    "        da = da.rename(rename_map)\n",
    "    if \"x\" not in da.dims or \"y\" not in da.dims:\n",
    "        raise ValueError(f\"Expected 'x' and 'y' dims, found {list(da.dims)}\")\n",
    "    # y descending (north -> south)\n",
    "    if np.all(np.diff(da[\"y\"].values) > 0):\n",
    "        da = da.sortby(\"y\", ascending=False)\n",
    "    # georef\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "    xv, yv = da[\"x\"].values, da[\"y\"].values\n",
    "    dx = float(np.mean(np.diff(xv)))\n",
    "    dy = float(np.mean(np.diff(yv)))\n",
    "    x0 = xv.min() - dx / 2.0\n",
    "    y0 = yv.max() + dy / 2.0\n",
    "    transform = Affine(dx, 0.0, x0, 0.0, -dy, y0)\n",
    "    da = da.rio.write_transform(transform, inplace=False)\n",
    "    return da\n",
    "\n",
    "def _pick_surface(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    If a vertical dimension exists, take surface (depth ~ 0) by coord if available,\n",
    "    otherwise first level. Handles common names.\n",
    "    \"\"\"\n",
    "    z_names = [d for d in da.dims if d.lower() in (\"depth\",\"lev\",\"z\",\"depthu\",\"depthv\",\"deptho\")]\n",
    "    if not z_names:\n",
    "        return da\n",
    "    z = z_names[0]\n",
    "    try:\n",
    "        if z in da.coords and np.issubdtype(da.coords[z].dtype, np.number):\n",
    "            # choose nearest to 0 if 0 present or nearest available\n",
    "            if 0 in set(np.asarray(da.coords[z]).round(6)):\n",
    "                da = da.sel({z: 0}, method=\"nearest\")\n",
    "            else:\n",
    "                da = da.sel({z: float(da.coords[z].values.min())}, method=\"nearest\")\n",
    "        else:\n",
    "            da = da.isel({z: 0})\n",
    "    except Exception:\n",
    "        da = da.isel({z: 0})\n",
    "    return da\n",
    "\n",
    "def _compute_global_bounds(var: xr.DataArray, low_q=LOW_Q, high_q=HIGH_Q):\n",
    "    \"\"\"Compute global robust quantiles across time/y/x; fallback to min/max if degenerate.\"\"\"\n",
    "    try:\n",
    "        q = var.quantile([low_q, high_q], dim=[d for d in var.dims if d in (\"time\",\"y\",\"x\",\"lat\",\"lon\",\"latitude\",\"longitude\")], skipna=True)\n",
    "        vmin = float(q.sel(quantile=low_q).values)\n",
    "        vmax = float(q.sel(quantile=high_q).values)\n",
    "    except Exception:\n",
    "        arr = var.values.astype(np.float32)\n",
    "        vmin, vmax = np.nanquantile(arr, [low_q, high_q])\n",
    "        vmin, vmax = float(vmin), float(vmax)\n",
    "    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "        # fallback to global min/max\n",
    "        try:\n",
    "            vmin = float(var.min(skipna=True).values)\n",
    "            vmax = float(var.max(skipna=True).values)\n",
    "        except Exception:\n",
    "            arr = var.values.astype(np.float32)\n",
    "            vmin, vmax = float(np.nanmin(arr)), float(np.nanmax(arr))\n",
    "    return vmin, vmax\n",
    "\n",
    "def _scale_to_uint8_fixed(da: xr.DataArray, vmin: float, vmax: float, nodata_val=NODATA_UINT8):\n",
    "    \"\"\"Scale with fixed vmin/vmax to 8-bit, reserving 0 as nodata.\"\"\"\n",
    "    arr = da.values.astype(np.float32)\n",
    "    mask = ~np.isfinite(arr)\n",
    "    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "        vmin = np.nanmin(arr)\n",
    "        vmax = np.nanmax(arr)\n",
    "        if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "            return np.full(arr.shape, nodata_val, dtype=np.uint8)\n",
    "    scaled = (arr - vmin) / (vmax - vmin + 1e-12)\n",
    "    scaled = np.clip(scaled, 0.0, 1.0)\n",
    "    scaled = (scaled * 254.0 + 1.0).astype(np.uint8)  # 1..255; 0 reserved as nodata\n",
    "    scaled[mask] = nodata_val\n",
    "    return scaled\n",
    "\n",
    "# ---------------------------\n",
    "# main\n",
    "# ---------------------------\n",
    "def main():\n",
    "    open_kwargs = {\"chunks\": \"auto\"} if OPEN_WITH_CHUNKS else {}\n",
    "    # Merge u and v by coords\n",
    "    ds = xr.open_mfdataset([U_PATH, V_PATH], combine=\"by_coords\", **open_kwargs)\n",
    "\n",
    "    if \"uo\" not in ds.data_vars or \"vo\" not in ds.data_vars:\n",
    "        # some CMEMS releases use 'uo'/'vo' names consistently; if not present, try to find them\n",
    "        raise ValueError(f\"Could not find variables 'uo' and 'vo' in the provided datasets. Found: {list(ds.data_vars)}\")\n",
    "\n",
    "    uo = ds[\"uo\"]\n",
    "    vo = ds[\"vo\"]\n",
    "\n",
    "    # Surface slice if needed\n",
    "    uo_sfc = _pick_surface(uo)\n",
    "    vo_sfc = _pick_surface(vo)\n",
    "\n",
    "    # Ensure time alignment\n",
    "    if \"time\" not in uo_sfc.dims or \"time\" not in vo_sfc.dims:\n",
    "        raise ValueError(\"Expected a 'time' dimension in both uo and vo.\")\n",
    "    # align by time intersection to be safe\n",
    "    uo_sfc, vo_sfc = xr.align(uo_sfc, vo_sfc, join=\"inner\")\n",
    "\n",
    "    # Compute speed magnitude\n",
    "    speed = xr.apply_ufunc(\n",
    "        lambda a, b: np.sqrt(a*a + b*b),\n",
    "        uo_sfc, vo_sfc,\n",
    "        dask=\"parallelized\", output_dtypes=[np.float32]\n",
    "    )\n",
    "    speed.name = \"current_speed\"\n",
    "    speed.attrs.update({\"long_name\": \"Ocean current speed\", \"units\": \"m s-1\"})\n",
    "\n",
    "    # Compute one pair of robust bounds across ALL timesteps\n",
    "    global_vmin, global_vmax = _compute_global_bounds(speed, LOW_Q, HIGH_Q)\n",
    "    print(f\"Global speed scaling bounds ({LOW_Q:.2f}-{HIGH_Q:.2f} quantiles): \"\n",
    "          f\"vmin={global_vmin:.5f}, vmax={global_vmax:.5f}\")\n",
    "\n",
    "    # Metadata holder\n",
    "    scaling_meta = {\n",
    "        \"variable\": \"current_speed\",\n",
    "        \"source_vars\": [\"uo\",\"vo\"],\n",
    "        \"units\": \"m s-1\",\n",
    "        \"quantiles\": [LOW_Q, HIGH_Q],\n",
    "        \"global_vmin\": float(global_vmin),\n",
    "        \"global_vmax\": float(global_vmax),\n",
    "        \"nodata_uint8\": NODATA_UINT8,\n",
    "        \"files\": []\n",
    "    }\n",
    "\n",
    "    # Iterate each time slice\n",
    "    T = speed.sizes[\"time\"]\n",
    "    for i in range(T):\n",
    "        slice_t = speed.isel(time=i).squeeze(drop=True)\n",
    "\n",
    "        # Ensure x/y + georeferencing\n",
    "        da = _ensure_xy(slice_t)\n",
    "\n",
    "        # Timestamp label YYYYMM\n",
    "        if \"time\" in slice_t.coords:\n",
    "            tlabel = np.datetime_as_string(slice_t[\"time\"].values, unit=\"D\").replace(\"-\", \"\")[:6]\n",
    "        else:\n",
    "            tlabel = f\"t{i:04d}\"\n",
    "\n",
    "        if WRITE_FLOAT32:\n",
    "            out_path = os.path.join(OUT_DIR, f\"current_speed_{tlabel}_float32.tif\")\n",
    "            da = da.rio.write_nodata(np.nan, inplace=False)\n",
    "            da.rio.to_raster(out_path, dtype=\"float32\", tiled=True, compress=COMPRESS)\n",
    "            print(f\"Wrote {out_path}\")\n",
    "            scaling_meta[\"files\"].append({\"file\": os.path.basename(out_path), \"tindex\": i, \"tlabel\": tlabel})\n",
    "        else:\n",
    "            scaled = _scale_to_uint8_fixed(da, global_vmin, global_vmax, NODATA_UINT8)\n",
    "            out_path = os.path.join(OUT_DIR, f\"current_speed_{tlabel}.tif\")\n",
    "            da8 = xr.DataArray(\n",
    "                scaled, dims=(\"y\",\"x\"), coords={\"y\": da[\"y\"], \"x\": da[\"x\"]},\n",
    "                name=\"current_speed_uint8\",\n",
    "                attrs={\n",
    "                    \"long_name\": \"Ocean current speed (scaled 8-bit, global bounds)\",\n",
    "                    \"units\": \"unitless (0=nodata)\",\n",
    "                    \"scale_vmin\": global_vmin,\n",
    "                    \"scale_vmax\": global_vmax\n",
    "                }\n",
    "            ).rio.write_crs(da.rio.crs).rio.write_transform(da.rio.transform())\n",
    "            da8 = da8.rio.write_nodata(NODATA_UINT8, inplace=False)\n",
    "            da8.rio.to_raster(out_path, dtype=\"uint8\", tiled=True, compress=COMPRESS)\n",
    "            scaling_meta[\"files\"].append({\"file\": os.path.basename(out_path), \"tindex\": i, \"tlabel\": tlabel})\n",
    "            print(f\"Wrote {out_path} using global scaling [{global_vmin:.5f}, {global_vmax:.5f}]\")\n",
    "\n",
    "    # Save sidecar metadata (for legends, rescaling)\n",
    "    if not WRITE_FLOAT32:\n",
    "        meta_path = os.path.join(OUT_DIR, \"current_speed_scaling_metadata.json\")\n",
    "        with open(meta_path, \"w\") as f:\n",
    "            json.dump(scaling_meta, f, indent=2)\n",
    "        print(f\"Saved {meta_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6688b2-41fc-422a-be42-a4e9b90bedd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel",
   "language": "python",
   "name": "claymodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
