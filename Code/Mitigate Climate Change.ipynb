{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a68d863-6d59-4533-a5a1-4f616f8e6808",
   "metadata": {},
   "source": [
    "# Mitigate Climate Change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe067026-4fd8-4760-b32a-46c11dfd2b63",
   "metadata": {},
   "source": [
    "This notebook outlines the general workflow for the data within the [Mitigate Climate Change](https://oceancentral.org/track/mitigate-climate-change) page of the Ocean Central website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c137a64-8efa-4766-a781-d5f48faa57d8",
   "metadata": {},
   "source": [
    "## Utils functions and globals used for making all figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeacb4e-82f3-4b15-9fec-3551c390abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import box\n",
    "import rioxarray\n",
    "import re\n",
    "\n",
    "from rasterio.features import geometry_mask\n",
    "from scipy.stats import linregress\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Open the biodiversity priority areas based on Zhao et al. 2020 (https://www.sciencedirect.com/science/article/abs/pii/S0006320719312182?via%3Dihub)\n",
    "masked_data = rioxarray.open_rasterio('../Data/masked_top_30_percent_over_water.tif')\n",
    "\n",
    "# Set the CRS for masked_data if it's not already set\n",
    "if 'crs' not in masked_data.attrs:\n",
    "    masked_data.rio.write_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "# Load SST dataset and EEZ shapefile\n",
    "seas_shapefile_path = '../Data/World_Seas_IHO_v3/World_Seas_IHO_v3.shp'\n",
    "SEAS_DF = gpd.read_file(seas_shapefile_path)\n",
    "\n",
    "# Calculate linear trend and p-value for each grid point\n",
    "def calculate_trend_and_significance(x):\n",
    "    if np.isnan(x).all():\n",
    "        return np.nan, np.nan, np.nan\n",
    "    else:\n",
    "        slope, intercept, _, p_value, _ = stats.linregress(range(len(x)), x)\n",
    "        return slope, intercept, p_value\n",
    "\n",
    "# Calculate the trend and significance of the trend at each pixel in an xarray dataset\n",
    "def calculate_trend_df(climate_df):\n",
    "    df_mean = climate_df.groupby('time.year').mean()\n",
    "    \n",
    "    # Apply the trend and p-value calculation to the entire dataset\n",
    "    results = xr.apply_ufunc(\n",
    "        calculate_trend_and_significance,\n",
    "        df_mean,\n",
    "        input_core_dims=[['year']],\n",
    "        vectorize=True,\n",
    "        output_core_dims=[[], [], []],\n",
    "        output_dtypes=[float, float, float]\n",
    "    )\n",
    "    \n",
    "    # Extract the trend and p-value into separate DataArrays\n",
    "    trends_da = xr.DataArray(results[0], coords=df_mean.isel(year=0).coords, name='trend')\n",
    "    pvalues_da = xr.DataArray(results[2], coords=df_mean.isel(year=0).coords, name='p_value')\n",
    "    \n",
    "    # Create a significance mask where p-value < 0.05\n",
    "    significant_da = xr.DataArray((pvalues_da < 0.05), coords=pvalues_da.coords, name='significant')\n",
    "    \n",
    "    # Combine trend, p-value, and significance mask into a single dataset\n",
    "    trend_significance_ds = xr.Dataset({\n",
    "        'trend': trends_da,\n",
    "        'p_value': pvalues_da,\n",
    "        'significant': significant_da\n",
    "    })\n",
    "    \n",
    "    # Set the CRS for the trends dataset to match the EEZ CRS\n",
    "    trend_significance_ds = trend_significance_ds.rio.write_crs(\"epsg:4326\")\n",
    "    return trend_significance_ds\n",
    "\n",
    "# Calculate area-weighted trend, significance for each sea/ocean area\n",
    "def area_trend(trend_significance_ds, SEAS_DF=SEAS_DF):\n",
    "    # Iterate over each sea/ocean area and calculate the area-weighted trend and significant area percentage\n",
    "    area_weighted_trends = []\n",
    "    \n",
    "    # Check if 'lat' and 'lon' are in the dataset, otherwise check for 'latitude' and 'longitude'\n",
    "    if 'lat' in trend_significance_ds.dims and 'lon' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'lat': 'y', 'lon': 'x'})\n",
    "    elif 'latitude' in trend_significance_ds.dims and 'longitude' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "\n",
    "    # Interpolate biodiversity priority areas to the same resolution as the climate data\n",
    "    masked_data_interp = masked_data.interp(\n",
    "        x=trend_significance_ds['x'],\n",
    "        y=trend_significance_ds['y'],\n",
    "        method='nearest'\n",
    "    )\n",
    "\n",
    "    # Calculate the area for each grid cell (assumes lat/lon grid)\n",
    "    lat = trend_significance_ds['y'].values\n",
    "    lon = trend_significance_ds['x'].values\n",
    "    \n",
    "    # Calculate grid cell area using Haversine formula or by approximation\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    \n",
    "    # Earth radius in kilometers\n",
    "    R = 6371\n",
    "    dlat = np.gradient(lat_rad)\n",
    "    dlon = np.gradient(lon_rad)\n",
    "    \n",
    "    # Approximate area calculation\n",
    "    cell_areas = (R**2 * np.outer(np.sin(dlat), dlon)) * np.cos(lat_rad[:, None])\n",
    "    \n",
    "    for i, row in SEAS_DF.iterrows():\n",
    "        try:\n",
    "            region_name = row['NAME']\n",
    "            area = row['area']\n",
    "            geom = row['geometry']\n",
    "    \n",
    "            # Mask SST trends with the sea geometry\n",
    "            masked_trends = trend_significance_ds['trend'].rio.clip([geom], drop=True)\n",
    "            masked_significance = trend_significance_ds['significant'].rio.clip([geom], drop=True)\n",
    "    \n",
    "            # Clip cell_areas to the same extent as masked_trends\n",
    "            cell_areas_clipped = xr.DataArray(\n",
    "                cell_areas, \n",
    "                dims=['y', 'x'], \n",
    "                coords={'y': trend_significance_ds['y'], 'x': trend_significance_ds['x']}\n",
    "            )\n",
    "            \n",
    "            # Set CRS for cell_areas_clipped to match the CRS of trend_significance_ds\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.write_crs('EPSG:4326')\n",
    "    \n",
    "            # Clip cell_areas to the same geometry\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.clip([geom], drop=True)\n",
    "        \n",
    "            # Compute the area-weighted trend\n",
    "            weighted_trend = (masked_trends * cell_areas_clipped).sum(dim=('y', 'x')) / cell_areas_clipped.sum()\n",
    "    \n",
    "            # Compute the total area that is significant\n",
    "            significant_masked_areas = (masked_significance * cell_areas_clipped).where(masked_significance, 0)\n",
    "            total_significant_area = significant_masked_areas.sum(dim=('y', 'x')).item()\n",
    "    \n",
    "            # Calculate the percentage of the area that is significant\n",
    "            total_area = cell_areas_clipped.sum()\n",
    "            significant_area_percent = (total_significant_area / total_area) * 100\n",
    "    \n",
    "            # Calculate the area for biodiversity based on the mask\n",
    "            area_biodiversity = ((masked_significance * cell_areas_clipped) * masked_data_interp).sum(dim=['x', 'y']).values\n",
    "    \n",
    "            # Store the result\n",
    "            area_weighted_trends.append({\n",
    "                'Region_Name': region_name,\n",
    "                'geometry': geom,\n",
    "                'Weighted_Trend': weighted_trend.item(),\n",
    "                'Sea_Area': area,\n",
    "                'Significant_Area': area*total_significant_area/total_area.item(),\n",
    "                'Significant_Area_Percent': 100*total_significant_area/total_area.item(),\n",
    "                'Biodiversity_Area': area*area_biodiversity[0]/total_area.item(),\n",
    "                'Biodiversity_Area_Percent': 100*area_biodiversity[0]/total_area.item(),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Convert the results to a GeoDataFrame for easy viewing\n",
    "    area_weighted_trends_gdf = gpd.GeoDataFrame(area_weighted_trends, crs=SEAS_DF.crs)\n",
    "    return area_weighted_trends_gdf\n",
    "\n",
    "def area_heatwave(temp_df, SEAS_DF=SEAS_DF):\n",
    "    area_heatwave = []\n",
    "\n",
    "    # Set CRS and rename dimensions and coordinates\n",
    "    temp_df = temp_df.rio.write_crs(\"epsg:4326\")\n",
    "    temp_df = temp_df.rename({'latdim': 'y', 'londim': 'x'}).rename({'lat': 'y', 'lon': 'x'})  # Adjust based on your dimensions\n",
    "\n",
    "    # Select heatwave categories >= 3 and aggregate over time\n",
    "    temp_df = (temp_df['heatwave_category'] >= 3).any(dim='time')\n",
    "\n",
    "    # Interpolate biodiversity priority areas to the same resolution as the climate data\n",
    "    masked_data_interp = masked_data.interp(\n",
    "        x=temp_df['x'],\n",
    "        y=temp_df['y'],\n",
    "        method='nearest'\n",
    "    )\n",
    "\n",
    "    # Calculate the area for each grid cell (assumes lat/lon grid)\n",
    "    lat = temp_df['y'].values\n",
    "    lon = temp_df['x'].values\n",
    "    \n",
    "    # Calculate grid cell area using Haversine formula or by approximation\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    \n",
    "    # Earth radius in kilometers\n",
    "    R = 6371\n",
    "    dlat = np.gradient(lat_rad)\n",
    "    dlon = np.gradient(lon_rad)\n",
    "    \n",
    "    # Approximate area calculation\n",
    "    cell_areas = (R**2 * np.outer(np.sin(dlat), dlon)) * np.cos(lat_rad[:, None])\n",
    "    \n",
    "    # Use tqdm to track progress through SEAS_DF.iterrows()\n",
    "    for i, row in tqdm(SEAS_DF.iterrows(), total=len(SEAS_DF), desc=\"Processing Sea Areas\"):\n",
    "        try:\n",
    "            region_name = row['NAME']\n",
    "            area = row['area']\n",
    "            geom = row['geometry']\n",
    "    \n",
    "            # Mask SST trends with the sea geometry\n",
    "            masked_df = temp_df.rio.clip([geom], drop=True)\n",
    "    \n",
    "            # Clip cell_areas to the same extent as masked_df\n",
    "            cell_areas_clipped = xr.DataArray(\n",
    "                cell_areas, \n",
    "                dims=['y', 'x'], \n",
    "                coords={'y': temp_df['y'], 'x': temp_df['x']}\n",
    "            )\n",
    "            \n",
    "            # Set CRS for cell_areas_clipped to match the CRS of trend_significance_ds\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.write_crs('EPSG:4326')\n",
    "    \n",
    "            # Clip cell_areas to the same geometry\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.clip([geom], drop=True)\n",
    "        \n",
    "            # Compute the total area that is impacted by a severe heatwave\n",
    "            heatwave_area = (masked_df * cell_areas_clipped).sum(dim=('y', 'x')).compute()  # Compute to convert from Dask array\n",
    "    \n",
    "            # Calculate the area for biodiversity based on the mask\n",
    "            area_biodiversity = ((masked_df * cell_areas_clipped) * masked_data_interp).sum(dim=['x', 'y']).compute()\n",
    "\n",
    "            total_area = cell_areas_clipped.sum(dim=('y', 'x')).compute()  # Ensure computation\n",
    "    \n",
    "            # Extract values after computing\n",
    "            heatwave_value = heatwave_area.item() if heatwave_area.size == 1 else heatwave_area.values[0]\n",
    "            total_area_value = total_area.item() if total_area.size == 1 else total_area.values[0]\n",
    "            area_biodiversity = area_biodiversity.item() if area_biodiversity.size == 1 else area_biodiversity.values[0]\n",
    "    \n",
    "            # Store the result\n",
    "            area_heatwave.append({\n",
    "                'Region_Name': region_name,\n",
    "                'geometry': geom,\n",
    "                'Heatwave_Area': area*heatwave_value/total_area_value,\n",
    "                'Heatwave_Area_Percent': 100*heatwave_value/total_area_value,\n",
    "                'Sea_Area': area,\n",
    "                'Biodiversity_Area': area*area_biodiversity/total_area_value,\n",
    "                'Biodiversity_Area_Percent': 100*area_biodiversity/total_area_value,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {region_name}: {e}\")\n",
    "\n",
    "    # Convert the results to a GeoDataFrame for easy viewing\n",
    "    area_heatwave_gdf = gpd.GeoDataFrame(area_heatwave, crs=SEAS_DF.crs)\n",
    "    return area_heatwave_gdf\n",
    "\n",
    "\n",
    "def area_coral_acidification_by_sea(\n",
    "    trend_significance_ds,\n",
    "    coral_gdf,\n",
    "    SEAS_DF,\n",
    "    all_touched=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Coral acidification exposure by marine area (IHO seas),\n",
    "    using raster-based fraction × true coral area.\n",
    "\n",
    "    Returns GeoDataFrame with one row per sea.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # ---------------------------\n",
    "    # 1. Prepare raster\n",
    "    # ---------------------------\n",
    "    ds = trend_significance_ds.copy().rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    if \"lat\" in ds.dims and \"lon\" in ds.dims:\n",
    "        ds = ds.rename({\"lat\": \"y\", \"lon\": \"x\"})\n",
    "\n",
    "    sig = ds[\"significant\"].astype(bool)\n",
    "\n",
    "    lat = ds[\"y\"].values\n",
    "    lon = ds[\"x\"].values\n",
    "\n",
    "    transform = Affine(\n",
    "        lon[1] - lon[0], 0, lon.min(),\n",
    "        0, lat[0] - lat[1], lat.max()\n",
    "    )\n",
    "\n",
    "    # ---------------------------\n",
    "    # 2. Prepare coral polygons\n",
    "    # ---------------------------\n",
    "    corals = coral_gdf.to_crs(\"EPSG:4326\").copy()\n",
    "    corals[\"geometry\"] = corals.geometry.buffer(0)\n",
    "\n",
    "    # Equal-area version for TRUE coral area\n",
    "    corals_eq = corals.to_crs(\"ESRI:54009\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # 3. Loop over seas\n",
    "    # ---------------------------\n",
    "    for _, row in tqdm(SEAS_DF.iterrows(), total=len(SEAS_DF), desc=\"Processing Seas\"):\n",
    "\n",
    "        sea_name = row[\"NAME\"]\n",
    "        sea_geom = row[\"geometry\"]\n",
    "\n",
    "        try:\n",
    "            # -----------------------------------\n",
    "            # A. Coral geometry within this sea\n",
    "            # -----------------------------------\n",
    "            coral_sea = gpd.overlay(\n",
    "                corals,\n",
    "                gpd.GeoDataFrame(geometry=[sea_geom], crs=\"EPSG:4326\"),\n",
    "                how=\"intersection\",\n",
    "            )\n",
    "\n",
    "            if coral_sea.empty:\n",
    "                continue\n",
    "\n",
    "            # True coral area in this sea (km²)\n",
    "            coral_sea_eq = coral_sea.to_crs(\"ESRI:54009\")\n",
    "            true_coral_area_km2 = coral_sea_eq.geometry.area.sum() / 1e6\n",
    "\n",
    "            # -----------------------------------\n",
    "            # B. Rasterize coral mask (sea-limited)\n",
    "            # -----------------------------------\n",
    "            coral_mask = rasterize(\n",
    "                [(g, 1) for g in coral_sea.geometry if not g.is_empty],\n",
    "                out_shape=(len(lat), len(lon)),\n",
    "                transform=transform,\n",
    "                fill=0,\n",
    "                all_touched=all_touched,\n",
    "                dtype=\"uint8\",\n",
    "            ).astype(bool)\n",
    "\n",
    "            if not coral_mask.any():\n",
    "                continue\n",
    "\n",
    "            # -----------------------------------\n",
    "            # C. Clip significance raster to sea\n",
    "            # -----------------------------------\n",
    "            sig_sea = sig.rio.clip([sea_geom], drop=False).values.astype(bool)\n",
    "\n",
    "            # -----------------------------------\n",
    "            # D. Fraction of coral pixels affected\n",
    "            # -----------------------------------\n",
    "            coral_pixels = coral_mask\n",
    "            affected_pixels = coral_mask & sig_sea\n",
    "\n",
    "            affected_fraction = (\n",
    "                affected_pixels.sum() / coral_pixels.sum()\n",
    "                if coral_pixels.sum() > 0 else np.nan\n",
    "            )\n",
    "\n",
    "            affected_coral_area_km2 = affected_fraction * true_coral_area_km2\n",
    "\n",
    "            # -----------------------------------\n",
    "            # E. Store\n",
    "            # -----------------------------------\n",
    "            results.append({\n",
    "                \"Region_Name\": sea_name,\n",
    "                \"True_Coral_Area_km2\": true_coral_area_km2,\n",
    "                \"Affected_Coral_Area_km2\": affected_coral_area_km2,\n",
    "                \"Affected_Coral_Area_Percent\": affected_fraction * 100,\n",
    "                \"geometry\": sea_geom,\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {sea_name}: {e}\")\n",
    "\n",
    "    return gpd.GeoDataFrame(results, crs=SEAS_DF.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5164ba-c570-424c-a810-d7f172f3de88",
   "metadata": {},
   "source": [
    "# Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c00d4-77cc-4782-a181-9f13bf4e6a9f",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca8ca4-21b0-469a-aefe-266348bb6091",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_1.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c41e63-e685-4d74-9e18-70249b5c02b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_data = pd.read_csv(\"../Data/GISTEMP_SST.csv\") # Data downloaded from GISS Surface Temperature Analysis (v4)\n",
    "gmst_data = pd.read_csv(\"../Data/GMST_GISTEMP4.csv\") # Data downloaded from GISS Surface Temperature Analysis (v4)\n",
    "\n",
    "ocean_data['Ocean_Annual'] = ocean_data['Lowess(5)']\n",
    "gmst_data['GMST_Annual'] = gmst_data['Lowess(5)']\n",
    "\n",
    "temp_data = ocean_data.merge(gmst_data,on='Year')\n",
    "\n",
    "# Calculate the mean of the 'No_Smoothing' column for the period 1880-1900\n",
    "base_period = temp_data[(temp_data['Year'] >= 1880) & (temp_data['Year'] <= 1900)]\n",
    "mean_sst_base_period = base_period['Ocean_Annual'].mean()\n",
    "mean_gmst_base_period = base_period['GMST_Annual'].mean()\n",
    "\n",
    "# Update the 'No_Smoothing' column to be anomalies relative to the period 1880-1900\n",
    "temp_data['Ocean_Annual'] = temp_data['Ocean_Annual'] - mean_sst_base_period\n",
    "temp_data['GMST_Annual'] = temp_data['GMST_Annual'] - mean_gmst_base_period\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(temp_data['Year'], temp_data['Ocean_Annual'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "temp_data['ocean_trend'] = intercept + slope * temp_data['Year']\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(temp_data['Year'], temp_data['GMST_Annual'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "temp_data['gmst_trend'] = intercept + slope * temp_data['Year']\n",
    "temp_data['paris_goal'] = 1.5\n",
    "\n",
    "# Save out as JSON\n",
    "temp_data[['Year','Ocean_Annual','ocean_trend','GMST_Annual','gmst_trend','paris_goal']].to_json(\n",
    "    \"../Data/Figure_1_temperature.json\", \n",
    "    orient=\"records\",  # list of dicts (one per row)\n",
    "    indent=2           # pretty print\n",
    ")\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "temp_data[['Year','Ocean_Annual','ocean_trend','GMST_Annual','gmst_trend','paris_goal']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126ca5c-8d7f-47e6-bf6e-d56ffaa63fe4",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea8702-867b-43ed-b85c-f0289d80a964",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_2.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5f7f6-5fd2-4431-a9ba-f0866c389e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "trends_df = xr.open_dataset(\"../Data/global_omi_tempsal_sst_trend_19932021_P20220331.nc\")\n",
    "\n",
    "# --------------------\n",
    "# CONFIG\n",
    "# --------------------\n",
    "OUTPUT_TIF_8BIT   = \"../Data/Figure_2_temperature.tif\"\n",
    "OUTPUT_TIF_FLOAT  = \"../Data/sst_trend_float32.tif\"\n",
    "USE_PERCENTILES   = True\n",
    "P_LOW, P_HIGH     = 2, 98\n",
    "VMIN_FIXED, VMAX_FIXED = -0.5, 0.5  # if USE_PERCENTILES=False\n",
    "\n",
    "# 1) Select the dataarray\n",
    "da = trends_df[\"sst_trends\"]\n",
    "if \"time\" in da.dims:\n",
    "    da = da.mean(dim=\"time\")\n",
    "\n",
    "# 2) Rename to x/y if needed\n",
    "rename_map = {}\n",
    "if \"lat\" in da.dims or \"lat\" in da.coords: rename_map[\"lat\"] = \"y\"\n",
    "if \"latitude\" in da.dims or \"latitude\" in da.coords: rename_map[\"latitude\"] = \"y\"\n",
    "if \"lon\" in da.dims or \"lon\" in da.coords: rename_map[\"lon\"] = \"x\"\n",
    "if \"longitude\" in da.dims or \"longitude\" in da.coords: rename_map[\"longitude\"] = \"x\"\n",
    "if rename_map:\n",
    "    da = da.rename(rename_map)\n",
    "\n",
    "# 3) Ensure y is north→south (descending)\n",
    "if da[\"y\"].values[0] < da[\"y\"].values[-1]:\n",
    "    da = da.sortby(\"y\", ascending=False)\n",
    "\n",
    "# 4) Register spatial dims & CRS\n",
    "da = da.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "if da.rio.crs is None:\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "\n",
    "# 5) Prepare data and scaling\n",
    "data = da.data.astype(np.float32)\n",
    "valid = np.isfinite(data)\n",
    "\n",
    "if USE_PERCENTILES:\n",
    "    vmin = float(np.nanpercentile(data, P_LOW))\n",
    "    vmax = float(np.nanpercentile(data, P_HIGH))\n",
    "else:\n",
    "    vmin, vmax = float(VMIN_FIXED), float(VMAX_FIXED)\n",
    "\n",
    "if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "    raise ValueError(f\"Bad scaling range: vmin={vmin}, vmax={vmax}\")\n",
    "\n",
    "# 6) Build the 8-bit layer (reserve 0 for NoData)\n",
    "scaled = np.zeros_like(data, dtype=np.uint8)  # 0 = NoData\n",
    "scaled_valid = (np.clip((data[valid] - vmin) / (vmax - vmin), 0.0, 1.0) * 254 + 1).astype(np.uint8)\n",
    "scaled[valid] = scaled_valid\n",
    "\n",
    "da8 = da.copy(data=scaled)\n",
    "\n",
    "# --- CRITICAL: clear CF encodings that carry a massive _FillValue ---\n",
    "da8.encoding.pop(\"_FillValue\", None)\n",
    "da8.encoding.pop(\"missing_value\", None)\n",
    "# (optional) also clear scale/offset if present\n",
    "da8.encoding.pop(\"scale_factor\", None)\n",
    "da8.encoding.pop(\"add_offset\", None)\n",
    "\n",
    "# Set nodata appropriate for uint8 (0)\n",
    "da8 = da8.rio.write_nodata(0, encoded=False, inplace=False)\n",
    "\n",
    "# 7a) Write 8-bit GeoTIFF\n",
    "da8.rio.to_raster(\n",
    "    OUTPUT_TIF_8BIT,\n",
    "    dtype=\"uint8\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "# 7b) Optional: write float32 with native values\n",
    "daf = da.where(np.isfinite(da))\n",
    "# Clear encodings here too\n",
    "daf.encoding.pop(\"_FillValue\", None)\n",
    "daf.encoding.pop(\"missing_value\", None)\n",
    "daf.encoding.pop(\"scale_factor\", None)\n",
    "daf.encoding.pop(\"add_offset\", None)\n",
    "\n",
    "# Use NaN as nodata for float32 (supported by rasterio/GTiff)\n",
    "daf = daf.rio.write_nodata(np.nan, encoded=False, inplace=False)\n",
    "\n",
    "daf.rio.to_raster(\n",
    "    OUTPUT_TIF_FLOAT,\n",
    "    dtype=\"float32\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Wrote {OUTPUT_TIF_8BIT} (uint8; 0=NoData, 1–255=data) and {OUTPUT_TIF_FLOAT} (float32). \"\n",
    "    f\"Scale for 8-bit: vmin={vmin:.4f}, vmax={vmax:.4f} (units of sst_trends).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a6928-80c1-41f0-8931-f136ed10e05f",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a55a1-632f-44d0-8e46-1897367aefa7",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_3.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c1715-7776-4190-9862-0da292a3576f",
   "metadata": {},
   "source": [
    "**This figure calculates the total area for each major sea region globally. It also calculates 1) the area for each sea region impacted by a severe marine heatwave in the year 2023 as well as 2) the area for each sea that both has priority biodiversity areas AND is impacted by a severe marine heatwave.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7329b-be8f-44d0-ba51-8efb83291da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2024 Heatwave data downloaded from NOAA's Coral Reef Watch https://coralreefwatch.noaa.gov/product/marine_heatwave/\n",
    "import xarray as xr\n",
    "import glob\n",
    "\n",
    "files = sorted(glob.glob(\"../Data/2023/*.nc\"))\n",
    "\n",
    "temp_df = xr.open_mfdataset(\"../Data/2023/*.nc\")\n",
    "\n",
    "area_df = area_heatwave(temp_df)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/Figure_3_temperature.geojson\",driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8102994c-1985-4949-a649-f91af99fdaad",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d2692-6a48-4d4e-bb91-66f9922df628",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_4.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f751c1c-b9d9-4d33-8808-06e75751cd34",
   "metadata": {},
   "source": [
    "$CO_2$ data retrieved from [Lan, X., P. Tans, & K.W. Thoning (2025). Trends in globally-averaged CO₂ determined from NOAA Global Monitoring Laboratory measurements (Version 2025-11) NOAA Global Monitoring Laboratory. https://doi.org/10.15138/9N0H-ZH07](https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.txt). Temperature data retrieved from GISS Surface Temperature Analysis (v4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ec2a1-6206-4554-82a1-3481eba4ff91",
   "metadata": {},
   "source": [
    "## Figure 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94da805-3110-4b81-983f-81d7d337f510",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_5.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc173db-e178-4e15-adf9-e93113e8bd23",
   "metadata": {},
   "source": [
    "Data were retrieved from [Hughes, T. P., et al. (2018). Spatial and temporal patterns of mass bleaching of corals in the Anthropocene. Science. – processed by Our World in Data](https://ourworldindata.org/grapher/coral-bleaching-events)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e83a16-7adb-4cc2-9e13-c146d714186c",
   "metadata": {},
   "source": [
    "## Figure 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c497b-be92-4549-b3b6-54eb7d8a4112",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_6.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b62bdc-ee90-488c-9957-35ea1cb62707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import xarray as xr\n",
    "import rioxarray  # pip install rioxarray rasterio\n",
    "\n",
    "# --- Load and compute heatwave days ---\n",
    "ds = xr.open_mfdataset(\"../Data/2023/*.nc\", combine=\"by_coords\")\n",
    "\n",
    "# If you already have temp_df, you can instead do:\n",
    "# hw_count = temp_df\n",
    "\n",
    "hw_count = (ds[\"heatwave_category\"] >= 1).sum(dim=\"time\")\n",
    "hw_count = hw_count.astype(\"float32\")\n",
    "\n",
    "# --- Rechunk along spatial dims for quantile ---\n",
    "# Identify spatial dimensions (everything except 'time')\n",
    "spatial_dims = [d for d in hw_count.dims if d != \"time\"]\n",
    "\n",
    "# Make each spatial dimension a single chunk\n",
    "hw_q = hw_count.chunk({d: -1 for d in spatial_dims})\n",
    "\n",
    "# --- Compute 2nd and 98th percentiles over space ---\n",
    "q = hw_q.quantile([0.02, 0.98], dim=spatial_dims, skipna=True).compute()\n",
    "\n",
    "p2 = float(q.sel(quantile=0.02).values)\n",
    "p98 = float(q.sel(quantile=0.98).values)\n",
    "\n",
    "print(f\"2nd percentile: {p2}, 98th percentile: {p98}\")\n",
    "\n",
    "if p98 <= p2:\n",
    "    raise ValueError(\"98th percentile is not greater than 2nd percentile; cannot scale safely.\")\n",
    "\n",
    "# --- Scale to 0–255 using the 2nd and 98th percentiles ---\n",
    "scaled = (hw_count - p2) / (p98 - p2) * 255.0\n",
    "scaled = scaled.clip(0, 255)\n",
    "scaled = scaled.fillna(0)\n",
    "\n",
    "# Round and cast to uint8\n",
    "scaled_uint8 = scaled.round().astype(\"uint8\")\n",
    "\n",
    "# --- Set spatial dims and CRS for Mapbox ---\n",
    "# Try to guess your x/y dimension names\n",
    "cands_x = [\"lon\", \"longitude\", \"x\", \"londim\"]\n",
    "cands_y = [\"lat\", \"latitude\", \"y\", \"latdim\"]\n",
    "\n",
    "x_dim = next(d for d in cands_x if d in scaled_uint8.dims)\n",
    "y_dim = next(d for d in cands_y if d in scaled_uint8.dims)\n",
    "\n",
    "scaled_uint8 = scaled_uint8.rio.set_spatial_dims(x_dim=x_dim, y_dim=y_dim, inplace=False)\n",
    "\n",
    "# Ensure WGS84\n",
    "if not scaled_uint8.rio.crs:\n",
    "    scaled_uint8 = scaled_uint8.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "# Optional: use 0 as nodata for transparency in Mapbox\n",
    "scaled_uint8 = scaled_uint8.rio.write_nodata(0)\n",
    "\n",
    "# --- Save as GeoTIFF ---\n",
    "out_path = \"Figure_6_temperature.tif\"\n",
    "scaled_uint8.rio.to_raster(out_path, dtype=\"uint8\")\n",
    "\n",
    "print(f\"Saved GeoTIFF: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54339b03-3b65-417a-b87d-2347c4a693f7",
   "metadata": {},
   "source": [
    "## Figure 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a29b4b-602e-477f-a151-d1c6f0b88748",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_7.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ffe68-b87c-4c87-b449-d6d27da859aa",
   "metadata": {},
   "source": [
    "Data were retrieved from [Jones et al. (2024) – with major processing by Our World in Data](https://ourworldindata.org/co2-and-greenhouse-gas-emissions#explore-data-on-co2-and-greenhouse-gas-emissions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994324d-af2f-49da-9a05-180d9bf5d7e1",
   "metadata": {},
   "source": [
    "## Figure 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d583d8f-6ce5-42d0-b1e9-2e651937944d",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_8.png\" style=\"width:50%;\">\n",
    "</p>\n",
    "\n",
    "Data were retrieved from [van Woesik and Kratochwill.](https://springernature.figshare.com/articles/dataset/Global_Coral_Bleaching_Database/17076290?backTo=%2Fcollections%2FA_Global_Coral-Bleaching_Database_GCBD_1998_2020%2F5314466&file=31573496)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1f78d-daca-4ecb-b8c3-da46dc209097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- paths ---\n",
    "db_path = \"../Data/Global_Coral_Bleaching_Database_SQLite_11_24_21.db\"\n",
    "out_geojson = \"../Data/Figure_8_temperature.geojson\"\n",
    "\n",
    "# --- connect to the SQLite database ---\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    s.Site_ID,\n",
    "    s.Site_Name,\n",
    "    s.Latitude_Degrees AS lat,\n",
    "    s.Longitude_Degrees AS lon\n",
    "FROM Site_Info_tbl s\n",
    "JOIN Sample_Event_tbl se ON s.Site_ID = se.Site_ID\n",
    "JOIN Bleaching_tbl b ON se.Sample_ID = b.Sample_ID\n",
    "WHERE b.Percent_Bleached IS NOT NULL\n",
    "  AND b.Percent_Bleached > 0\n",
    "  AND s.Latitude_Degrees IS NOT NULL\n",
    "  AND s.Longitude_Degrees IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "bleach_sites = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Found {len(bleach_sites)} sites with bleaching records.\")\n",
    "\n",
    "# --- fix bytes columns (decode to str) ---\n",
    "for col in bleach_sites.columns:\n",
    "    if bleach_sites[col].dtype == object:\n",
    "        bleach_sites[col] = bleach_sites[col].apply(\n",
    "            lambda v: v.decode(\"utf-8\", errors=\"ignore\") if isinstance(v, (bytes, bytearray)) else v\n",
    "        )\n",
    "\n",
    "# (Optional) keep just a few clean columns\n",
    "# bleach_sites = bleach_sites[[\"Site_ID\", \"Site_Name\", \"lat\", \"lon\"]].copy()\n",
    "\n",
    "# --- convert to GeoDataFrame ---\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    bleach_sites,\n",
    "    geometry=gpd.points_from_xy(bleach_sites[\"lon\"], bleach_sites[\"lat\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# --- save as GeoJSON ---\n",
    "gdf.to_file(out_geojson, driver=\"GeoJSON\")\n",
    "print(f\"Saved to {out_geojson}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e24ffbc-d1f7-4940-a42d-67c0f0cb4a2b",
   "metadata": {},
   "source": [
    "# Salinity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5b29e-d916-4d88-a01b-9cb7f4a3a7fc",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9090c02-0d5c-4ed5-bc90-30deb056defe",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_salinity_1.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc03ef6-b653-46bf-9a5d-01ba19a94020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "salt_df = xr.open_dataset(\"../Data/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "salt_df = salt_df['salinity'].mean(dim=['lat','lon']).resample(time='Y').mean()\n",
    "\n",
    "final_subset = salt_df.sel(time=slice('1994-01-01', None))\n",
    "\n",
    "# Create a pandas DataFrame with these columns\n",
    "df = pd.DataFrame({\n",
    "    'time': final_subset['time'].values,\n",
    "    'salinity': final_subset.values,\n",
    "})\n",
    "\n",
    "# Convert 'time' to datetime\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Convert datetime to a numerical value for linear regression (using ordinal format)\n",
    "df['time_ordinal'] = df['time'].map(pd.Timestamp.toordinal)\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(df['time_ordinal'], df['salinity'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "df['linear_trend'] = intercept + slope * df['time_ordinal']\n",
    "\n",
    "df[['time','salinity','linear_trend']].to_csv(\"../Data/Figure_1_salinity.csv\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df[['time','salinity','linear_trend']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af9036-4fe4-4c9a-83c7-215181777c26",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f90f8-5b07-4765-adf3-c00f41db7f1b",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_salinity_2.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ee663-1312-494c-b02d-5f8c0127bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "salt_df = xr.open_dataset(\"../Data/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(salt_df['salinity'])\n",
    "\n",
    "# --------------------\n",
    "# CONFIG\n",
    "# --------------------\n",
    "OUTPUT_TIF_8BIT   = \"./Figure_2_salinity.tif\"\n",
    "OUTPUT_TIF_FLOAT  = \"./salinity_trend_float32.tif\"\n",
    "USE_PERCENTILES   = True\n",
    "P_LOW, P_HIGH     = 2, 98\n",
    "VMIN_FIXED, VMAX_FIXED = -0.5, 0.5  # if USE_PERCENTILES=False\n",
    "\n",
    "# 1) Select the dataarray: salinity trend\n",
    "da = trend_significance_ds['trend']\n",
    "\n",
    "# 2) Rename to x/y if needed\n",
    "rename_map = {}\n",
    "if \"lat\" in da.dims or \"lat\" in da.coords: rename_map[\"lat\"] = \"y\"\n",
    "if \"latitude\" in da.dims or \"latitude\" in da.coords: rename_map[\"latitude\"] = \"y\"\n",
    "if \"lon\" in da.dims or \"lon\" in da.coords: rename_map[\"lon\"] = \"x\"\n",
    "if \"longitude\" in da.dims or \"longitude\" in da.coords: rename_map[\"longitude\"] = \"x\"\n",
    "if rename_map:\n",
    "    da = da.rename(rename_map)\n",
    "\n",
    "# 3) Ensure y is north→south (descending)\n",
    "if da[\"y\"].values[0] < da[\"y\"].values[-1]:\n",
    "    da = da.sortby(\"y\", ascending=False)\n",
    "# ensure (y, x) order\n",
    "if tuple(da.dims) != (\"y\", \"x\"):\n",
    "    da = da.transpose(\"y\", \"x\")\n",
    "\n",
    "# 4) Register spatial dims & force EPSG:4326\n",
    "da = da.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "\n",
    "# If CRS is missing, assume geographic lon/lat; then force-reproject to EPSG:4326\n",
    "if da.rio.crs is None:\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "\n",
    "# If CRS is not 4326, reproject it so the saved rasters are truly EPSG:4326\n",
    "if str(da.rio.crs) != \"EPSG:4326\":\n",
    "    # Use nearest for categorical-like; bilinear is typical for continuous SLA\n",
    "    da = da.rio.reproject(\"EPSG:4326\", resampling=rioxarray.rio.reproject.Resampling.bilinear)\n",
    "\n",
    "# Reassert dims order after any reprojection (just in case)\n",
    "if tuple(da.dims) != (\"y\", \"x\"):\n",
    "    da = da.transpose(\"y\", \"x\")\n",
    "\n",
    "\n",
    "# 5) Compute scaling range (Dask-safe)\n",
    "import dask.array as dsa\n",
    "\n",
    "def compute_percentiles_safe(da, p_low, p_high):\n",
    "    \"\"\"\n",
    "    Try dask.array.nanpercentile; if that fails (older dask/xarray),\n",
    "    fall back to a coarse sample to keep memory in check.\n",
    "    \"\"\"\n",
    "    if getattr(da, \"chunks\", None):\n",
    "        try:\n",
    "            vmin = float(dsa.nanpercentile(da.data, p_low).compute())\n",
    "            vmax = float(dsa.nanpercentile(da.data, p_high).compute())\n",
    "            return vmin, vmax\n",
    "        except Exception:\n",
    "            pass  # fall back to sampled approach\n",
    "\n",
    "    # Fallback: sample every Nth pixel (keeps memory tiny)\n",
    "    step_y = max(int(len(da.y) // 512), 1) if \"y\" in da.dims else 4\n",
    "    step_x = max(int(len(da.x) // 512), 1) if \"x\" in da.dims else 4\n",
    "    das = da.isel(\n",
    "        y=slice(0, None, step_y) if \"y\" in da.dims else slice(None),\n",
    "        x=slice(0, None, step_x) if \"x\" in da.dims else slice(None),\n",
    "    ).load()  # small enough to load\n",
    "    vmin = float(np.nanpercentile(das.values, p_low))\n",
    "    vmax = float(np.nanpercentile(das.values, p_high))\n",
    "    return vmin, vmax\n",
    "\n",
    "if USE_PERCENTILES:\n",
    "    vmin, vmax = compute_percentiles_safe(da, P_LOW, P_HIGH)\n",
    "else:\n",
    "    vmin, vmax = float(VMIN_FIXED), float(VMAX_FIXED)\n",
    "\n",
    "if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "    raise ValueError(f\"Bad scaling range: vmin={vmin}, vmax={vmax}\")\n",
    "\n",
    "\n",
    "# 6) Build the 8-bit layer with xr.where (no boolean indexing)\n",
    "# Reserve 0 for NoData; valid cells map to [1,255]\n",
    "norm = ((da - vmin) / (vmax - vmin)).clip(0, 1)\n",
    "scaled_da = xr.where(np.isfinite(da), norm * 254.0 + 1.0, 0.0).astype(\"uint8\")\n",
    "\n",
    "# Clear troublesome encodings on the uint8 view\n",
    "for k in (\"_FillValue\", \"missing_value\", \"scale_factor\", \"add_offset\"):\n",
    "    scaled_da.encoding.pop(k, None)\n",
    "\n",
    "# Set nodata appropriate for uint8 (0)\n",
    "scaled_da = scaled_da.rio.write_nodata(0, encoded=False, inplace=False)\n",
    "\n",
    "# 7a) Write 8-bit GeoTIFF (works with Dask; will stream-chunk if array is chunked)\n",
    "scaled_da.rio.to_raster(\n",
    "    OUTPUT_TIF_8BIT,\n",
    "    dtype=\"uint8\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "# 7b) Write float32 GeoTIFF with native values\n",
    "daf = da.where(np.isfinite(da)).astype(\"float32\")\n",
    "for k in (\"_FillValue\", \"missing_value\", \"scale_factor\", \"add_offset\"):\n",
    "    daf.encoding.pop(k, None)\n",
    "# Use NaN as nodata for float32\n",
    "daf = daf.rio.write_nodata(np.nan, encoded=False, inplace=False)\n",
    "\n",
    "daf.rio.to_raster(\n",
    "    OUTPUT_TIF_FLOAT,\n",
    "    dtype=\"float32\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Wrote {OUTPUT_TIF_8BIT} (uint8; 0=NoData, 1–255=data) and {OUTPUT_TIF_FLOAT} (float32). \"\n",
    "    f\"8-bit scale: vmin={vmin:.4f}, vmax={vmax:.4f} (native SLA units).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09894f6-aa65-4b23-b39a-f45e51671fe3",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40a8ae-1476-4a1e-b6f5-1146e1f66587",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_salinity_3.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c66766-7acd-46ad-bbbc-a57a0d54344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_df = xr.open_dataset(\"../Data/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(salt_df['salinity'])\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/Figure_3_salinity.geojson\",driver=\"GeoJSON\")\n",
    "\n",
    "del salt_df, area_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e07c51-729b-4c85-84b7-dbf550e3014a",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdaa8d3-49c1-4d5c-8a75-e6f5ad41d77a",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_salinity_4.png\" style=\"width:50%;\">\n",
    "</p>\n",
    "\n",
    "**This figure reproduces Figure 3A from [Delworth et al. (2022)](https://www.pnas.org/doi/10.1073/pnas.2116655119). It plots an index of the Atlantic Meridional Overturning Circulation (AMOC) and how it changes over time. There are 6 scenarios --- 2 of which have historical data (\"HISTORICAL\" (representing a historical evolution of anthropogenic emissions) and \"NATURAL\" (no changes to anthropogenic emissions)), and 5 have projections (\"NATURAL\" again, as well as different Shared Socioeconomic Pathway (SSP) scenarios for how emissions will change in the future). Each scenario is run several times, which provides the measures of uncertainty presented in the figure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0806b72-69d8-4cd6-abb5-910daf23d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the file with more flexibility in handling its format to try and correctly parse it.\n",
    "with open('../Data/TAR_FIGURE_3_AMOC_45N', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Adjust the column naming logic to include the full scenario name as requested\n",
    "scenario_data = {}\n",
    "current_scenario = None\n",
    "time_range = range(1921, 2100)\n",
    "\n",
    "for line in lines:\n",
    "    # Check if line indicates a new scenario/ensemble using the SPEAR_c192_o1 pattern\n",
    "    scenario_match = re.search(r'SPEAR_c192_o1_(.+)_ENS_(\\d+)', line)\n",
    "    if scenario_match:\n",
    "        # Preserve the full scenario name (like HIST_SSP585_ALLForc) and ensemble number\n",
    "        scenario_name = scenario_match.group(1) + \"_\" + scenario_match.group(2)\n",
    "        current_scenario = scenario_name\n",
    "        scenario_data[current_scenario] = []\n",
    "    else:\n",
    "        # If the line contains numerical data, extract and add it to the current scenario\n",
    "        match = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line.strip())\n",
    "        if match and current_scenario:\n",
    "            scenario_data[current_scenario].extend([float(value) for value in match])\n",
    "\n",
    "# Creating a DataFrame where the first column is the time and each subsequent column is a scenario/ensemble\n",
    "df = pd.DataFrame({'Year': list(time_range)})\n",
    "\n",
    "for scenario, values in scenario_data.items():\n",
    "    df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
    "\n",
    "# Define scenario prefixes to filter the columns\n",
    "scenarios = ['SSP119', 'SSP245', 'SSP534', 'SSP585', 'NATURAL']\n",
    "\n",
    "# Initialize a result DataFrame\n",
    "result = pd.DataFrame()\n",
    "result['Year'] = df['Year']\n",
    "\n",
    "# Loop through each scenario to calculate the mean and confidence intervals\n",
    "for scenario in scenarios:\n",
    "    # Identify relevant columns for the current scenario\n",
    "    scenario_columns = [col for col in df.columns if scenario in col]\n",
    "    \n",
    "    if scenario_columns:  # Ensure there are columns for this scenario\n",
    "        # Calculate mean and confidence intervals\n",
    "        result[f'{scenario}_Mean'] = df[scenario_columns].mean(axis=1)\n",
    "        result[f'{scenario}_CI_Lower'] = df[scenario_columns].quantile(0.05, axis=1)\n",
    "        result[f'{scenario}_CI_Upper'] = df[scenario_columns].quantile(0.95, axis=1)\n",
    "\n",
    "# Create historical scenario columns by aggregating the SSP scenarios up to the year 2014\n",
    "historical_columns = ['SSP119', 'SSP245', 'SSP534', 'SSP585']\n",
    "\n",
    "# Create a mask for years up to 2014\n",
    "mask_historical = result['Year'] <= 2014\n",
    "\n",
    "# Calculate mean and confidence intervals for historical scenarios\n",
    "result['Historical_Mean'] = result.loc[mask_historical, [f'{scenario}_Mean' for scenario in historical_columns]].mean(axis=1)\n",
    "result['Historical_CI_Lower'] = result.loc[mask_historical, [f'{scenario}_CI_Lower' for scenario in historical_columns]].mean(axis=1)\n",
    "result['Historical_CI_Upper'] = result.loc[mask_historical, [f'{scenario}_CI_Upper' for scenario in historical_columns]].mean(axis=1)\n",
    "\n",
    "# Set SSP columns to NaN for years up to 2014\n",
    "for scenario in historical_columns:\n",
    "    result.loc[mask_historical, f'{scenario}_Mean'] = np.nan\n",
    "    result.loc[mask_historical, f'{scenario}_CI_Lower'] = np.nan\n",
    "    result.loc[mask_historical, f'{scenario}_CI_Upper'] = np.nan\n",
    "\n",
    "# Set Historical columns to NaN for years after 2014\n",
    "result.loc[~mask_historical, 'Historical_Mean'] = np.nan\n",
    "result.loc[~mask_historical, 'Historical_CI_Lower'] = np.nan\n",
    "result.loc[~mask_historical, 'Historical_CI_Upper'] = np.nan\n",
    "\n",
    "result.to_csv(\"../Data/Figure_4_salinity.csv\")\n",
    "\n",
    "result.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb5a62-976a-4316-8ffa-53ef38a66b74",
   "metadata": {},
   "source": [
    "## Figure 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b1d659-ccd7-42ae-bc0c-e83f9f551250",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_salinity_5.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864918e9-969e-4945-b468-e6cb9559f981",
   "metadata": {},
   "source": [
    "Monthly data were retrieved from CMEMS [Mercator Ocean International / Copernicus Marine Service (2023). Global Ocean Physics Reanalysis (GLOBAL_MULTIYEAR_PHY_001_030) [Data set]. Copernicus Marine Service. https://doi.org/10.48670/moi-00021](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe02b21-1e1f-40fd-862b-cbede65687df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, json, datetime as dt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from affine import Affine\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "IN_PATH = \"../Data/cmems_mod_glo_phy_my_0.083deg_P1M-m_1760469819927.nc\"\n",
    "OUT_DIR = \"../Data/Figure_5_salinity\"\n",
    "VAR_NAME_HINTS = [\"so\", \"salinity\"]\n",
    "WRITE_FLOAT32 = False   # set True to write Float32 GeoTIFFs instead of 8-bit\n",
    "LOW_Q, HIGH_Q = 0.02, 0.98   # robust percentiles for 8-bit scaling\n",
    "COMPRESS = \"DEFLATE\"    # GeoTIFF compression\n",
    "NODATA_UINT8 = 0        # 0 reserved as nodata in 8-bit output\n",
    "OPEN_WITH_CHUNKS = True # try to enable dask-friendly quantiles\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def pick_var(ds: xr.Dataset) -> xr.DataArray:\n",
    "    for name in VAR_NAME_HINTS:\n",
    "        if name in ds.data_vars:\n",
    "            return ds[name]\n",
    "    return next(iter(ds.data_vars.values()))\n",
    "\n",
    "def ensure_xy(ds_like: xr.DataArray) -> xr.DataArray:\n",
    "    da = ds_like\n",
    "    rename_map = {}\n",
    "    if \"lon\" in da.dims: rename_map[\"lon\"] = \"x\"\n",
    "    if \"longitude\" in da.dims: rename_map[\"longitude\"] = \"x\"\n",
    "    if \"lat\" in da.dims: rename_map[\"lat\"] = \"y\"\n",
    "    if \"latitude\" in da.dims: rename_map[\"latitude\"] = \"y\"\n",
    "    if rename_map:\n",
    "        da = da.rename(rename_map)\n",
    "    if \"x\" not in da.dims or \"y\" not in da.dims:\n",
    "        raise ValueError(f\"Expected 'x' and 'y' dims, found {list(da.dims)}\")\n",
    "    if np.all(np.diff(da[\"y\"].values) > 0):\n",
    "        da = da.sortby(\"y\", ascending=False)\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "    xv, yv = da[\"x\"].values, da[\"y\"].values\n",
    "    dx = float(np.mean(np.diff(xv)))\n",
    "    dy = float(np.mean(np.diff(yv)))\n",
    "    x0 = xv.min() - dx / 2.0\n",
    "    y0 = yv.max() + dy / 2.0\n",
    "    transform = Affine(dx, 0.0, x0, 0.0, -dy, y0)\n",
    "    da = da.rio.write_transform(transform, inplace=False)\n",
    "    return da\n",
    "\n",
    "def compute_global_bounds(var: xr.DataArray, low_q=LOW_Q, high_q=HIGH_Q):\n",
    "    \"\"\"\n",
    "    Compute global robust quantiles across all time/y/x and return (vmin, vmax).\n",
    "    Tries xarray.quantile (dask-friendly), falls back to numpy if needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        q = var.quantile([low_q, high_q], dim=[d for d in var.dims if d in (\"time\",\"y\",\"x\",\"lat\",\"lon\",\"latitude\",\"longitude\")], skipna=True)\n",
    "        vmin = float(q.sel(quantile=low_q).values)\n",
    "        vmax = float(q.sel(quantile=high_q).values)\n",
    "    except Exception:\n",
    "        arr = var.values.astype(np.float32)  # may be large!\n",
    "        vmin, vmax = np.nanquantile(arr, [low_q, high_q])\n",
    "        vmin, vmax = float(vmin), float(vmax)\n",
    "    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin == vmax:\n",
    "        # very flat / missing data; fallback to global min/max\n",
    "        try:\n",
    "            vmin = float(var.min(skipna=True).values)\n",
    "            vmax = float(var.max(skipna=True).values)\n",
    "        except Exception:\n",
    "            arr = var.values.astype(np.float32)\n",
    "            vmin, vmax = float(np.nanmin(arr)), float(np.nanmax(arr))\n",
    "    return vmin, vmax\n",
    "\n",
    "def scale_to_uint8_fixed(da: xr.DataArray, vmin: float, vmax: float, nodata_val=NODATA_UINT8):\n",
    "    \"\"\"\n",
    "    Scale with fixed vmin/vmax to 8-bit, reserving 0 as nodata.\n",
    "    \"\"\"\n",
    "    arr = da.values.astype(np.float32)\n",
    "    mask = ~np.isfinite(arr)\n",
    "    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin == vmax:\n",
    "        # avoid divide-by-zero\n",
    "        vmin = np.nanmin(arr)\n",
    "        vmax = np.nanmax(arr)\n",
    "        if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin == vmax:\n",
    "            scaled = np.full(arr.shape, nodata_val, dtype=np.uint8)\n",
    "            return scaled\n",
    "    scaled = (arr - vmin) / (vmax - vmin + 1e-12)\n",
    "    scaled = np.clip(scaled, 0.0, 1.0)\n",
    "    scaled = (scaled * 254.0 + 1.0).astype(np.uint8)\n",
    "    scaled[mask] = nodata_val\n",
    "    return scaled\n",
    "\n",
    "def main():\n",
    "    open_kwargs = {\"chunks\": \"auto\"} if OPEN_WITH_CHUNKS else {}\n",
    "    ds = xr.open_dataset(IN_PATH, **open_kwargs)\n",
    "    var = pick_var(ds)\n",
    "    if \"time\" not in var.dims:\n",
    "        raise ValueError(\"Expected a 'time' dimension in salinity DataArray.\")\n",
    "\n",
    "    # --- Compute one pair of robust bounds across ALL timesteps ---\n",
    "    global_vmin, global_vmax = compute_global_bounds(var, LOW_Q, HIGH_Q)\n",
    "    print(f\"Global scaling bounds ({LOW_Q:.2f}-{HIGH_Q:.2f} quantiles): vmin={global_vmin:.4f}, vmax={global_vmax:.4f}\")\n",
    "\n",
    "    scaling_meta = {\n",
    "        \"variable\": str(var.name),\n",
    "        \"quantiles\": [LOW_Q, HIGH_Q],\n",
    "        \"global_vmin\": float(global_vmin),\n",
    "        \"global_vmax\": float(global_vmax),\n",
    "        \"nodata_uint8\": NODATA_UINT8,\n",
    "        \"files\": []\n",
    "    }\n",
    "\n",
    "    for i in range(var.sizes[\"time\"]):\n",
    "        slice_t = var.isel(time=i).squeeze(drop=True)\n",
    "        da = ensure_xy(slice_t)\n",
    "\n",
    "        if \"time\" in slice_t.coords:\n",
    "            tlabel = np.datetime_as_string(slice_t[\"time\"].values, unit=\"D\").replace(\"-\", \"\")[:6]  # YYYYMM\n",
    "        else:\n",
    "            tlabel = f\"t{i:04d}\"\n",
    "\n",
    "        if WRITE_FLOAT32:\n",
    "            out_path = os.path.join(OUT_DIR, f\"salinity_{tlabel}_float32.tif\")\n",
    "            da = da.rio.write_nodata(np.nan, inplace=False)\n",
    "            da.rio.to_raster(out_path, dtype=\"float32\", tiled=True, compress=COMPRESS)\n",
    "            print(f\"Wrote {out_path}\")\n",
    "            scaling_meta[\"files\"].append({\"file\": os.path.basename(out_path), \"tindex\": i, \"tlabel\": tlabel})\n",
    "        else:\n",
    "            scaled = scale_to_uint8_fixed(da, global_vmin, global_vmax, NODATA_UINT8)\n",
    "            out_path = os.path.join(OUT_DIR, f\"salinity_{tlabel}.tif\")\n",
    "            da8 = xr.DataArray(\n",
    "                scaled, dims=(\"y\",\"x\"), coords={\"y\": da[\"y\"], \"x\": da[\"x\"]},\n",
    "                name=\"salinity_uint8\",\n",
    "                attrs={\"long_name\": \"Sea Water Practical Salinity (scaled 8-bit, global bounds)\",\n",
    "                       \"units\": \"unitless (0=nodata)\",\n",
    "                       \"scale_vmin\": global_vmin, \"scale_vmax\": global_vmax}\n",
    "            ).rio.write_crs(da.rio.crs).rio.write_transform(da.rio.transform())\n",
    "            da8 = da8.rio.write_nodata(NODATA_UINT8, inplace=False)\n",
    "            da8.rio.to_raster(out_path, dtype=\"uint8\", tiled=True, compress=COMPRESS)\n",
    "            scaling_meta[\"files\"].append({\"file\": os.path.basename(out_path), \"tindex\": i, \"tlabel\": tlabel})\n",
    "            print(f\"Wrote {out_path} using global scaling [{global_vmin:.4f}, {global_vmax:.4f}]\")\n",
    "\n",
    "    if not WRITE_FLOAT32:\n",
    "        meta_path = os.path.join(OUT_DIR, \"salinity_scaling_metadata.json\")\n",
    "        with open(meta_path, \"w\") as f:\n",
    "            json.dump(scaling_meta, f, indent=2)\n",
    "        print(f\"Saved {meta_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b17496-faf5-4fae-9292-2d99ef0eb966",
   "metadata": {},
   "source": [
    "## Figure 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a153f-9582-4422-8c70-4b8e93663f16",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_salinity_6.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31103fc6-c7d8-4cdf-84d6-9898a9bfde03",
   "metadata": {},
   "source": [
    "Monthly data were retrieved from CMEMS [Mercator Ocean International / Copernicus Marine Service (2023). Global Ocean Physics Reanalysis (GLOBAL_MULTIYEAR_PHY_001_030) [Data set]. Copernicus Marine Service. https://doi.org/10.48670/moi-00021](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_001_030/description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0620df74-a8ab-466f-b826-abf1f30ba924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, json\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from affine import Affine\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "U_PATH = \"../Data/cmems_mod_glo_phy_my_0.083deg_P1M-m_1760473504514.nc\"  # uo\n",
    "V_PATH = \"../Data/cmems_mod_glo_phy_my_0.083deg_P1M-m_1760473531668.nc\"  # vo\n",
    "OUT_DIR = \"../Data/Figure_6_salinity\"\n",
    "WRITE_FLOAT32 = False        # True -> Float32 GeoTIFFs; False -> 8-bit scaled GeoTIFFs\n",
    "LOW_Q, HIGH_Q = 0.02, 0.98   # robust percentile bounds used globally across ALL timesteps\n",
    "COMPRESS = \"DEFLATE\"         # GeoTIFF compression\n",
    "NODATA_UINT8 = 0             # 0 reserved as nodata in 8-bit output\n",
    "OPEN_WITH_CHUNKS = True      # dask-friendly\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# helpers\n",
    "# ---------------------------\n",
    "def _ensure_xy(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"Rename lon/lat to x/y if needed, set CRS, ensure y desc, attach affine transform.\"\"\"\n",
    "    rename_map = {}\n",
    "    if \"lon\" in da.dims: rename_map[\"lon\"] = \"x\"\n",
    "    if \"longitude\" in da.dims: rename_map[\"longitude\"] = \"x\"\n",
    "    if \"lat\" in da.dims: rename_map[\"lat\"] = \"y\"\n",
    "    if \"latitude\" in da.dims: rename_map[\"latitude\"] = \"y\"\n",
    "    if rename_map:\n",
    "        da = da.rename(rename_map)\n",
    "    if \"x\" not in da.dims or \"y\" not in da.dims:\n",
    "        raise ValueError(f\"Expected 'x' and 'y' dims, found {list(da.dims)}\")\n",
    "    # y descending (north -> south)\n",
    "    if np.all(np.diff(da[\"y\"].values) > 0):\n",
    "        da = da.sortby(\"y\", ascending=False)\n",
    "    # georef\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "    xv, yv = da[\"x\"].values, da[\"y\"].values\n",
    "    dx = float(np.mean(np.diff(xv)))\n",
    "    dy = float(np.mean(np.diff(yv)))\n",
    "    x0 = xv.min() - dx / 2.0\n",
    "    y0 = yv.max() + dy / 2.0\n",
    "    transform = Affine(dx, 0.0, x0, 0.0, -dy, y0)\n",
    "    da = da.rio.write_transform(transform, inplace=False)\n",
    "    return da\n",
    "\n",
    "def _pick_surface(da: xr.DataArray) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    If a vertical dimension exists, take surface (depth ~ 0) by coord if available,\n",
    "    otherwise first level. Handles common names.\n",
    "    \"\"\"\n",
    "    z_names = [d for d in da.dims if d.lower() in (\"depth\",\"lev\",\"z\",\"depthu\",\"depthv\",\"deptho\")]\n",
    "    if not z_names:\n",
    "        return da\n",
    "    z = z_names[0]\n",
    "    try:\n",
    "        if z in da.coords and np.issubdtype(da.coords[z].dtype, np.number):\n",
    "            # choose nearest to 0 if 0 present or nearest available\n",
    "            if 0 in set(np.asarray(da.coords[z]).round(6)):\n",
    "                da = da.sel({z: 0}, method=\"nearest\")\n",
    "            else:\n",
    "                da = da.sel({z: float(da.coords[z].values.min())}, method=\"nearest\")\n",
    "        else:\n",
    "            da = da.isel({z: 0})\n",
    "    except Exception:\n",
    "        da = da.isel({z: 0})\n",
    "    return da\n",
    "\n",
    "def _compute_global_bounds(var: xr.DataArray, low_q=LOW_Q, high_q=HIGH_Q):\n",
    "    \"\"\"Compute global robust quantiles across time/y/x; fallback to min/max if degenerate.\"\"\"\n",
    "    try:\n",
    "        q = var.quantile([low_q, high_q], dim=[d for d in var.dims if d in (\"time\",\"y\",\"x\",\"lat\",\"lon\",\"latitude\",\"longitude\")], skipna=True)\n",
    "        vmin = float(q.sel(quantile=low_q).values)\n",
    "        vmax = float(q.sel(quantile=high_q).values)\n",
    "    except Exception:\n",
    "        arr = var.values.astype(np.float32)\n",
    "        vmin, vmax = np.nanquantile(arr, [low_q, high_q])\n",
    "        vmin, vmax = float(vmin), float(vmax)\n",
    "    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "        # fallback to global min/max\n",
    "        try:\n",
    "            vmin = float(var.min(skipna=True).values)\n",
    "            vmax = float(var.max(skipna=True).values)\n",
    "        except Exception:\n",
    "            arr = var.values.astype(np.float32)\n",
    "            vmin, vmax = float(np.nanmin(arr)), float(np.nanmax(arr))\n",
    "    return vmin, vmax\n",
    "\n",
    "def _scale_to_uint8_fixed(da: xr.DataArray, vmin: float, vmax: float, nodata_val=NODATA_UINT8):\n",
    "    \"\"\"Scale with fixed vmin/vmax to 8-bit, reserving 0 as nodata.\"\"\"\n",
    "    arr = da.values.astype(np.float32)\n",
    "    mask = ~np.isfinite(arr)\n",
    "    if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "        vmin = np.nanmin(arr)\n",
    "        vmax = np.nanmax(arr)\n",
    "        if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "            return np.full(arr.shape, nodata_val, dtype=np.uint8)\n",
    "    scaled = (arr - vmin) / (vmax - vmin + 1e-12)\n",
    "    scaled = np.clip(scaled, 0.0, 1.0)\n",
    "    scaled = (scaled * 254.0 + 1.0).astype(np.uint8)  # 1..255; 0 reserved as nodata\n",
    "    scaled[mask] = nodata_val\n",
    "    return scaled\n",
    "\n",
    "# ---------------------------\n",
    "# main\n",
    "# ---------------------------\n",
    "def main():\n",
    "    open_kwargs = {\"chunks\": \"auto\"} if OPEN_WITH_CHUNKS else {}\n",
    "    # Merge u and v by coords\n",
    "    ds = xr.open_mfdataset([U_PATH, V_PATH], combine=\"by_coords\", **open_kwargs)\n",
    "\n",
    "    if \"uo\" not in ds.data_vars or \"vo\" not in ds.data_vars:\n",
    "        # some CMEMS releases use 'uo'/'vo' names consistently; if not present, try to find them\n",
    "        raise ValueError(f\"Could not find variables 'uo' and 'vo' in the provided datasets. Found: {list(ds.data_vars)}\")\n",
    "\n",
    "    uo = ds[\"uo\"]\n",
    "    vo = ds[\"vo\"]\n",
    "\n",
    "    # Surface slice if needed\n",
    "    uo_sfc = _pick_surface(uo)\n",
    "    vo_sfc = _pick_surface(vo)\n",
    "\n",
    "    # Ensure time alignment\n",
    "    if \"time\" not in uo_sfc.dims or \"time\" not in vo_sfc.dims:\n",
    "        raise ValueError(\"Expected a 'time' dimension in both uo and vo.\")\n",
    "    # align by time intersection to be safe\n",
    "    uo_sfc, vo_sfc = xr.align(uo_sfc, vo_sfc, join=\"inner\")\n",
    "\n",
    "    # Compute speed magnitude\n",
    "    speed = xr.apply_ufunc(\n",
    "        lambda a, b: np.sqrt(a*a + b*b),\n",
    "        uo_sfc, vo_sfc,\n",
    "        dask=\"parallelized\", output_dtypes=[np.float32]\n",
    "    )\n",
    "    speed.name = \"current_speed\"\n",
    "    speed.attrs.update({\"long_name\": \"Ocean current speed\", \"units\": \"m s-1\"})\n",
    "\n",
    "    # Compute one pair of robust bounds across ALL timesteps\n",
    "    global_vmin, global_vmax = _compute_global_bounds(speed, LOW_Q, HIGH_Q)\n",
    "    print(f\"Global speed scaling bounds ({LOW_Q:.2f}-{HIGH_Q:.2f} quantiles): \"\n",
    "          f\"vmin={global_vmin:.5f}, vmax={global_vmax:.5f}\")\n",
    "\n",
    "    # Metadata holder\n",
    "    scaling_meta = {\n",
    "        \"variable\": \"current_speed\",\n",
    "        \"source_vars\": [\"uo\",\"vo\"],\n",
    "        \"units\": \"m s-1\",\n",
    "        \"quantiles\": [LOW_Q, HIGH_Q],\n",
    "        \"global_vmin\": float(global_vmin),\n",
    "        \"global_vmax\": float(global_vmax),\n",
    "        \"nodata_uint8\": NODATA_UINT8,\n",
    "        \"files\": []\n",
    "    }\n",
    "\n",
    "    # Iterate each time slice\n",
    "    T = speed.sizes[\"time\"]\n",
    "    for i in range(T):\n",
    "        slice_t = speed.isel(time=i).squeeze(drop=True)\n",
    "\n",
    "        # Ensure x/y + georeferencing\n",
    "        da = _ensure_xy(slice_t)\n",
    "\n",
    "        # Timestamp label YYYYMM\n",
    "        if \"time\" in slice_t.coords:\n",
    "            tlabel = np.datetime_as_string(slice_t[\"time\"].values, unit=\"D\").replace(\"-\", \"\")[:6]\n",
    "        else:\n",
    "            tlabel = f\"t{i:04d}\"\n",
    "\n",
    "        if WRITE_FLOAT32:\n",
    "            out_path = os.path.join(OUT_DIR, f\"current_speed_{tlabel}_float32.tif\")\n",
    "            da = da.rio.write_nodata(np.nan, inplace=False)\n",
    "            da.rio.to_raster(out_path, dtype=\"float32\", tiled=True, compress=COMPRESS)\n",
    "            print(f\"Wrote {out_path}\")\n",
    "            scaling_meta[\"files\"].append({\"file\": os.path.basename(out_path), \"tindex\": i, \"tlabel\": tlabel})\n",
    "        else:\n",
    "            scaled = _scale_to_uint8_fixed(da, global_vmin, global_vmax, NODATA_UINT8)\n",
    "            out_path = os.path.join(OUT_DIR, f\"current_speed_{tlabel}.tif\")\n",
    "            da8 = xr.DataArray(\n",
    "                scaled, dims=(\"y\",\"x\"), coords={\"y\": da[\"y\"], \"x\": da[\"x\"]},\n",
    "                name=\"current_speed_uint8\",\n",
    "                attrs={\n",
    "                    \"long_name\": \"Ocean current speed (scaled 8-bit, global bounds)\",\n",
    "                    \"units\": \"unitless (0=nodata)\",\n",
    "                    \"scale_vmin\": global_vmin,\n",
    "                    \"scale_vmax\": global_vmax\n",
    "                }\n",
    "            ).rio.write_crs(da.rio.crs).rio.write_transform(da.rio.transform())\n",
    "            da8 = da8.rio.write_nodata(NODATA_UINT8, inplace=False)\n",
    "            da8.rio.to_raster(out_path, dtype=\"uint8\", tiled=True, compress=COMPRESS)\n",
    "            scaling_meta[\"files\"].append({\"file\": os.path.basename(out_path), \"tindex\": i, \"tlabel\": tlabel})\n",
    "            print(f\"Wrote {out_path} using global scaling [{global_vmin:.5f}, {global_vmax:.5f}]\")\n",
    "\n",
    "    # Save sidecar metadata (for legends, rescaling)\n",
    "    if not WRITE_FLOAT32:\n",
    "        meta_path = os.path.join(OUT_DIR, \"current_speed_scaling_metadata.json\")\n",
    "        with open(meta_path, \"w\") as f:\n",
    "            json.dump(scaling_meta, f, indent=2)\n",
    "        print(f\"Saved {meta_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f584f95-3fbf-4194-bf96-0a3c2cb11043",
   "metadata": {},
   "source": [
    "# Acidity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f165cf-f15f-459a-b639-081c03f5146d",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c5d34-0c27-460f-a2af-2fc631f8dd25",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_acidity_1.png\" style=\"width:50%;\">\n",
    "</p>\n",
    "\n",
    "**This figure gets pH data from OceanSODA and also plots a linear trend over the period 1982 to present.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128e6a4e-3890-44d8-bc19-4ce14adfd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "acid_df = xr.open_dataset(\"../Data/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "acid_df = acid_df['ph_total'].mean(dim=['lat','lon']).resample(time='Y').mean()\n",
    "\n",
    "# Create a pandas DataFrame with these columns\n",
    "df = pd.DataFrame({\n",
    "    'time': acid_df['time'].values,\n",
    "    'ph_total': acid_df.values,\n",
    "})\n",
    "\n",
    "# Convert 'time' to datetime\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Convert datetime to a numerical value for linear regression (using ordinal format)\n",
    "df['time_ordinal'] = df['time'].map(pd.Timestamp.toordinal)\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(df['time_ordinal'], df['ph_total'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "df['linear_trend'] = intercept + slope * df['time_ordinal']\n",
    "\n",
    "df[['time','ph_total','linear_trend']].to_csv(\"../Data/Figure_1_acidity.csv\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df[['time','ph_total','linear_trend']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abad28a-5e56-4d3d-b2b0-91e1af60cac6",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf495e25-d69d-48b6-ba93-173042b80f80",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_acidity_2.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b290ea5-01d2-433a-bc5e-974d237b333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "acid_df = xr.open_dataset(\"../Data/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(acid_df['ph_total'])\n",
    "\n",
    "# --------------------\n",
    "# CONFIG\n",
    "# --------------------\n",
    "OUTPUT_TIF_8BIT   = \"./Figure_2_acidity.tif\"\n",
    "OUTPUT_TIF_FLOAT  = \"./acidity_trend_float32.tif\"\n",
    "USE_PERCENTILES   = True\n",
    "P_LOW, P_HIGH     = 2, 98\n",
    "VMIN_FIXED, VMAX_FIXED = -0.5, 0.5  # if USE_PERCENTILES=False\n",
    "\n",
    "# 1) Select the dataarray: salinity trend\n",
    "da = trend_significance_ds['trend']\n",
    "\n",
    "# 2) Rename to x/y if needed\n",
    "rename_map = {}\n",
    "if \"lat\" in da.dims or \"lat\" in da.coords: rename_map[\"lat\"] = \"y\"\n",
    "if \"latitude\" in da.dims or \"latitude\" in da.coords: rename_map[\"latitude\"] = \"y\"\n",
    "if \"lon\" in da.dims or \"lon\" in da.coords: rename_map[\"lon\"] = \"x\"\n",
    "if \"longitude\" in da.dims or \"longitude\" in da.coords: rename_map[\"longitude\"] = \"x\"\n",
    "if rename_map:\n",
    "    da = da.rename(rename_map)\n",
    "\n",
    "# 3) Ensure y is north→south (descending)\n",
    "if da[\"y\"].values[0] < da[\"y\"].values[-1]:\n",
    "    da = da.sortby(\"y\", ascending=False)\n",
    "# ensure (y, x) order\n",
    "if tuple(da.dims) != (\"y\", \"x\"):\n",
    "    da = da.transpose(\"y\", \"x\")\n",
    "\n",
    "# 4) Register spatial dims & force EPSG:4326\n",
    "da = da.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "\n",
    "# If CRS is missing, assume geographic lon/lat; then force-reproject to EPSG:4326\n",
    "if da.rio.crs is None:\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "\n",
    "# If CRS is not 4326, reproject it so the saved rasters are truly EPSG:4326\n",
    "if str(da.rio.crs) != \"EPSG:4326\":\n",
    "    # Use nearest for categorical-like; bilinear is typical for continuous SLA\n",
    "    da = da.rio.reproject(\"EPSG:4326\", resampling=rioxarray.rio.reproject.Resampling.bilinear)\n",
    "\n",
    "# Reassert dims order after any reprojection (just in case)\n",
    "if tuple(da.dims) != (\"y\", \"x\"):\n",
    "    da = da.transpose(\"y\", \"x\")\n",
    "\n",
    "\n",
    "# 5) Compute scaling range (Dask-safe)\n",
    "import dask.array as dsa\n",
    "\n",
    "def compute_percentiles_safe(da, p_low, p_high):\n",
    "    \"\"\"\n",
    "    Try dask.array.nanpercentile; if that fails (older dask/xarray),\n",
    "    fall back to a coarse sample to keep memory in check.\n",
    "    \"\"\"\n",
    "    if getattr(da, \"chunks\", None):\n",
    "        try:\n",
    "            vmin = float(dsa.nanpercentile(da.data, p_low).compute())\n",
    "            vmax = float(dsa.nanpercentile(da.data, p_high).compute())\n",
    "            return vmin, vmax\n",
    "        except Exception:\n",
    "            pass  # fall back to sampled approach\n",
    "\n",
    "    # Fallback: sample every Nth pixel (keeps memory tiny)\n",
    "    step_y = max(int(len(da.y) // 512), 1) if \"y\" in da.dims else 4\n",
    "    step_x = max(int(len(da.x) // 512), 1) if \"x\" in da.dims else 4\n",
    "    das = da.isel(\n",
    "        y=slice(0, None, step_y) if \"y\" in da.dims else slice(None),\n",
    "        x=slice(0, None, step_x) if \"x\" in da.dims else slice(None),\n",
    "    ).load()  # small enough to load\n",
    "    vmin = float(np.nanpercentile(das.values, p_low))\n",
    "    vmax = float(np.nanpercentile(das.values, p_high))\n",
    "    return vmin, vmax\n",
    "\n",
    "if USE_PERCENTILES:\n",
    "    vmin, vmax = compute_percentiles_safe(da, P_LOW, P_HIGH)\n",
    "else:\n",
    "    vmin, vmax = float(VMIN_FIXED), float(VMAX_FIXED)\n",
    "\n",
    "if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "    raise ValueError(f\"Bad scaling range: vmin={vmin}, vmax={vmax}\")\n",
    "\n",
    "\n",
    "# 6) Build the 8-bit layer with xr.where (no boolean indexing)\n",
    "# Reserve 0 for NoData; valid cells map to [1,255]\n",
    "norm = ((da - vmin) / (vmax - vmin)).clip(0, 1)\n",
    "scaled_da = xr.where(np.isfinite(da), norm * 254.0 + 1.0, 0.0).astype(\"uint8\")\n",
    "\n",
    "# Clear troublesome encodings on the uint8 view\n",
    "for k in (\"_FillValue\", \"missing_value\", \"scale_factor\", \"add_offset\"):\n",
    "    scaled_da.encoding.pop(k, None)\n",
    "\n",
    "# Set nodata appropriate for uint8 (0)\n",
    "scaled_da = scaled_da.rio.write_nodata(0, encoded=False, inplace=False)\n",
    "\n",
    "# 7a) Write 8-bit GeoTIFF (works with Dask; will stream-chunk if array is chunked)\n",
    "scaled_da.rio.to_raster(\n",
    "    OUTPUT_TIF_8BIT,\n",
    "    dtype=\"uint8\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "# 7b) Write float32 GeoTIFF with native values\n",
    "daf = da.where(np.isfinite(da)).astype(\"float32\")\n",
    "for k in (\"_FillValue\", \"missing_value\", \"scale_factor\", \"add_offset\"):\n",
    "    daf.encoding.pop(k, None)\n",
    "# Use NaN as nodata for float32\n",
    "daf = daf.rio.write_nodata(np.nan, encoded=False, inplace=False)\n",
    "\n",
    "daf.rio.to_raster(\n",
    "    OUTPUT_TIF_FLOAT,\n",
    "    dtype=\"float32\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Wrote {OUTPUT_TIF_8BIT} (uint8; 0=NoData, 1–255=data) and {OUTPUT_TIF_FLOAT} (float32). \"\n",
    "    f\"8-bit scale: vmin={vmin:.4f}, vmax={vmax:.4f} (native SLA units).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2448b26-7ea3-4b7c-8af1-1fd71429c2ce",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bae297-7989-48cf-9675-30c5caae1bc0",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_acidity_3.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422dba9-5828-4097-a3f1-7a3cb2476490",
   "metadata": {},
   "outputs": [],
   "source": [
    "acid_df = xr.open_dataset(\"../Data/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(acid_df['ph_total'])\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/Figure_3_acidity.geojson\",driver=\"GeoJSON\")\n",
    "\n",
    "del acid_df, area_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f5d0f-d5b0-4708-bd1b-35e73a4a8a42",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc0f4a-b39f-455e-9677-f65a43d35f9e",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_acidity_4.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69294c19-7d3c-49ce-861a-e2f477702990",
   "metadata": {},
   "outputs": [],
   "source": [
    "corals = gpd.read_file(\n",
    "    \"../Data/REEF_FORMING_CORALS/reef_forming_corals_dissolved.shp\"\n",
    ")\n",
    "\n",
    "coral_by_sea = area_coral_acidification_by_sea(\n",
    "    trend_significance_ds=trend_ds,\n",
    "    coral_gdf=corals,\n",
    "    SEAS_DF=SEAS_DF,\n",
    ")\n",
    "\n",
    "out_path = \"Figure_4_acidity.geojson\"\n",
    "\n",
    "coral_by_sea.to_file(\n",
    "    out_path,\n",
    "    driver=\"GeoJSON\"\n",
    ")\n",
    "\n",
    "print(f\"Saved GeoJSON to: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b46db9f-caa5-4cf8-a636-73cb01aa6d78",
   "metadata": {},
   "source": [
    "## Figure 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377cff7b-24ad-4ef3-ad79-9f1fabb718a7",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_acidity_5.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce241fc-0d5a-4e9e-84f3-62990a7d9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "co2_df = xr.open_dataset(\"../Data/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "# Latitude weights\n",
    "weights = np.cos(np.deg2rad(co2_df['lat']))\n",
    "\n",
    "# Weighted mean over lat/lon for selected variables\n",
    "weighted = co2_df[['ph_total','spco2']].weighted(weights)\n",
    "mean_timeseries = weighted.mean(dim=['lat','lon']).to_pandas()\n",
    "\n",
    "# Reset index so time is a column\n",
    "df_out = mean_timeseries.reset_index()\n",
    "\n",
    "# Save to JSON (records orientation = list of dicts)\n",
    "out_json = df_out[['time','spco2','ph_total']].to_json(\n",
    "    orient=\"records\", date_format=\"iso\"\n",
    ")\n",
    "\n",
    "# Write to file\n",
    "with open(\"../Data/Figure_5_acidity.json\", \"w\") as f:\n",
    "    f.write(out_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399adc0e-3eae-493c-9dc6-da58b4f83bb6",
   "metadata": {},
   "source": [
    "## Figure 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac4fb7-7410-4424-809f-1e4de974a4c8",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_acidity_6.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba3b05-0b08-48e5-80a0-4b9af63372a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- paths ---\n",
    "db_path = \"../Data/Global_Coral_Bleaching_Database_SQLite_11_24_21.db\"\n",
    "out_geojson = \"../Data/Figure_6_acidity.geojson\"\n",
    "\n",
    "# --- connect to the SQLite database ---\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    s.Site_ID,\n",
    "    s.Site_Name,\n",
    "    s.Latitude_Degrees AS lat,\n",
    "    s.Longitude_Degrees AS lon\n",
    "FROM Site_Info_tbl s\n",
    "JOIN Sample_Event_tbl se ON s.Site_ID = se.Site_ID\n",
    "JOIN Bleaching_tbl b ON se.Sample_ID = b.Sample_ID\n",
    "WHERE b.Percent_Bleached IS NOT NULL\n",
    "  AND b.Percent_Bleached > 0\n",
    "  AND s.Latitude_Degrees IS NOT NULL\n",
    "  AND s.Longitude_Degrees IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "bleach_sites = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"Found {len(bleach_sites)} sites with bleaching records.\")\n",
    "\n",
    "# --- fix bytes columns (decode to str) ---\n",
    "for col in bleach_sites.columns:\n",
    "    if bleach_sites[col].dtype == object:\n",
    "        bleach_sites[col] = bleach_sites[col].apply(\n",
    "            lambda v: v.decode(\"utf-8\", errors=\"ignore\") if isinstance(v, (bytes, bytearray)) else v\n",
    "        )\n",
    "\n",
    "# (Optional) keep just a few clean columns\n",
    "# bleach_sites = bleach_sites[[\"Site_ID\", \"Site_Name\", \"lat\", \"lon\"]].copy()\n",
    "\n",
    "# --- convert to GeoDataFrame ---\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    bleach_sites,\n",
    "    geometry=gpd.points_from_xy(bleach_sites[\"lon\"], bleach_sites[\"lat\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# --- save as GeoJSON ---\n",
    "gdf.to_file(out_geojson, driver=\"GeoJSON\")\n",
    "print(f\"Saved to {out_geojson}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15984f-d58b-4ad1-9faf-f71bad21a55c",
   "metadata": {},
   "source": [
    "## Figure 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb757d8d-3491-4bd7-94f1-d90dc305e1a2",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_acidity_7.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5282d72-3e14-452f-ba65-07603e398701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "co2_df = xr.open_dataset(\"../Data/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "# --------------------\n",
    "# CONFIG\n",
    "# --------------------\n",
    "OUTPUT_TIF_8BIT   = \"../Data/Figure_7_acidity.tif\"\n",
    "OUTPUT_TIF_FLOAT  = \"../Data/co2_trend_float32.tif\"\n",
    "USE_PERCENTILES   = True\n",
    "P_LOW, P_HIGH     = 2, 98\n",
    "VMIN_FIXED, VMAX_FIXED = -0.5, 0.5  # if USE_PERCENTILES=False\n",
    "\n",
    "# 1) Select the dataarray: salinity trend\n",
    "da = co2_df['spco2'].mean(dim='time')\n",
    "\n",
    "# 2) Rename to x/y if needed\n",
    "rename_map = {}\n",
    "if \"lat\" in da.dims or \"lat\" in da.coords: rename_map[\"lat\"] = \"y\"\n",
    "if \"latitude\" in da.dims or \"latitude\" in da.coords: rename_map[\"latitude\"] = \"y\"\n",
    "if \"lon\" in da.dims or \"lon\" in da.coords: rename_map[\"lon\"] = \"x\"\n",
    "if \"longitude\" in da.dims or \"longitude\" in da.coords: rename_map[\"longitude\"] = \"x\"\n",
    "if rename_map:\n",
    "    da = da.rename(rename_map)\n",
    "\n",
    "# 3) Ensure y is north→south (descending)\n",
    "if da[\"y\"].values[0] < da[\"y\"].values[-1]:\n",
    "    da = da.sortby(\"y\", ascending=False)\n",
    "# ensure (y, x) order\n",
    "if tuple(da.dims) != (\"y\", \"x\"):\n",
    "    da = da.transpose(\"y\", \"x\")\n",
    "\n",
    "# 4) Register spatial dims & force EPSG:4326\n",
    "da = da.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "\n",
    "# If CRS is missing, assume geographic lon/lat; then force-reproject to EPSG:4326\n",
    "if da.rio.crs is None:\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "\n",
    "# If CRS is not 4326, reproject it so the saved rasters are truly EPSG:4326\n",
    "if str(da.rio.crs) != \"EPSG:4326\":\n",
    "    # Use nearest for categorical-like; bilinear is typical for continuous SLA\n",
    "    da = da.rio.reproject(\"EPSG:4326\", resampling=rioxarray.rio.reproject.Resampling.bilinear)\n",
    "\n",
    "# Reassert dims order after any reprojection (just in case)\n",
    "if tuple(da.dims) != (\"y\", \"x\"):\n",
    "    da = da.transpose(\"y\", \"x\")\n",
    "\n",
    "\n",
    "# 5) Compute scaling range (Dask-safe)\n",
    "import dask.array as dsa\n",
    "\n",
    "def compute_percentiles_safe(da, p_low, p_high):\n",
    "    \"\"\"\n",
    "    Try dask.array.nanpercentile; if that fails (older dask/xarray),\n",
    "    fall back to a coarse sample to keep memory in check.\n",
    "    \"\"\"\n",
    "    if getattr(da, \"chunks\", None):\n",
    "        try:\n",
    "            vmin = float(dsa.nanpercentile(da.data, p_low).compute())\n",
    "            vmax = float(dsa.nanpercentile(da.data, p_high).compute())\n",
    "            return vmin, vmax\n",
    "        except Exception:\n",
    "            pass  # fall back to sampled approach\n",
    "\n",
    "    # Fallback: sample every Nth pixel (keeps memory tiny)\n",
    "    step_y = max(int(len(da.y) // 512), 1) if \"y\" in da.dims else 4\n",
    "    step_x = max(int(len(da.x) // 512), 1) if \"x\" in da.dims else 4\n",
    "    das = da.isel(\n",
    "        y=slice(0, None, step_y) if \"y\" in da.dims else slice(None),\n",
    "        x=slice(0, None, step_x) if \"x\" in da.dims else slice(None),\n",
    "    ).load()  # small enough to load\n",
    "    vmin = float(np.nanpercentile(das.values, p_low))\n",
    "    vmax = float(np.nanpercentile(das.values, p_high))\n",
    "    return vmin, vmax\n",
    "\n",
    "if USE_PERCENTILES:\n",
    "    vmin, vmax = compute_percentiles_safe(da, P_LOW, P_HIGH)\n",
    "else:\n",
    "    vmin, vmax = float(VMIN_FIXED), float(VMAX_FIXED)\n",
    "\n",
    "if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "    raise ValueError(f\"Bad scaling range: vmin={vmin}, vmax={vmax}\")\n",
    "\n",
    "\n",
    "# 6) Build the 8-bit layer with xr.where (no boolean indexing)\n",
    "# Reserve 0 for NoData; valid cells map to [1,255]\n",
    "norm = ((da - vmin) / (vmax - vmin)).clip(0, 1)\n",
    "scaled_da = xr.where(np.isfinite(da), norm * 254.0 + 1.0, 0.0).astype(\"uint8\")\n",
    "\n",
    "# Clear troublesome encodings on the uint8 view\n",
    "for k in (\"_FillValue\", \"missing_value\", \"scale_factor\", \"add_offset\"):\n",
    "    scaled_da.encoding.pop(k, None)\n",
    "\n",
    "# Set nodata appropriate for uint8 (0)\n",
    "scaled_da = scaled_da.rio.write_nodata(0, encoded=False, inplace=False)\n",
    "\n",
    "# 7a) Write 8-bit GeoTIFF (works with Dask; will stream-chunk if array is chunked)\n",
    "scaled_da.rio.to_raster(\n",
    "    OUTPUT_TIF_8BIT,\n",
    "    dtype=\"uint8\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "# 7b) Write float32 GeoTIFF with native values\n",
    "daf = da.where(np.isfinite(da)).astype(\"float32\")\n",
    "for k in (\"_FillValue\", \"missing_value\", \"scale_factor\", \"add_offset\"):\n",
    "    daf.encoding.pop(k, None)\n",
    "# Use NaN as nodata for float32\n",
    "daf = daf.rio.write_nodata(np.nan, encoded=False, inplace=False)\n",
    "\n",
    "daf.rio.to_raster(\n",
    "    OUTPUT_TIF_FLOAT,\n",
    "    dtype=\"float32\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Wrote {OUTPUT_TIF_8BIT} (uint8; 0=NoData, 1–255=data) and {OUTPUT_TIF_FLOAT} (float32). \"\n",
    "    f\"8-bit scale: vmin={vmin:.4f}, vmax={vmax:.4f} (native SLA units).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97cb40e-a54a-4aaa-9605-e5d4142bf0a6",
   "metadata": {},
   "source": [
    "# Sea Level Rise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0973435-5a5e-483f-b274-c11fbf135d13",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceaeb79-b7a4-432a-876b-404605a2692b",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_slr_1.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c4e95-89cc-41d0-8e18-0b7f125dfdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load the data from the file, obtain unique hyperlink from https://sealevel.nasa.gov/\n",
    "url = 'https://deotb6e7tfubr.cloudfront.net/s3-edaf5da92e0ce48fb61175c28b67e95d/podaac-ops-cumulus-protected.s3.us-west-2.amazonaws.com/NASA_SSH_GMSL_INDICATOR/NASA_SSH_GMSL_INDICATOR.txt?A-userid=ps4813&Expires=1758135329&Signature=h-v-~pNMAoeIxGWvEFIMUGYPX-1~zNUUVVD1ugzD7z9Jujn0kT3ZQBIgmcRPfaByPVVl2JhQiudnWl0sGQIPnJi1IrVCMwVd0Ye6KF~5-Wi6UuKTFJVSXae75hw0SJu4H64TiqpZWVCpRgEi-Q2LZ1kUJdVA2VNc4TMbbW2QVLIkazBXJHlZme~omHoLvGrAIF5GtDSEj2HTjZRCP9r6OBybnbNK7huNmSvJNPCt9y-GhqqB8TbUDkxpg78KBXkF-oOnu~pWLVUDtWatlv1UOhljqULhg4-QawOSohnBEEXoSZAKN~daGhAyccVjpAeaPR8VQpmS~ZUgugyMxoFxyQ__&Key-Pair-Id=K3RGFTW2DFGMID'\n",
    "# Fetch the content\n",
    "response = requests.get(url)\n",
    "content = response.text\n",
    "\n",
    "# Split the content into lines\n",
    "lines = content.split('\\n')\n",
    "\n",
    "# Find the index of the line containing \"Header_End\"\n",
    "header_end_index = next(i for i, line in enumerate(lines) if \"Header_End\" in line)\n",
    "\n",
    "# Read the data, skipping the header rows\n",
    "raw_data = pd.read_csv(io.StringIO('\\n'.join(lines[header_end_index + 1:])), \n",
    "                       sep='\\s+', \n",
    "                       header=None)\n",
    "\n",
    "# Create a new DataFrame with 'date' and 'SLR' columns\n",
    "df = pd.DataFrame({\n",
    "    'date': raw_data[0],\n",
    "    'SLR': raw_data[2] - raw_data[2].iloc[0]  # Shifting SLR so that the first value is 0\n",
    "})\n",
    "\n",
    "# Function to convert fractional year to datetime (year, month, day only)\n",
    "def fractional_year_to_datetime(year):\n",
    "    year_int = int(year)  # Extract the integer part\n",
    "    remainder = year - year_int  # Get the fractional part\n",
    "    beginning_of_year = datetime(year_int, 1, 1)\n",
    "    days_in_year = (datetime(year_int + 1, 1, 1) - beginning_of_year).days\n",
    "    return (beginning_of_year + timedelta(days=remainder * days_in_year)).date()\n",
    "\n",
    "# Convert the fractional years in 'date' column to datetime (year-month-day)\n",
    "df['date'] = df['date'].apply(fractional_year_to_datetime)\n",
    "\n",
    "# Extract the year from the 'date' column and create a new 'year' column\n",
    "df['year'] = df['date'].apply(lambda x: x.year)\n",
    "\n",
    "# Group by the 'year' column and calculate the mean for the 'SLR' column\n",
    "df_grouped = df.groupby('year').mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Fit a linear trend\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df_grouped['year'], df_grouped['SLR'])\n",
    "\n",
    "# Add the linear trend to the DataFrame\n",
    "df_grouped['linear_trend'] = slope * df_grouped['year'] + intercept\n",
    "\n",
    "# Save the grouped data to a JSON file\n",
    "df_grouped.to_json(\"../Data/Figure_1_SLR.json\", orient=\"records\", date_format=\"iso\")\n",
    "\n",
    "# Display the first few rows of the grouped DataFrame\n",
    "df_grouped.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88152f57-5b25-48df-8f5f-6bebd0153b41",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cec87d-20eb-4ee0-be2f-d09358809646",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_slr_2.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89838d9f-efc3-4e1a-af9c-0977471ceec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "# Copernicus Climate Change Service, Climate Data Store, (2018): Sea level daily gridded data from satellite observations for the global ocean from 1993 to present. Copernicus Climate Change Service (C3S) Climate Data Store (CDS)\n",
    "SLR_df = xr.open_mfdataset(\"../Data/dataset-satellite-sea-level-global-dc7f92ea-2d3e-4fc6-b767-836a5b8c0bff/*.nc\")\n",
    "\n",
    "# --------------------\n",
    "# CONFIG\n",
    "# --------------------\n",
    "OUTPUT_TIF_8BIT   = \"./Figure_2_SLR.tif\"\n",
    "OUTPUT_TIF_FLOAT  = \"./sla_mean_float32.tif\"\n",
    "USE_PERCENTILES   = True\n",
    "P_LOW, P_HIGH     = 2, 98\n",
    "VMIN_FIXED, VMAX_FIXED = -0.5, 0.5  # if USE_PERCENTILES=False\n",
    "\n",
    "# 1) Select the dataarray: mean SLA over time\n",
    "da = SLR_df[\"sla\"]\n",
    "if \"time\" in da.dims:\n",
    "    da = da.mean(dim=\"time\")\n",
    "\n",
    "# 2) Rename to x/y if needed\n",
    "rename_map = {}\n",
    "if \"lat\" in da.dims or \"lat\" in da.coords: rename_map[\"lat\"] = \"y\"\n",
    "if \"latitude\" in da.dims or \"latitude\" in da.coords: rename_map[\"latitude\"] = \"y\"\n",
    "if \"lon\" in da.dims or \"lon\" in da.coords: rename_map[\"lon\"] = \"x\"\n",
    "if \"longitude\" in da.dims or \"longitude\" in da.coords: rename_map[\"longitude\"] = \"x\"\n",
    "if rename_map:\n",
    "    da = da.rename(rename_map)\n",
    "\n",
    "# 3) Ensure y is north→south (descending)\n",
    "if da[\"y\"].values[0] < da[\"y\"].values[-1]:\n",
    "    da = da.sortby(\"y\", ascending=False)\n",
    "# ensure (y, x) order\n",
    "if tuple(da.dims) != (\"y\", \"x\"):\n",
    "    da = da.transpose(\"y\", \"x\")\n",
    "\n",
    "# 4) Register spatial dims & force EPSG:4326\n",
    "da = da.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "\n",
    "# If CRS is missing, assume geographic lon/lat; then force-reproject to EPSG:4326\n",
    "if da.rio.crs is None:\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "\n",
    "# If CRS is not 4326, reproject it so the saved rasters are truly EPSG:4326\n",
    "if str(da.rio.crs) != \"EPSG:4326\":\n",
    "    # Use nearest for categorical-like; bilinear is typical for continuous SLA\n",
    "    da = da.rio.reproject(\"EPSG:4326\", resampling=rioxarray.rio.reproject.Resampling.bilinear)\n",
    "\n",
    "# Reassert dims order after any reprojection (just in case)\n",
    "if tuple(da.dims) != (\"y\", \"x\"):\n",
    "    da = da.transpose(\"y\", \"x\")\n",
    "\n",
    "\n",
    "# 5) Compute scaling range (Dask-safe)\n",
    "import dask.array as dsa\n",
    "\n",
    "def compute_percentiles_safe(da, p_low, p_high):\n",
    "    \"\"\"\n",
    "    Try dask.array.nanpercentile; if that fails (older dask/xarray),\n",
    "    fall back to a coarse sample to keep memory in check.\n",
    "    \"\"\"\n",
    "    if getattr(da, \"chunks\", None):\n",
    "        try:\n",
    "            vmin = float(dsa.nanpercentile(da.data, p_low).compute())\n",
    "            vmax = float(dsa.nanpercentile(da.data, p_high).compute())\n",
    "            return vmin, vmax\n",
    "        except Exception:\n",
    "            pass  # fall back to sampled approach\n",
    "\n",
    "    # Fallback: sample every Nth pixel (keeps memory tiny)\n",
    "    step_y = max(int(len(da.y) // 512), 1) if \"y\" in da.dims else 4\n",
    "    step_x = max(int(len(da.x) // 512), 1) if \"x\" in da.dims else 4\n",
    "    das = da.isel(\n",
    "        y=slice(0, None, step_y) if \"y\" in da.dims else slice(None),\n",
    "        x=slice(0, None, step_x) if \"x\" in da.dims else slice(None),\n",
    "    ).load()  # small enough to load\n",
    "    vmin = float(np.nanpercentile(das.values, p_low))\n",
    "    vmax = float(np.nanpercentile(das.values, p_high))\n",
    "    return vmin, vmax\n",
    "\n",
    "if USE_PERCENTILES:\n",
    "    vmin, vmax = compute_percentiles_safe(da, P_LOW, P_HIGH)\n",
    "else:\n",
    "    vmin, vmax = float(VMIN_FIXED), float(VMAX_FIXED)\n",
    "\n",
    "if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "    raise ValueError(f\"Bad scaling range: vmin={vmin}, vmax={vmax}\")\n",
    "\n",
    "\n",
    "# 6) Build the 8-bit layer with xr.where (no boolean indexing)\n",
    "# Reserve 0 for NoData; valid cells map to [1,255]\n",
    "norm = ((da - vmin) / (vmax - vmin)).clip(0, 1)\n",
    "scaled_da = xr.where(np.isfinite(da), norm * 254.0 + 1.0, 0.0).astype(\"uint8\")\n",
    "\n",
    "# Clear troublesome encodings on the uint8 view\n",
    "for k in (\"_FillValue\", \"missing_value\", \"scale_factor\", \"add_offset\"):\n",
    "    scaled_da.encoding.pop(k, None)\n",
    "\n",
    "# Set nodata appropriate for uint8 (0)\n",
    "scaled_da = scaled_da.rio.write_nodata(0, encoded=False, inplace=False)\n",
    "\n",
    "# 7a) Write 8-bit GeoTIFF (works with Dask; will stream-chunk if array is chunked)\n",
    "scaled_da.rio.to_raster(\n",
    "    OUTPUT_TIF_8BIT,\n",
    "    dtype=\"uint8\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "# 7b) Write float32 GeoTIFF with native values\n",
    "daf = da.where(np.isfinite(da)).astype(\"float32\")\n",
    "for k in (\"_FillValue\", \"missing_value\", \"scale_factor\", \"add_offset\"):\n",
    "    daf.encoding.pop(k, None)\n",
    "# Use NaN as nodata for float32\n",
    "daf = daf.rio.write_nodata(np.nan, encoded=False, inplace=False)\n",
    "\n",
    "daf.rio.to_raster(\n",
    "    OUTPUT_TIF_FLOAT,\n",
    "    dtype=\"float32\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Wrote {OUTPUT_TIF_8BIT} (uint8; 0=NoData, 1–255=data) and {OUTPUT_TIF_FLOAT} (float32). \"\n",
    "    f\"8-bit scale: vmin={vmin:.4f}, vmax={vmax:.4f} (native SLA units).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0570d-7525-4a15-8208-a58a493ef4b9",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7976d7a9-53ec-4e6e-bc63-05be7822a66e",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_slr_3.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee9992-0006-4cfb-a06b-ba84bc4b36e5",
   "metadata": {},
   "source": [
    "The data were derived from [Horwath et al.](https://essd.copernicus.org/articles/14/411/2022/). The data at the country-level were pulled from [Sea Level Explorer](https://earth.gov/sealevel/sea-level-explorer/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e32be6-a009-42d6-be5b-d9aaa7773d59",
   "metadata": {},
   "source": [
    "## Figures 4 and 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c7703-f108-4e0f-b93a-4345dd60f56b",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_slr_4.png\" style=\"width:50%;\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_slr_9.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae27548-e72a-409b-b3a2-1fc068522fb2",
   "metadata": {},
   "source": [
    "Global trends were retrieved from [NOAA](https://tidesandcurrents.noaa.gov/sltrends/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296a328-2139-415a-b4c4-fa437c97e9f9",
   "metadata": {},
   "source": [
    "## Figures 5 and 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa60c7-cedb-427a-bf9b-2d884e6199bf",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_slr_5.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986223b-6916-48b9-a5ad-8cbd2a4534dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# === CONFIGURATION — adapt paths/names to your data ===\n",
    "EEZ_SHP = \"../Data/Intersect_EEZ_IHO_v5_20241010/Intersect_EEZ_IHO_v5_20241010.shp\"  # adjust path\n",
    "OUTPUT_DIR = Path(\"sea_level_flood_data\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MASTER_CSV = Path(\"Figure_5_SLR.csv\")\n",
    "\n",
    "BASE_URL = \"https://d3qt3aobtsas2h.cloudfront.net/edge/ws/search/sealevelgovglobal\"\n",
    "\n",
    "# Load EEZ shapefile — must contain both EEZ_MRGID and ISO_SOV1\n",
    "eez = gpd.read_file(EEZ_SHP)\n",
    "required = [\"EEZ_MRGID\", \"ISO_SOV1\"]\n",
    "for field in required:\n",
    "    if field not in eez.columns:\n",
    "        raise ValueError(f\"Shapefile missing required field: {field}. Found columns: {eez.columns}\")\n",
    "\n",
    "eez_records = eez[[\"EEZ_MRGID\", \"ISO_SOV1\"]].drop_duplicates()\n",
    "print(\"Will attempt to fetch data for\", len(eez_records), \"EEZ records\")\n",
    "\n",
    "master = []\n",
    "\n",
    "for _, rec_meta in eez_records.iterrows():\n",
    "    mrg = rec_meta[\"EEZ_MRGID\"]\n",
    "    iso = rec_meta[\"ISO_SOV1\"]\n",
    "\n",
    "    params = {\n",
    "        \"mrg_id\": mrg,\n",
    "        \"format\": \"csv\"  # try xlsx first; fallback to csv if needed\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(BASE_URL, params=params, timeout=60)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"[{iso} / MRGID {mrg}] Download failed: {e}\")\n",
    "        continue\n",
    "\n",
    "    content = resp.content\n",
    "    content_type = resp.headers.get(\"Content-Type\",\"\")\n",
    "    # guess file extension\n",
    "    if \"spreadsheetml\" in content_type or content.startswith(b\"PK\"):\n",
    "        ext = \"xlsx\"\n",
    "    else:\n",
    "        ext = \"csv\"\n",
    "\n",
    "    fn = OUTPUT_DIR / f\"{iso}_{mrg}.{ext}\"\n",
    "    with open(fn, \"wb\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    # read flood data sheet\n",
    "    try:\n",
    "        if ext == \"xlsx\":\n",
    "            df = pd.read_excel(fn, sheet_name=\"Recent-Flooding\")\n",
    "        else:\n",
    "            df = pd.read_csv(fn)\n",
    "    except Exception as e:\n",
    "        print(f\"[{iso} / MRGID {mrg}] Could not read Recent-Flooding sheet: {e}\")\n",
    "        continue\n",
    "\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # Expect columns like: \"High Tide Flood Days Minor\", \"High Tide Flood Days Moderate\", \"High Tide Flood Days Major\"\n",
    "    fldays_cols = [\n",
    "        \"High Tide Flood Days Minor\",\n",
    "        \"High Tide Flood Days Moderate\",\n",
    "        \"High Tide Flood Days Major\"\n",
    "    ]\n",
    "\n",
    "    missing = [c for c in fldays_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[{iso} / MRGID {mrg}] Missing Flood-Days columns: {missing}. Available columns: {df.columns.tolist()}\")\n",
    "        continue\n",
    "\n",
    "    # Sum over all rows (all years)\n",
    "    total_flood_days = df[fldays_cols].sum(axis=1).sum()  # sum minor+moderate+major, then sum all years\n",
    "\n",
    "    master.append({\n",
    "        \"ISO_SOV1\": iso,\n",
    "        \"MRGID\": mrg,\n",
    "        \"Number_of_High_Tide_Flood_Days\": total_flood_days\n",
    "    })\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Save master CSV\n",
    "if master:\n",
    "    pd.DataFrame(master).to_csv(MASTER_CSV, index=False)\n",
    "    print(\"Saved flood-day summary to:\", MASTER_CSV)\n",
    "else:\n",
    "    print(\"No flood-day data collected. Check for errors or missing sheets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55944c74-038f-4b91-a968-db71dc1e50d8",
   "metadata": {},
   "source": [
    "## Figures 6 and 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f6416-abe6-4d47-9b28-088be689502d",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_slr_6.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c157e4f-d119-4722-952c-43732f8a7255",
   "metadata": {},
   "source": [
    "Data were retrieved from Supplementary Data 1 of Kulp, S.A., Strauss, B.H. New elevation data triple estimates of global vulnerability to sea-level rise and coastal flooding. Nat Commun 10, 4844 (2019). https://doi.org/10.1038/s41467-019-12808-z."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b26b8-8bb9-4077-9281-652307c2be27",
   "metadata": {},
   "source": [
    "## Figures 7 and 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe5d804-1125-435d-94fc-db153b4901af",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_slr_7.png\" style=\"width:50%;\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_slr_10.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de4040-4592-42ce-bafc-5a32e023dd3e",
   "metadata": {},
   "source": [
    "The mangrove extent raster was created from the Global Mangrove Watch Dataset from 2020. See the Restore Ecosystems notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a940be3-f856-4635-ab4a-ae280da1aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def latlon_cell_areas_km2(x, y):\n",
    "    \"\"\"\n",
    "    Compute grid-cell areas (km²) for a lat/lon grid.\n",
    "    Assumes x = lon (degrees), y = lat (degrees).\n",
    "    \"\"\"\n",
    "    R = 6371.0  # Earth radius in km\n",
    "    lon_rad = np.deg2rad(x)\n",
    "    lat_rad = np.deg2rad(y)\n",
    "\n",
    "    dlon = np.abs(np.gradient(lon_rad))   # (x,)\n",
    "    dlat = np.abs(np.gradient(lat_rad))   # (y,)\n",
    "\n",
    "    lat_upper = lat_rad + 0.5 * dlat\n",
    "    lat_lower = lat_rad - 0.5 * dlat\n",
    "\n",
    "    row_term = np.abs(np.sin(lat_upper) - np.sin(lat_lower))\n",
    "    area = (R**2) * row_term[:, None] * dlon[None, :]\n",
    "    return area\n",
    "\n",
    "\n",
    "def prep_aqueduct_inun_mask_from_tif(\n",
    "    tif_path: str,\n",
    "    depth_m_threshold: float = 0.0,\n",
    "    assume_band: int = 1,\n",
    "    to_crs: str = \"EPSG:4326\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Read an Aqueduct Floods hazard GeoTIFF (inundation depth in meters),\n",
    "    reproject to EPSG:4326, and return a uint8 mask where depth >= threshold.\n",
    "\n",
    "    Notes:\n",
    "    - Aqueduct hazard maps represent inundation depth (meters). :contentReference[oaicite:1]{index=1}\n",
    "    \"\"\"\n",
    "    da = rioxarray.open_rasterio(tif_path, masked=True).squeeze()\n",
    "\n",
    "    # If there's still a band dimension, select one\n",
    "    if \"band\" in da.dims:\n",
    "        da = da.sel(band=assume_band)\n",
    "\n",
    "    # Ensure CRS is known\n",
    "    if da.rio.crs is None:\n",
    "        raise ValueError(\n",
    "            \"Raster has no CRS. Check the .tif metadata and set it via da.rio.write_crs(...).\"\n",
    "        )\n",
    "\n",
    "    # Reproject to EPSG:4326 for consistent vector clipping & area weighting\n",
    "    if str(da.rio.crs) != to_crs:\n",
    "        da = da.rio.reproject(to_crs)\n",
    "\n",
    "    # Standardize spatial dims for your pipeline\n",
    "    da = da.rename({\"y\": \"y\", \"x\": \"x\"}).rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\")\n",
    "\n",
    "    # Build mask (depth >= threshold)\n",
    "    mask_u8 = (da >= depth_m_threshold).astype(\"uint8\")\n",
    "\n",
    "    return mask_u8\n",
    "\n",
    "\n",
    "def saltmarsh_slr_overlap_by_country_fast(\n",
    "    slr_mask_u8,\n",
    "    countries,\n",
    "    saltmarsh_gdf,\n",
    "    country_name_col=\"NAME\",\n",
    "    equal_area_crs=\"ESRI:54009\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Unchanged logic; 'slr_mask_u8' can be ANY uint8 raster mask (0/1) in EPSG:4326.\n",
    "    \"\"\"\n",
    "    slr_mask_u8 = slr_mask_u8.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Cell areas (km²) for EPSG:4326 grid\n",
    "    cell_area = latlon_cell_areas_km2(\n",
    "        slr_mask_u8[\"x\"].values,\n",
    "        slr_mask_u8[\"y\"].values,\n",
    "    )\n",
    "\n",
    "    cell_area_da = xr.DataArray(\n",
    "        cell_area,\n",
    "        dims=[\"y\", \"x\"],\n",
    "        coords={\"y\": slr_mask_u8[\"y\"], \"x\": slr_mask_u8[\"x\"]},\n",
    "    ).rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Prep vectors\n",
    "    countries = countries.to_crs(\"EPSG:4326\").copy()\n",
    "    countries[\"geometry\"] = countries.geometry.buffer(0)\n",
    "\n",
    "    saltmarsh = saltmarsh_gdf.to_crs(\"EPSG:4326\").copy()\n",
    "    saltmarsh[\"geometry\"] = saltmarsh.geometry.buffer(0)\n",
    "\n",
    "    # Spatial index (critical)\n",
    "    sindex = saltmarsh.sindex\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for _, country in tqdm(countries.iterrows(), total=len(countries), desc=\"Processing countries\"):\n",
    "        country_name = country.get(country_name_col, \"unknown\")\n",
    "        country_geom = country.geometry\n",
    "\n",
    "        try:\n",
    "            # 1) Spatially filter mangroves/saltmarshes to this sea\n",
    "            bbox_matches = list(sindex.intersection(country_geom.bounds))\n",
    "            if not bbox_matches:\n",
    "                continue\n",
    "\n",
    "            saltmarsh_sub = saltmarsh.iloc[bbox_matches]\n",
    "            saltmarsh_sub = saltmarsh_sub[saltmarsh_sub.intersects(country_geom)]\n",
    "            if saltmarsh_sub.empty:\n",
    "                continue\n",
    "\n",
    "            # 2) Clip + dissolve\n",
    "            saltmarsh_in_country = gpd.clip(saltmarsh_sub, country_geom)\n",
    "            saltmarsh_union = saltmarsh_in_country.unary_union\n",
    "\n",
    "            saltmarsh_country_gdf = gpd.GeoDataFrame(\n",
    "                [{\"geometry\": saltmarsh_union}], crs=\"EPSG:4326\"\n",
    "            )\n",
    "\n",
    "            # 3) True mangrove/saltmarsh area (km²)\n",
    "            saltmarsh_eq = saltmarsh_country_gdf.to_crs(equal_area_crs)\n",
    "            true_saltmarsh_km2 = saltmarsh_eq.geometry.area.sum() / 1e6\n",
    "            if true_saltmarsh_km2 <= 0:\n",
    "                continue\n",
    "\n",
    "            # 4) Raster clip (mask + cell areas)\n",
    "            mask_clip = slr_mask_u8.rio.clip(saltmarsh_country_gdf.geometry, drop=True)\n",
    "            area_clip = cell_area_da.rio.clip(saltmarsh_country_gdf.geometry, drop=True)\n",
    "\n",
    "            affected_pix_km2 = (mask_clip * area_clip).sum(dim=(\"y\", \"x\")).item()\n",
    "            total_pix_km2 = area_clip.sum(dim=(\"y\", \"x\")).item()\n",
    "\n",
    "            frac = affected_pix_km2 / total_pix_km2 if total_pix_km2 > 0 else 0.0\n",
    "            print(country_name, \": \", frac)\n",
    "\n",
    "            records.append({\n",
    "                \"Country_Name\": country_name,\n",
    "                \"Total_Saltmarsh_Area_km2\": true_saltmarsh_km2,\n",
    "                \"Flood_Affected_Saltmarsh_Area_km2\": frac * true_saltmarsh_km2,\n",
    "                \"Flood_Affected_Saltmarsh_Area_Percent\": 100 * frac,\n",
    "                \"geometry\": country_geom,\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {country_name}: {e}\")\n",
    "\n",
    "    return gpd.GeoDataFrame(records, crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "FLOOD_TIF = \"../Data/inuncoast_rcp8p5_wtsub_2080_rp0050_0_perc_50.tif\"\n",
    "SALTMARSH_SHP = \"../Data/WCMC027_Saltmarsh_v6_1/01_Data/WCMC027_Saltmarshes_Py_v6_1.shp\"\n",
    "COUNTRY_SHP = \"../Data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp\"\n",
    "\n",
    "# -------------------------\n",
    "# Load data\n",
    "# -------------------------\n",
    "# Pick a threshold:\n",
    "# - 0.0 means “any modeled inundation”\n",
    "# - 0.1 means “>= 10 cm inundation depth”\n",
    "flood_mask = prep_aqueduct_inun_mask_from_tif(\n",
    "    FLOOD_TIF,\n",
    "    depth_m_threshold=0.0,\n",
    ")\n",
    "\n",
    "saltmarsh = gpd.read_file(SALTMARSH_SHP)\n",
    "countries = gpd.read_file(COUNTRY_SHP)\n",
    "\n",
    "# -------------------------\n",
    "# Compute overlap\n",
    "# -------------------------\n",
    "saltmarsh_flood_by_country = saltmarsh_slr_overlap_by_country_fast(\n",
    "    slr_mask_u8=flood_mask,\n",
    "    countries=countries,\n",
    "    saltmarsh_gdf=saltmarsh,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Save\n",
    "# -------------------------\n",
    "saltmarsh_flood_by_country.to_file(\n",
    "    \"Figure_7_SLR.geojson\",\n",
    "    driver=\"GeoJSON\",\n",
    ")\n",
    "\n",
    "saltmarsh_flood_by_country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9e00d1-68ba-42df-915f-79b8357b6f34",
   "metadata": {},
   "source": [
    "## Figures 8 and 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2385abec-7fec-4b08-9f54-a89f0f300228",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_slr_8.png\" style=\"width:50%;\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_slr_10.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e688a4-fdff-4f32-97ed-22df441afbf1",
   "metadata": {},
   "source": [
    "The saltmarsh data were retrieved from Mcowen C, Weatherdon LV, Bochove J, Sullivan E, Blyth S, Zockler C, Stanwell-Smith D, Kingston N, Martin CS, Spalding M, Fletcher S (2017). A global map of saltmarshes. Biodiversity Data Journal 5: e11764. Paper doi: https://doi.org/10.3897/BDJ.5.e11764; Data URL: http://data.unep-wcmc.org/datasets/43. See the Restore Ecosystems notebook for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c898b7-c347-411c-87db-83da8558273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def latlon_cell_areas_km2(x, y):\n",
    "    \"\"\"\n",
    "    Compute grid-cell areas (km²) for a lat/lon grid.\n",
    "    Assumes x = lon (degrees), y = lat (degrees).\n",
    "    \"\"\"\n",
    "    R = 6371.0  # Earth radius in km\n",
    "    lon_rad = np.deg2rad(x)\n",
    "    lat_rad = np.deg2rad(y)\n",
    "\n",
    "    dlon = np.abs(np.gradient(lon_rad))   # (x,)\n",
    "    dlat = np.abs(np.gradient(lat_rad))   # (y,)\n",
    "\n",
    "    lat_upper = lat_rad + 0.5 * dlat\n",
    "    lat_lower = lat_rad - 0.5 * dlat\n",
    "\n",
    "    row_term = np.abs(np.sin(lat_upper) - np.sin(lat_lower))\n",
    "    area = (R**2) * row_term[:, None] * dlon[None, :]\n",
    "    return area\n",
    "\n",
    "\n",
    "def prep_aqueduct_inun_mask_from_tif(\n",
    "    tif_path: str,\n",
    "    depth_m_threshold: float = 0.0,\n",
    "    assume_band: int = 1,\n",
    "    to_crs: str = \"EPSG:4326\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Read an Aqueduct Floods hazard GeoTIFF (inundation depth in meters),\n",
    "    reproject to EPSG:4326, and return a uint8 mask where depth >= threshold.\n",
    "\n",
    "    Notes:\n",
    "    - Aqueduct hazard maps represent inundation depth (meters). :contentReference[oaicite:1]{index=1}\n",
    "    \"\"\"\n",
    "    da = rioxarray.open_rasterio(tif_path, masked=True).squeeze()\n",
    "\n",
    "    # If there's still a band dimension, select one\n",
    "    if \"band\" in da.dims:\n",
    "        da = da.sel(band=assume_band)\n",
    "\n",
    "    # Ensure CRS is known\n",
    "    if da.rio.crs is None:\n",
    "        raise ValueError(\n",
    "            \"Raster has no CRS. Check the .tif metadata and set it via da.rio.write_crs(...).\"\n",
    "        )\n",
    "\n",
    "    # Reproject to EPSG:4326 for consistent vector clipping & area weighting\n",
    "    if str(da.rio.crs) != to_crs:\n",
    "        da = da.rio.reproject(to_crs)\n",
    "\n",
    "    # Standardize spatial dims for your pipeline\n",
    "    da = da.rename({\"y\": \"y\", \"x\": \"x\"}).rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\")\n",
    "\n",
    "    # Build mask (depth >= threshold)\n",
    "    mask_u8 = (da >= depth_m_threshold).astype(\"uint8\")\n",
    "\n",
    "    return mask_u8\n",
    "\n",
    "\n",
    "def mangrove_slr_overlap_by_country_fast(\n",
    "    slr_mask_u8,\n",
    "    country_gdf,\n",
    "    mangrove_gdf,\n",
    "    country_name_col=\"NAME\",\n",
    "    equal_area_crs=\"ESRI:54009\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Unchanged logic; 'slr_mask_u8' can be ANY uint8 raster mask (0/1) in EPSG:4326.\n",
    "    \"\"\"\n",
    "    slr_mask_u8 = slr_mask_u8.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Cell areas (km²) for EPSG:4326 grid\n",
    "    cell_area = latlon_cell_areas_km2(\n",
    "        slr_mask_u8[\"x\"].values,\n",
    "        slr_mask_u8[\"y\"].values,\n",
    "    )\n",
    "\n",
    "    cell_area_da = xr.DataArray(\n",
    "        cell_area,\n",
    "        dims=[\"y\", \"x\"],\n",
    "        coords={\"y\": slr_mask_u8[\"y\"], \"x\": slr_mask_u8[\"x\"]},\n",
    "    ).rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "    # Prep vectors\n",
    "    countries = country_gdf.to_crs(\"EPSG:4326\").copy()\n",
    "    countries[\"geometry\"] = countries.geometry.buffer(0)\n",
    "\n",
    "    mangroves = mangrove_gdf.to_crs(\"EPSG:4326\").copy()\n",
    "    mangroves[\"geometry\"] = mangroves.geometry.buffer(0)\n",
    "\n",
    "    # Spatial index (critical)\n",
    "    sindex = mangroves.sindex\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for _, country in tqdm(countries.iterrows(), total=len(countries), desc=\"Processing countries\"):\n",
    "        country_name = country.get(country_name_col, \"unknown\")\n",
    "        country_geom = country.geometry\n",
    "\n",
    "        try:\n",
    "            # 1) Spatially filter mangroves/saltmarshes to this sea\n",
    "            bbox_matches = list(sindex.intersection(country_geom.bounds))\n",
    "            if not bbox_matches:\n",
    "                continue\n",
    "\n",
    "            mangroves_sub = mangroves.iloc[bbox_matches]\n",
    "            mangroves_sub = mangroves_sub[mangroves_sub.intersects(country_geom)]\n",
    "            if mangroves_sub.empty:\n",
    "                continue\n",
    "\n",
    "            # 2) Clip + dissolve\n",
    "            mangrove_in_country = gpd.clip(mangroves_sub, country_geom)\n",
    "            mangrove_union = mangrove_in_country.unary_union\n",
    "\n",
    "            mangrove_country_gdf = gpd.GeoDataFrame(\n",
    "                [{\"geometry\": mangrove_union}], crs=\"EPSG:4326\"\n",
    "            )\n",
    "\n",
    "            # 3) True mangrove/saltmarsh area (km²)\n",
    "            mangrove_eq = mangrove_country_gdf.to_crs(equal_area_crs)\n",
    "            true_mangrove_km2 = mangrove_eq.geometry.area.sum() / 1e6\n",
    "            if true_mangrove_km2 <= 0:\n",
    "                continue\n",
    "\n",
    "            # 4) Raster clip (mask + cell areas)\n",
    "            mask_clip = slr_mask_u8.rio.clip(mangrove_country_gdf.geometry, drop=True)\n",
    "            area_clip = cell_area_da.rio.clip(mangrove_country_gdf.geometry, drop=True)\n",
    "\n",
    "            affected_pix_km2 = (mask_clip * area_clip).sum(dim=(\"y\", \"x\")).item()\n",
    "            total_pix_km2 = area_clip.sum(dim=(\"y\", \"x\")).item()\n",
    "\n",
    "            frac = affected_pix_km2 / total_pix_km2 if total_pix_km2 > 0 else 0.0\n",
    "            print(country_name, \": \", frac)\n",
    "\n",
    "            records.append({\n",
    "                \"Country_Name\": country_name,\n",
    "                \"Total_Mangrove_Area_km2\": true_mangrove_km2,\n",
    "                \"Flood_Affected_Mangrove_Area_km2\": frac * true_mangrove_km2,\n",
    "                \"Flood_Affected_Mangrove_Area_Percent\": 100 * frac,\n",
    "                \"geometry\": country_geom,\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {country_name}: {e}\")\n",
    "\n",
    "    return gpd.GeoDataFrame(records, crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "FLOOD_TIF = \"../Data/inuncoast_rcp8p5_wtsub_2080_rp0050_0_perc_50.tif\"\n",
    "MANGROVE_SHP = \"../Data/gmw_v3_2020_vec/gmw_v3_2020_vec.shp\"\n",
    "COUNTRY_SHP = \"../Data/ne_50m_admin_0_countries/ne_50m_admin_0_countries.shp\"\n",
    "\n",
    "# -------------------------\n",
    "# Load data\n",
    "# -------------------------\n",
    "# Pick a threshold:\n",
    "# - 0.0 means “any modeled inundation”\n",
    "# - 0.1 means “>= 10 cm inundation depth”\n",
    "flood_mask = prep_aqueduct_inun_mask_from_tif(\n",
    "    FLOOD_TIF,\n",
    "    depth_m_threshold=0.0,\n",
    ")\n",
    "\n",
    "mangrove = gpd.read_file(MANGROVE_SHP)\n",
    "country = gpd.read_file(COUNTRY_SHP)\n",
    "\n",
    "# -------------------------\n",
    "# Compute overlap\n",
    "# -------------------------\n",
    "mangrove_flood_by_country = mangrove_slr_overlap_by_country_fast(\n",
    "    slr_mask_u8=flood_mask,\n",
    "    country_gdf=country,\n",
    "    mangrove_gdf=mangrove,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Save\n",
    "# -------------------------\n",
    "mangrove_flood_by_country.to_file(\n",
    "    \"Figure_8_SLR.geojson\",\n",
    "    driver=\"GeoJSON\",\n",
    ")\n",
    "\n",
    "mangrove_flood_by_country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb237c83-d456-498d-ab69-d3644fa1454c",
   "metadata": {},
   "source": [
    "# Sea Ice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0700b55d-c4c4-407a-9f9b-7f882fa544e2",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41872a4-04d7-4a1d-ab79-3da7a2f60c72",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_ice_1.png\" style=\"width:50%;\">\n",
    "</p>\n",
    "\n",
    "**This figure gets sea ice extent timseries data from [NSIDC](https://noaadata.apps.nsidc.org/NOAA/G02135/) and also plots a linear trend over the period 1978 to present.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f1186-a2c7-419a-ab97-b349c210556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Base URLs for the NSIDC Sea Ice Index monthly data (North and South)\n",
    "base_urls = {\n",
    "    \"north\": \"https://noaadata.apps.nsidc.org/NOAA/G02135/north/monthly/data/\",\n",
    "    \"south\": \"https://noaadata.apps.nsidc.org/NOAA/G02135/south/monthly/data/\"\n",
    "}\n",
    "\n",
    "# List of file names for North and South\n",
    "file_names = {\n",
    "    \"north\": [f\"N_{month:02d}_extent_v3.0.csv\" for month in range(1, 13)],\n",
    "    \"south\": [f\"S_{month:02d}_extent_v3.0.csv\" for month in range(1, 13)]\n",
    "}\n",
    "\n",
    "# Function to download and load a single file\n",
    "def download_and_load(base_url, file_name):\n",
    "    url = base_url + file_name\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = StringIO(response.text)\n",
    "        df = pd.read_csv(data)\n",
    "        df['mo'] = int(file_name.split('_')[1])  # Extract month from filename\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed to download {file_name}\")\n",
    "        return None\n",
    "\n",
    "# Download and load all files for North and South\n",
    "dataframes = {}\n",
    "for region in base_urls:\n",
    "    dataframes[region] = [download_and_load(base_urls[region], file) for file in file_names[region]]\n",
    "\n",
    "# Remove any None values (failed downloads) and concatenate dataframes\n",
    "for region in dataframes:\n",
    "    dataframes[region] = [df for df in dataframes[region] if df is not None]\n",
    "    dataframes[region] = pd.concat(dataframes[region], ignore_index=True)\n",
    "    dataframes[region] = dataframes[region].sort_values(['year', 'mo']).reset_index(drop=True)\n",
    "\n",
    "# Add north and south data together for corresponding year-month pairs\n",
    "combined_data = pd.merge(\n",
    "    dataframes['north'], \n",
    "    dataframes['south'], \n",
    "    on=['year', 'mo'], \n",
    "    suffixes=('_north', '_south')\n",
    ")\n",
    "\n",
    "# Calculate total extent (this assumes 'extent' column exists in both north and south data)\n",
    "combined_data['total_extent'] = combined_data[' extent_north'] + combined_data[' extent_south']\n",
    "\n",
    "combined_data = combined_data.query(\"` extent_north` != -9999\")\n",
    "\n",
    "# Calculate the annual average for extent_north and extent_south\n",
    "annual_avg = combined_data.groupby('year').mean(numeric_only=True)[[' extent_north', ' extent_south']]\n",
    "\n",
    "# Calculate the linear trend for extent_north\n",
    "slope_north, intercept_north, r_value_north, p_value_north, std_err_north = stats.linregress(\n",
    "    annual_avg.index, annual_avg[' extent_north']\n",
    ")\n",
    "\n",
    "# Calculate the linear trend for extent_south\n",
    "slope_south, intercept_south, r_value_south, p_value_south, std_err_south = stats.linregress(\n",
    "    annual_avg.index, annual_avg[' extent_south']\n",
    ")\n",
    "\n",
    "# Add the linear trend values as new columns to the DataFrame\n",
    "annual_avg['linear_trend_north'] = slope_north * annual_avg.index + intercept_north\n",
    "annual_avg['linear_trend_south'] = slope_south * annual_avg.index + intercept_south\n",
    "\n",
    "annual_avg.to_csv(\"../Data/Figure_1_sea_ice.csv\")\n",
    "\n",
    "# Display the first few rows of the annual averages with trends\n",
    "annual_avg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f37468b-6f0a-4c54-872f-bdebe0d0d609",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2408c3-089e-482e-a484-465148415e09",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_ice_2.png\" style=\"width:50%;\">\n",
    "</p>\n",
    "\n",
    "**The videos of Northern and Southern sea ice concentrations are a raw pass-through of sea ice concentration data from [NSIDC](https://noaadata.apps.nsidc.org/NOAA/G02135/).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12951cb6-44e5-41af-844e-222b4f6100aa",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7652d7e-e8cf-479a-926b-3c5c1d40f9e7",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_ice_3.png\" style=\"width:50%;\">\n",
    "</p>\n",
    "\n",
    "$CO_2$ data retrieved from [Lan, X., P. Tans, & K.W. Thoning (2025). Trends in globally-averaged CO₂ determined from NOAA Global Monitoring Laboratory measurements (Version 2025-11) NOAA Global Monitoring Laboratory. https://doi.org/10.15138/9N0H-ZH07](https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.txt). Temperature data retrieved from GISS Surface Temperature Analysis (v4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446bd2f4-1283-4ffe-8ea7-6f52b31e7563",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf7a53-697d-4402-8772-aa443e3204db",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_ice_4.png\" style=\"width:50%;\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ea41c-ea08-46cd-bf14-6d0bf85fc004",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sea-level components (MONTHLY):\n",
    "  1) NASA_SSH_GMSL_INDICATOR.txt  (GMSL, cm)\n",
    "  2) ocean_mass_200204_202506.txt (GRACE/GRACE-FO ocean mass, mm; deseasoned)\n",
    "  3) mean_thermosteric_sea_level_anomaly_0-2000_seasonal.nc (steric 0–2000 m; time units: 'months since 1955-01-01 00:00:00')\n",
    "\n",
    "Outputs:\n",
    "  - ../Data/Figure_4_sea_ice.png\n",
    "  - ../Data/Figure_4_sea_ice.csv\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------\n",
    "# Config\n",
    "# --------------------\n",
    "GMSL_TXT   = \"../Data/NASA_SSH_GMSL_INDICATOR.txt\"\n",
    "MASS_TXT   = \"../Data/ocean_mass_200204_202506.txt\"\n",
    "STERIC_NC  = \"../Data/mean_thermosteric_sea_level_anomaly_0-2000_seasonal.nc\"\n",
    "\n",
    "OUT_PNG = \"../Data/Figure_4_sea_ice.png\"\n",
    "OUT_CSV = \"../Data/Figure_4_sea_ice.csv\"\n",
    "SMOOTH_WINDOW_MONTHS = 6\n",
    "\n",
    "# --------------------\n",
    "# Helpers\n",
    "# --------------------\n",
    "def decimal_year_to_timestamp(y: float) -> pd.Timestamp:\n",
    "    year = int(np.floor(y)); rem = float(y) - year\n",
    "    start = datetime(year, 1, 1); end = datetime(year + 1, 1, 1)\n",
    "    return pd.Timestamp(start + (end - start) * rem)\n",
    "\n",
    "def read_decimal_table(path: str, col_names, usecols=None) -> pd.DataFrame:\n",
    "    raw = Path(path).read_text()\n",
    "    data_lines = \"\\n\".join(ln for ln in raw.splitlines() if re.match(r\"^\\s*[0-9]{4}\\.\", ln))\n",
    "    df = pd.read_csv(StringIO(data_lines), sep=r\"\\s+\", header=None, engine=\"python\")\n",
    "    df.columns = col_names[: df.shape[1]]\n",
    "    if usecols is not None: df = df[usecols]\n",
    "    t = pd.to_datetime(df.iloc[:,0].apply(decimal_year_to_timestamp))\n",
    "    return df.set_index(t).drop(columns=df.columns[0]).sort_index()\n",
    "\n",
    "def to_mm(s: pd.Series, units_hint: str | None) -> pd.Series:\n",
    "    if not units_hint: return s\n",
    "    u = units_hint.lower()\n",
    "    if u in {\"cm\",\"centimeter\",\"centimeters\"}: return s * 10.0\n",
    "    if u in {\"m\",\"meter\",\"meters\"}:           return s * 1000.0\n",
    "    return s\n",
    "\n",
    "def months_since_to_datetime(months: np.ndarray, ref_str: str) -> pd.DatetimeIndex:\n",
    "    ref = pd.Timestamp(ref_str)\n",
    "    out = []\n",
    "    for m in months.astype(float):\n",
    "        mi = int(np.floor(m)); frac = float(m) - mi\n",
    "        base = ref + pd.DateOffset(months=mi)\n",
    "        month_start = pd.Timestamp(base.year, base.month, 1)\n",
    "        month_end = month_start + pd.DateOffset(months=1)\n",
    "        days = (month_end - month_start).days\n",
    "        out.append(month_start + pd.Timedelta(days=frac * days))\n",
    "    return pd.DatetimeIndex(out)\n",
    "\n",
    "def best_month_shift(steric_m: pd.Series, anchor_idx: pd.DatetimeIndex, candidates=(-1,0,1)) -> int:\n",
    "    best_k, best_n = 0, -1\n",
    "    for k in candidates:\n",
    "        n = len(anchor_idx.intersection(steric_m.shift(k, freq=\"MS\").index))\n",
    "        if n > best_n or (n == best_n and abs(k) < abs(best_k)):\n",
    "            best_k, best_n = k, n\n",
    "    return best_k\n",
    "\n",
    "# --------------------\n",
    "# 1) GMSL (NASA SSH) — cm → mm → MONTHLY\n",
    "# --------------------\n",
    "gmsl = read_decimal_table(GMSL_TXT, [\"time_dec\",\"gmsl_cm\",\"gmsl60_cm\"], usecols=[\"time_dec\",\"gmsl_cm\"])\n",
    "gmsl_m = to_mm(gmsl[\"gmsl_cm\"], \"cm\").resample(\"MS\").mean().interpolate(\"time\")\n",
    "\n",
    "# --------------------\n",
    "# 2) Ocean mass (GRACE/GRACE-FO) — deseasoned mm → MONTHLY\n",
    "# --------------------\n",
    "mass = read_decimal_table(MASS_TXT,\n",
    "                          [\"time_dec\",\"ocean_mass_mm\",\"sigma_mm\",\"ocean_mass_deseasoned_mm\"],\n",
    "                          usecols=[\"time_dec\",\"ocean_mass_deseasoned_mm\"])\n",
    "mass_m = mass[\"ocean_mass_deseasoned_mm\"].resample(\"MS\").mean().interpolate(\"time\")\n",
    "\n",
    "# --------------------\n",
    "# 3) Steric (0–2000 m) — months since 1955-01-01 → MONTHLY (robust)\n",
    "# --------------------\n",
    "ds = xr.open_dataset(STERIC_NC, decode_times=False)\n",
    "if \"seas_a_mm_WO\" not in ds: raise KeyError(\"Missing 'seas_a_mm_WO' in steric NetCDF.\")\n",
    "\n",
    "time_units = ds[\"time\"].attrs.get(\"units\",\"\")\n",
    "m = re.match(r\"months since\\s+([0-9]{4}-[0-9]{2}-[0-9]{2}(?:\\s+[0-9:]+)?)\", time_units)\n",
    "if not m: raise ValueError(f\"Unexpected steric time units: {time_units!r}\")\n",
    "times = months_since_to_datetime(ds[\"time\"].values, m.group(1))\n",
    "\n",
    "steric = pd.Series(ds[\"seas_a_mm_WO\"].values.astype(\"float64\"), index=times)\n",
    "steric = steric.replace([-2147483648, -2.14748365e9, 2.14748365e9], np.nan).dropna()\n",
    "steric = to_mm(steric, ds[\"seas_a_mm_WO\"].attrs.get(\"units\",\"mm\")).sort_index()\n",
    "\n",
    "# Map to month-start stamps FIRST (so anchors survive), collapse duplicates\n",
    "steric_ms = steric.copy()\n",
    "steric_ms.index = steric_ms.index.to_period(\"M\").to_timestamp(how=\"start\")\n",
    "steric_ms = steric_ms.groupby(level=0).mean()\n",
    "\n",
    "# Now build full monthly index and interpolate between anchors\n",
    "start = steric_ms.index.min()\n",
    "end   = steric_ms.index.max()\n",
    "monthly_index = pd.date_range(start, end, freq=\"MS\")\n",
    "steric_m = steric_ms.reindex(monthly_index).interpolate(method=\"time\")\n",
    "\n",
    "# Auto-fix 1-month anchor offset if present\n",
    "anchor_idx = gmsl_m.dropna().index.intersection(mass_m.dropna().index)\n",
    "k = best_month_shift(steric_m, anchor_idx, candidates=(-1,0,1))\n",
    "if k != 0:\n",
    "    steric_m = steric_m.shift(k, freq=\"MS\")\n",
    "    print(f\"[info] steric month shift applied: {k:+d}\")\n",
    "\n",
    "# --------------------\n",
    "# Align ocean mass vertically at first common month\n",
    "# --------------------\n",
    "df_raw = pd.concat({\"GMSL_mm\": gmsl_m, \"OceanMass_mm\": mass_m, \"Steric_mm\": steric_m}, axis=1)\n",
    "common = df_raw.dropna(subset=[\"GMSL_mm\",\"OceanMass_mm\",\"Steric_mm\"])\n",
    "if common.empty: raise ValueError(\"No common month across GMSL, OceanMass, Steric after alignment.\")\n",
    "\n",
    "t0 = common.index.min()\n",
    "delta = (common.loc[t0,\"GMSL_mm\"] - common.loc[t0,\"Steric_mm\"]) - common.loc[t0,\"OceanMass_mm\"]\n",
    "mass_m_aligned = mass_m + delta\n",
    "print(f\"[info] ocean mass vertical delta: {delta:.3f} mm at {t0.date()}\")\n",
    "\n",
    "# --------------------\n",
    "# Final MONTHLY dataframe + smoothing\n",
    "# --------------------\n",
    "df_m = pd.concat({\"GMSL_mm\": gmsl_m, \"OceanMass_mm\": mass_m_aligned, \"Steric_mm\": steric_m}, axis=1)\n",
    "df_m[\"MassPlusSteric_mm\"] = df_m[\"OceanMass_mm\"] + df_m[\"Steric_mm\"]\n",
    "df_sm = df_m.rolling(window=SMOOTH_WINDOW_MONTHS, min_periods=1).mean()\n",
    "\n",
    "# --------------------\n",
    "# Plot + save\n",
    "# --------------------\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "df_sm[\"GMSL_mm\"].plot(ax=ax, linewidth=2, label=\"Global mean sea level (NASA)\")\n",
    "df_sm[\"OceanMass_mm\"].plot(ax=ax, linewidth=2, label=\"Global ocean mass (JPL, aligned)\")\n",
    "df_sm[\"Steric_mm\"].plot(ax=ax, linewidth=2, label=\"Global steric (SIO, 0–2000 m)\")\n",
    "\n",
    "ax.set_ylabel(\"Sea level (mm)\")\n",
    "ax.set_title(\"Global sea level rise & components (monthly; mass aligned)\")\n",
    "ax.grid(True, alpha=0.3); ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout(); plt.savefig(OUT_PNG, dpi=200)\n",
    "\n",
    "df_sm.to_csv(OUT_CSV, index_label=\"date\")\n",
    "print(f\"Saved figure → {OUT_PNG}\")\n",
    "print(f\"Saved CSV    → {OUT_CSV}\")\n",
    "\n",
    "# --------------------\n",
    "# Save JSON for Figure 4\n",
    "# --------------------\n",
    "import json\n",
    "\n",
    "JSON_PATH = \"../Data/Figure_4_sea_ice.json\"\n",
    "\n",
    "# Use the *unsmoothed* monthly series; require all three present\n",
    "json_df = (\n",
    "    df_m[[\"Steric_mm\", \"OceanMass_mm\", \"GMSL_mm\"]]\n",
    "    .dropna(subset=[\"Steric_mm\", \"OceanMass_mm\", \"GMSL_mm\"])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"date\",\n",
    "                     \"Steric_mm\": \"steric\",\n",
    "                     \"OceanMass_mm\": \"mass\",\n",
    "                     \"GMSL_mm\": \"total\"})\n",
    ")\n",
    "\n",
    "# Format date as YYYY-MM-DD and round values a bit (optional)\n",
    "json_records = [\n",
    "    {\n",
    "        \"date\": d.strftime(\"%Y-%m-%d\"),\n",
    "        \"steric\": float(round(s, 3)),\n",
    "        \"mass\": float(round(m, 3)),\n",
    "        \"total\": float(round(t, 3)),\n",
    "    }\n",
    "    for d, s, m, t in zip(\n",
    "        json_df[\"date\"], json_df[\"steric\"], json_df[\"mass\"], json_df[\"total\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "with open(JSON_PATH, \"w\") as f:\n",
    "    json.dump(json_records, f, indent=2)\n",
    "\n",
    "print(f\"Saved JSON    → {JSON_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a08670-69d1-409e-a6d1-4a099afd0a60",
   "metadata": {},
   "source": [
    "## Figure 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3149c-858f-4dbf-a157-649958faf7e0",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_ice_5.png\" style=\"width:50%;\">\n",
    "</p>\n",
    "\n",
    "**Solar radiation data were retrieved from [NASA CERES EBAF Edition 4.2](https://asdc.larc.nasa.gov/project/CERES/CERES_EBAF_Edition4.2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036ec55-61cf-479f-8b68-cee90b66b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "\n",
    "a = xr.open_mfdataset(\"../Data/CERES_EBAF_Edition4.2_200003-202407.nc\")\n",
    "\n",
    "# 1) Difference\n",
    "diff = a[\"solar_mon\"] - a[\"toa_sw_all_mon\"]\n",
    "\n",
    "# 2) Area weights (cosine latitude)\n",
    "lat_name = \"lat\" if \"lat\" in a.coords else \"latitude\"\n",
    "lon_name = \"lon\" if \"lon\" in a.coords else \"longitude\"\n",
    "\n",
    "weights = np.cos(np.deg2rad(a[lat_name]))\n",
    "# Broadcast to full grid\n",
    "weights_2d = weights.broadcast_like(diff)\n",
    "\n",
    "# 3) Weighted mean over lat/lon at each *time step* (monthly or whatever your data are)\n",
    "num = (diff * weights_2d).sum(dim=[lat_name, lon_name], skipna=True)\n",
    "den = weights_2d.where(np.isfinite(diff)).sum(dim=[lat_name, lon_name])\n",
    "awm_ts = num / den  # (time,) DataArray\n",
    "\n",
    "# 4) Annual averages (works with regular or cftime calendars)\n",
    "annual = awm_ts.groupby(\"time.year\").mean(\"time\")  # (year,) DataArray\n",
    "\n",
    "# 5) Convert to JSON; write each year at Jan 1 for a clean ISO date (or keep as int)\n",
    "records = []\n",
    "for y, val in zip(annual[\"year\"].values.tolist(), annual.values.tolist()):\n",
    "    # Use ISO date \"YYYY-01-01T00:00:00\" for nicer time axis downstream\n",
    "    records.append({\n",
    "        \"date\": datetime(int(y), 1, 1).isoformat(),\n",
    "        \"value\": float(val) if np.isfinite(val) else None\n",
    "    })\n",
    "\n",
    "out_path = \"../Data/Figure_5_sea_ice.json\"\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(records, f, indent=2)\n",
    "\n",
    "print(f\"Saved {out_path} with {len(records)} annual rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e8285c-1cba-464a-b058-223e85e00a9a",
   "metadata": {},
   "source": [
    "## Figure 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b1eeb-c1b4-475a-a5c7-908bc225ff38",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_ice_6.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da4bf06-2cbc-414c-aa47-f7e716a8ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# IUCN: Species in Sea-Ice-Related Habitats (latest status per species)\n",
    "# ============================\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pycountry\n",
    "\n",
    "from dotenv import load_dotenv \n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "TOKEN = os.getenv(\"IUCN_API_KEY\")\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "API_BASE = \"https://api.iucnredlist.org/api/v4\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"Authorization\": TOKEN\n",
    "}\n",
    "\n",
    "# Output paths\n",
    "OUT_DIR = \"../Data\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_HABITATS = os.path.join(OUT_DIR, \"iucn_habitats_all.csv\")\n",
    "OUT_ASSESS_RAW = os.path.join(OUT_DIR, \"iucn_sea_ice_assessments_raw.csv\")\n",
    "OUT_SPECIES_CSV = os.path.join(OUT_DIR, \"sea_ice_species_latest.csv\")\n",
    "OUT_SPECIES_JSON = os.path.join(OUT_DIR, \"sea_ice_species_latest.json\")\n",
    "\n",
    "# ---- Sea-ice habitat whitelist ----\n",
    "# High-precision core:\n",
    "CORE_SEA_ICE_CODES = {\"10_1\", \"9_1\"}  # Epipelagic + Neritic Pelagic\n",
    "# Optional haul-out / colony associates near ice edges:\n",
    "COASTAL_ADDONS = {\"13_1\", \"12_1\", \"12_2\"}  # Coastal cliffs/islands, rocky & sandy shorelines\n",
    "\n",
    "# Choose which to use:\n",
    "SEA_ICE_HABITAT_CODES = CORE_SEA_ICE_CODES  # or CORE_SEA_ICE_CODES | COASTAL_ADDONS\n",
    "\n",
    "# Optional: keep only records with locations in polar regions\n",
    "USE_POLAR_LOCATION_FILTER = True\n",
    "\n",
    "# Arctic ISO2 (+ territories frequently present in IUCN locations)\n",
    "ARCTIC_ISO2 = {\n",
    "    \"CA\",  # Canada\n",
    "    \"GL\",  # Greenland\n",
    "    \"IS\",  # Iceland\n",
    "    \"NO\",  # Norway\n",
    "    \"RU\",  # Russia\n",
    "    \"US\",  # United States (Alaska)\n",
    "    \"SJ\",  # Svalbard & Jan Mayen\n",
    "    \"FO\",  # Faroe Islands (edge case; include to be safe)\n",
    "    \"DK\",  # Denmark (parent for GL; some records use DK)\n",
    "}\n",
    "# Antarctic / sub-Antarctic ISO2 (+ external territories used by IUCN)\n",
    "ANTARCTIC_ISO2 = {\n",
    "    \"AQ\",  # Antarctica\n",
    "    \"GS\",  # South Georgia & South Sandwich Islands (UK)\n",
    "    \"TF\",  # French Southern Territories\n",
    "    \"HM\",  # Heard Island & McDonald Islands (AU)\n",
    "    \"BV\",  # Bouvet Island (NO)\n",
    "    \"FK\",  # Falkland Islands\n",
    "    # Some subantarctic islands may appear under parent countries:\n",
    "    \"NZ\",  # Campbell, Auckland, Antipodes, etc. (NZ)\n",
    "    \"AU\",  # Macquarie (AU)\n",
    "    \"ZA\",  # Prince Edward & Marion (ZA)\n",
    "    \"AR\",  # South Orkney/South Shetland in some datasets under AR/CL\n",
    "    \"CL\",  # Chilean Antarctic Territory\n",
    "    \"GB\",  # UK parent for GS sometimes\n",
    "}\n",
    "POLAR_ISO2 = ARCTIC_ISO2 | ANTARCTIC_ISO2\n",
    "\n",
    "# ---------------------------\n",
    "# HELPERS\n",
    "# ---------------------------\n",
    "def _retry_get(url, headers, retries=6, backoff=0.6, timeout=60):\n",
    "    for i in range(retries):\n",
    "        r = requests.get(url, headers=headers, timeout=timeout)\n",
    "        if r.status_code == 200:\n",
    "            return r\n",
    "        if r.status_code == 429:\n",
    "            wait = backoff * (2 ** i)\n",
    "            print(f\"[429] Rate-limited. Sleeping {wait:.1f}s…\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "        if 500 <= r.status_code < 600:\n",
    "            wait = backoff * (2 ** i)\n",
    "            print(f\"[{r.status_code}] Server error. Retry in {wait:.1f}s…\")\n",
    "            time.sleep(wait)\n",
    "            continue\n",
    "        raise RuntimeError(f\"GET {url} failed: {r.status_code} - {r.text[:300]}\")\n",
    "    raise RuntimeError(f\"GET {url} exhausted retries.\")\n",
    "\n",
    "def list_all_habitats():\n",
    "    \"\"\"Fetch all habitat definitions (paged) and cache to CSV.\"\"\"\n",
    "    page, per_page = 1, 200\n",
    "    rows = []\n",
    "    while True:\n",
    "        url = f\"{API_BASE}/habitats/?page={page}&per_page={per_page}\"\n",
    "        r = _retry_get(url, HEADERS)\n",
    "        data = r.json()\n",
    "        items = data.get(\"habitats\", data.get(\"data\", [])) or []\n",
    "        if not items:\n",
    "            break\n",
    "        rows.extend(items)\n",
    "        total_pages = int(r.headers.get(\"total-pages\", page))\n",
    "        if page >= total_pages:\n",
    "            break\n",
    "        page += 1\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df.to_csv(OUT_HABITATS, index=False)\n",
    "    return df\n",
    "\n",
    "def pick_whitelisted_habitats(hdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return rows for the whitelisted habitat codes with their descriptions (if available).\"\"\"\n",
    "    if hdf is None or hdf.empty:\n",
    "        # Fallback: synthesize minimal table\n",
    "        return pd.DataFrame({\"code\": sorted(SEA_ICE_HABITAT_CODES), \"description_en\": [None]*len(SEA_ICE_HABITAT_CODES)})\n",
    "    h = hdf.copy()\n",
    "    h[\"code\"] = h[\"code\"].astype(str)\n",
    "    # Normalize description field if nested\n",
    "    if \"description\" in h.columns and isinstance(h[\"description\"].iloc[0], dict):\n",
    "        h[\"description_en\"] = h[\"description\"].apply(lambda d: (d or {}).get(\"en\"))\n",
    "    elif \"name\" in h.columns:\n",
    "        h[\"description_en\"] = h[\"name\"]\n",
    "    else:\n",
    "        h[\"description_en\"] = None\n",
    "    return h[h[\"code\"].isin(SEA_ICE_HABITAT_CODES)][[\"code\", \"description_en\"]].drop_duplicates().sort_values(\"code\")\n",
    "\n",
    "def get_assessments_for_habitat(habitat_code: str):\n",
    "    \"\"\"\n",
    "    /api/v4/habitats/{id_or_code}\n",
    "    IUCN accepts codes like '10_1' and '9_1' here.\n",
    "    \"\"\"\n",
    "    page, per_page = 1, 100\n",
    "    out = []\n",
    "    while True:\n",
    "        url = f\"{API_BASE}/habitats/{habitat_code}?page={page}&per_page={per_page}\"\n",
    "        r = _retry_get(url, HEADERS)\n",
    "        data = r.json()\n",
    "        out.extend(data.get(\"assessments\", []))\n",
    "        total_pages = int(r.headers.get(\"total-pages\", 1))\n",
    "        if page >= total_pages:\n",
    "            break\n",
    "        page += 1\n",
    "    return out\n",
    "\n",
    "def get_assessment_detail(assessment_id: int):\n",
    "    url = f\"{API_BASE}/assessment/{assessment_id}\"\n",
    "    r = _retry_get(url, HEADERS)\n",
    "    return r.json()\n",
    "\n",
    "def extract_locations(det_json):\n",
    "    \"\"\"Return sorted set of ISO2 location codes from detail JSON.\"\"\"\n",
    "    locs = det_json.get(\"locations\", []) or []\n",
    "    codes = sorted({loc.get(\"code\") for loc in locs if loc.get(\"code\")})\n",
    "    return codes\n",
    "\n",
    "def extract_threat_codes(det_json):\n",
    "    threats = det_json.get(\"threats\", []) or []\n",
    "    return sorted({t.get(\"code\") for t in threats if t.get(\"code\")})\n",
    "\n",
    "# ---------------------------\n",
    "# MAIN\n",
    "# ---------------------------\n",
    "def main():\n",
    "    if TOKEN == \"REPLACE_ME_WITH_YOUR_TOKEN\" or not TOKEN:\n",
    "        raise SystemExit(\"Please set IUCN_API_TOKEN env var or edit TOKEN.\")\n",
    "\n",
    "    # 1) Habitats listing (for descriptions); safe to proceed if this fails\n",
    "    try:\n",
    "        habitats_df = list_all_habitats()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: could not list habitats ({e}). Proceeding without descriptions.\")\n",
    "        habitats_df = pd.DataFrame()\n",
    "\n",
    "    whitelist_df = pick_whitelisted_habitats(habitats_df)\n",
    "    if whitelist_df.empty:\n",
    "        print(\"No habitat descriptions found for whitelist; proceeding with codes only.\")\n",
    "        whitelist_df = pd.DataFrame({\"code\": sorted(SEA_ICE_HABITAT_CODES), \"description_en\": [None]*len(SEA_ICE_HABITAT_CODES)})\n",
    "\n",
    "    print(\"\\nUsing habitat codes:\")\n",
    "    print(whitelist_df.to_string(index=False))\n",
    "\n",
    "    # 2) Pull assessments for selected habitats\n",
    "    all_assess_raw = []\n",
    "    for _, row in tqdm(whitelist_df.iterrows(), total=len(whitelist_df), desc=\"Habitats\"):\n",
    "        code = row[\"code\"]\n",
    "        try:\n",
    "            assessments = get_assessments_for_habitat(code)\n",
    "            for a in assessments:\n",
    "                all_assess_raw.append({\n",
    "                    \"habitat_code\": code,\n",
    "                    \"habitat_desc_en\": row.get(\"description_en\"),\n",
    "                    \"assessment_id\": a.get(\"assessment_id\"),\n",
    "                    \"sis_taxon_id\": a.get(\"sis_taxon_id\"),\n",
    "                    \"year_published\": a.get(\"year_published\"),\n",
    "                    \"latest\": a.get(\"latest\"),\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed habitat {code}: {e}\")\n",
    "\n",
    "    raw_df = pd.DataFrame(all_assess_raw).dropna(subset=[\"assessment_id\"])\n",
    "    if raw_df.empty:\n",
    "        raise RuntimeError(\"No assessments returned for selected habitats.\")\n",
    "    raw_df[\"assessment_id\"] = raw_df[\"assessment_id\"].astype(int)\n",
    "    raw_df = raw_df.drop_duplicates()\n",
    "    raw_df.to_csv(OUT_ASSESS_RAW, index=False)\n",
    "\n",
    "    # 3) Keep latest assessment per species\n",
    "    latest_idx = (raw_df\n",
    "        .sort_values([\"sis_taxon_id\", \"year_published\", \"latest\", \"assessment_id\"],\n",
    "                     ascending=[True, False, False, False])\n",
    "        .groupby(\"sis_taxon_id\", as_index=False)\n",
    "        .head(1)\n",
    "        .index\n",
    "    )\n",
    "    latest_df = raw_df.loc[latest_idx].reset_index(drop=True)\n",
    "\n",
    "    # 4) Enrich with detail (status, names, trend, locations, threats)\n",
    "    enriched = []\n",
    "    for _, r in tqdm(latest_df.iterrows(), total=len(latest_df), desc=\"Details\"):\n",
    "        aid = int(r[\"assessment_id\"])\n",
    "        try:\n",
    "            det = get_assessment_detail(aid)\n",
    "        except Exception as e:\n",
    "            print(f\"Detail failed {aid}: {e}\")\n",
    "            continue\n",
    "\n",
    "        red = det.get(\"red_list_category\", {}) or {}\n",
    "        tax = det.get(\"taxon\", {}) or {}\n",
    "        common_names = tax.get(\"common_names\", []) or []\n",
    "        eng_common = next((n[\"name\"] for n in common_names if n.get(\"language\") == \"eng\"), None)\n",
    "        locations = extract_locations(det)\n",
    "        threat_codes = extract_threat_codes(det)\n",
    "\n",
    "        pop_trend_val = det.get(\"population_trend\")\n",
    "        if isinstance(pop_trend_val, dict):\n",
    "            pop_trend_val = pop_trend_val.get(\"code\")\n",
    "\n",
    "        enriched.append({\n",
    "            \"assessment_id\": aid,\n",
    "            \"sis_taxon_id\": det.get(\"sis_taxon_id\"),\n",
    "            \"scientific_name\": tax.get(\"scientific_name\"),\n",
    "            \"english_common_name\": eng_common,\n",
    "            \"class_name\": tax.get(\"class_name\"),\n",
    "            \"order_name\": tax.get(\"order_name\"),\n",
    "            \"family_name\": tax.get(\"family_name\"),\n",
    "            \"year_published\": det.get(\"year_published\"),\n",
    "            \"status_code\": red.get(\"code\"),\n",
    "            \"status_text\": red.get(\"text\"),\n",
    "            \"population_trend\": pop_trend_val,\n",
    "            \"locations_iso2\": \",\".join(locations) if locations else None,\n",
    "            \"threat_codes\": \",\".join(threat_codes) if threat_codes else None,\n",
    "            \"habitat_code\": r.get(\"habitat_code\"),\n",
    "            \"habitat_desc_en\": r.get(\"habitat_desc_en\"),\n",
    "            \"detail_url\": det.get(\"url\"),\n",
    "            \"is_latest\": bool(r.get(\"latest\")),\n",
    "        })\n",
    "\n",
    "    species_df = pd.DataFrame(enriched).drop_duplicates(subset=[\"sis_taxon_id\"]).reset_index(drop=True)\n",
    "\n",
    "    if species_df.empty:\n",
    "        raise RuntimeError(\"No species details assembled. Check token/permissions or reduce filters.\")\n",
    "\n",
    "    # 5) Optional polar location filter\n",
    "    if USE_POLAR_LOCATION_FILTER:\n",
    "        def _has_polar_loc(s):\n",
    "            if not isinstance(s, str) or not s:\n",
    "                return False\n",
    "            codes = {c.strip() for c in s.split(\",\") if c.strip()}\n",
    "            return bool(codes & POLAR_ISO2)\n",
    "        species_df = species_df[species_df[\"locations_iso2\"].apply(_has_polar_loc)].reset_index(drop=True)\n",
    "\n",
    "    # 6) Save outputs\n",
    "    species_df = species_df.sort_values(\n",
    "        [\"status_code\", \"class_name\", \"scientific_name\"], na_position=\"last\"\n",
    "    )\n",
    "    species_df.to_csv(OUT_SPECIES_CSV, index=False)\n",
    "\n",
    "    minimal_records = species_df[[\n",
    "        \"sis_taxon_id\", \"scientific_name\", \"english_common_name\",\n",
    "        \"class_name\", \"order_name\", \"family_name\",\n",
    "        \"status_code\", \"status_text\", \"population_trend\",\n",
    "        \"year_published\", \"habitat_code\", \"habitat_desc_en\",\n",
    "        \"locations_iso2\", \"threat_codes\", \"detail_url\"\n",
    "    ]].to_dict(orient=\"records\")\n",
    "    with open(OUT_SPECIES_JSON, \"w\") as f:\n",
    "        json.dump(minimal_records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nSaved: {OUT_SPECIES_CSV}\\nSaved: {OUT_SPECIES_JSON}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3e8cb-e98a-4957-adcf-c24dc60ef729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"../Data/sea_ice_species_latest.csv\")\n",
    "\n",
    "# Keep only latest records\n",
    "latest = df.query(\"is_latest\").copy()\n",
    "\n",
    "# Mapping for IUCN threat codes\n",
    "threats_mapping = {\n",
    "    'DD': 'Data Deficient',\n",
    "    'LC': 'Least Concern',\n",
    "    'NT': 'Near Threatened',\n",
    "    'VU': 'Vulnerable',\n",
    "    'EN': 'Endangered',\n",
    "    'CR': 'Critically Endangered',\n",
    "    'EW': 'Extinct in the Wild',\n",
    "    'EX': 'Extinct',\n",
    "    'NE': 'Not Evaluated',\n",
    "}\n",
    "\n",
    "# Map threat codes to labels\n",
    "latest['Threat'] = latest['status_code'].replace(threats_mapping)\n",
    "\n",
    "# Map numeric trend codes to text labels\n",
    "latest.loc[latest[\"population_trend\"] == 0, 'Trend'] = \"Increasing\"\n",
    "latest.loc[latest[\"population_trend\"] == 1, 'Trend'] = \"Decreasing\"\n",
    "latest.loc[latest[\"population_trend\"] == 2, 'Trend'] = \"Stable\"\n",
    "latest.loc[latest[\"population_trend\"] == 3, 'Trend'] = \"Unknown\"\n",
    "\n",
    "# Deduplicate by sis_taxon_id to avoid repeats\n",
    "latest = latest.drop_duplicates(subset=\"sis_taxon_id\", keep=\"first\")\n",
    "\n",
    "# Keep only relevant columns\n",
    "out = latest[[\"english_common_name\", \"Threat\", \"Trend\"]].copy()\n",
    "\n",
    "# Save to JSON\n",
    "out.to_json(\"../Data/Figure_6_sea_ice.json\", orient=\"records\", force_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ Saved {len(out)} unique species to Figure_6_sea_ice.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97f0e7-8d13-4124-9e77-39da73c798e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest.query(\"status_code == 'EN'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf47db-67a6-4d95-9e20-f8a5b192cb97",
   "metadata": {},
   "source": [
    "## Figure 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d68e58-f3cb-4f4a-a24e-b02395ff1d21",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_ice_7.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5728204-537e-4933-92ba-1162cb4d120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "trends_df = xr.open_dataset(\"../Data/global_omi_tempsal_sst_trend_19932021_P20220331.nc\")\n",
    "\n",
    "# --------------------\n",
    "# CONFIG\n",
    "# --------------------\n",
    "OUTPUT_TIF_8BIT   = \"../Data/Figure_7_sea_ice.tif\"\n",
    "OUTPUT_TIF_FLOAT  = \"../Data/sst_trend_float32.tif\"\n",
    "USE_PERCENTILES   = True\n",
    "P_LOW, P_HIGH     = 2, 98\n",
    "VMIN_FIXED, VMAX_FIXED = -0.5, 0.5  # if USE_PERCENTILES=False\n",
    "\n",
    "# 1) Select the dataarray\n",
    "da = trends_df[\"sst_trends\"]\n",
    "if \"time\" in da.dims:\n",
    "    da = da.mean(dim=\"time\")\n",
    "\n",
    "# 2) Rename to x/y if needed\n",
    "rename_map = {}\n",
    "if \"lat\" in da.dims or \"lat\" in da.coords: rename_map[\"lat\"] = \"y\"\n",
    "if \"latitude\" in da.dims or \"latitude\" in da.coords: rename_map[\"latitude\"] = \"y\"\n",
    "if \"lon\" in da.dims or \"lon\" in da.coords: rename_map[\"lon\"] = \"x\"\n",
    "if \"longitude\" in da.dims or \"longitude\" in da.coords: rename_map[\"longitude\"] = \"x\"\n",
    "if rename_map:\n",
    "    da = da.rename(rename_map)\n",
    "\n",
    "# 3) Ensure y is north→south (descending)\n",
    "if da[\"y\"].values[0] < da[\"y\"].values[-1]:\n",
    "    da = da.sortby(\"y\", ascending=False)\n",
    "\n",
    "# 4) Register spatial dims & CRS\n",
    "da = da.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "if da.rio.crs is None:\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "\n",
    "# 5) Prepare data and scaling\n",
    "data = da.data.astype(np.float32)\n",
    "valid = np.isfinite(data)\n",
    "\n",
    "if USE_PERCENTILES:\n",
    "    vmin = float(np.nanpercentile(data, P_LOW))\n",
    "    vmax = float(np.nanpercentile(data, P_HIGH))\n",
    "else:\n",
    "    vmin, vmax = float(VMIN_FIXED), float(VMAX_FIXED)\n",
    "\n",
    "if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "    raise ValueError(f\"Bad scaling range: vmin={vmin}, vmax={vmax}\")\n",
    "\n",
    "# 6) Build the 8-bit layer (reserve 0 for NoData)\n",
    "scaled = np.zeros_like(data, dtype=np.uint8)  # 0 = NoData\n",
    "scaled_valid = (np.clip((data[valid] - vmin) / (vmax - vmin), 0.0, 1.0) * 254 + 1).astype(np.uint8)\n",
    "scaled[valid] = scaled_valid\n",
    "\n",
    "da8 = da.copy(data=scaled)\n",
    "\n",
    "# --- CRITICAL: clear CF encodings that carry a massive _FillValue ---\n",
    "da8.encoding.pop(\"_FillValue\", None)\n",
    "da8.encoding.pop(\"missing_value\", None)\n",
    "# (optional) also clear scale/offset if present\n",
    "da8.encoding.pop(\"scale_factor\", None)\n",
    "da8.encoding.pop(\"add_offset\", None)\n",
    "\n",
    "# Set nodata appropriate for uint8 (0)\n",
    "da8 = da8.rio.write_nodata(0, encoded=False, inplace=False)\n",
    "\n",
    "# 7a) Write 8-bit GeoTIFF\n",
    "da8.rio.to_raster(\n",
    "    OUTPUT_TIF_8BIT,\n",
    "    dtype=\"uint8\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "# 7b) Optional: write float32 with native values\n",
    "daf = da.where(np.isfinite(da))\n",
    "# Clear encodings here too\n",
    "daf.encoding.pop(\"_FillValue\", None)\n",
    "daf.encoding.pop(\"missing_value\", None)\n",
    "daf.encoding.pop(\"scale_factor\", None)\n",
    "daf.encoding.pop(\"add_offset\", None)\n",
    "\n",
    "# Use NaN as nodata for float32 (supported by rasterio/GTiff)\n",
    "daf = daf.rio.write_nodata(np.nan, encoded=False, inplace=False)\n",
    "\n",
    "daf.rio.to_raster(\n",
    "    OUTPUT_TIF_FLOAT,\n",
    "    dtype=\"float32\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Wrote {OUTPUT_TIF_8BIT} (uint8; 0=NoData, 1–255=data) and {OUTPUT_TIF_FLOAT} (float32). \"\n",
    "    f\"Scale for 8-bit: vmin={vmin:.4f}, vmax={vmax:.4f} (units of sst_trends).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d98c76-58c8-4e75-ac05-c711e2bbbcd7",
   "metadata": {},
   "source": [
    "## Figure 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84afd2a-56ca-4cbe-9f16-6590e86e406e",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_ice_8.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04a09d-8e7b-4289-b5b1-e71f9fdbbf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copernicus Climate Change Service, Climate Data Store, (2018): Sea level daily gridded data from satellite observations for the global ocean from 1993 to present. Copernicus Climate Change Service (C3S) Climate Data Store (CDS)\n",
    "SLR_df = xr.open_mfdataset(\"../Data/dataset-satellite-sea-level-global-dc7f92ea-2d3e-4fc6-b767-836a5b8c0bff/*.nc\")\n",
    "\n",
    "# Adjust longitudes from 0-360 to -180 to 180\n",
    "SLR_df = SLR_df.assign_coords(longitude=(((SLR_df.longitude + 180) % 360) - 180)).sortby('longitude')\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(SLR_df['sla'].load())\n",
    "\n",
    "# --------------------\n",
    "# CONFIG\n",
    "# --------------------\n",
    "OUTPUT_TIF_8BIT   = \"./Figure_8_sea_ice.tif\"\n",
    "OUTPUT_TIF_FLOAT  = \"./SLR_trend_float32.tif\"\n",
    "USE_PERCENTILES   = True\n",
    "P_LOW, P_HIGH     = 2, 98\n",
    "VMIN_FIXED, VMAX_FIXED = -0.5, 0.5  # if USE_PERCENTILES=False\n",
    "\n",
    "# 2) Rename to x/y if needed\n",
    "rename_map = {}\n",
    "if \"lat\" in da.dims or \"lat\" in da.coords: rename_map[\"lat\"] = \"y\"\n",
    "if \"latitude\" in da.dims or \"latitude\" in da.coords: rename_map[\"latitude\"] = \"y\"\n",
    "if \"lon\" in da.dims or \"lon\" in da.coords: rename_map[\"lon\"] = \"x\"\n",
    "if \"longitude\" in da.dims or \"longitude\" in da.coords: rename_map[\"longitude\"] = \"x\"\n",
    "if rename_map:\n",
    "    da = da.rename(rename_map)\n",
    "\n",
    "# 3) Ensure y is north→south (descending)\n",
    "if da[\"y\"].values[0] < da[\"y\"].values[-1]:\n",
    "    da = da.sortby(\"y\", ascending=False)\n",
    "\n",
    "# 4) Register spatial dims & CRS\n",
    "da = da.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "if da.rio.crs is None:\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "\n",
    "# 5) Prepare data and scaling\n",
    "data = da.data.astype(np.float32)\n",
    "valid = np.isfinite(data)\n",
    "\n",
    "if USE_PERCENTILES:\n",
    "    vmin = float(np.nanpercentile(data, P_LOW))\n",
    "    vmax = float(np.nanpercentile(data, P_HIGH))\n",
    "else:\n",
    "    vmin, vmax = float(VMIN_FIXED), float(VMAX_FIXED)\n",
    "\n",
    "if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "    raise ValueError(f\"Bad scaling range: vmin={vmin}, vmax={vmax}\")\n",
    "\n",
    "# 6) Build the 8-bit layer (reserve 0 for NoData)\n",
    "scaled = np.zeros_like(data, dtype=np.uint8)  # 0 = NoData\n",
    "scaled_valid = (np.clip((data[valid] - vmin) / (vmax - vmin), 0.0, 1.0) * 254 + 1).astype(np.uint8)\n",
    "scaled[valid] = scaled_valid\n",
    "\n",
    "da8 = da.copy(data=scaled)\n",
    "\n",
    "# --- CRITICAL: clear CF encodings that carry a massive _FillValue ---\n",
    "da8.encoding.pop(\"_FillValue\", None)\n",
    "da8.encoding.pop(\"missing_value\", None)\n",
    "# (optional) also clear scale/offset if present\n",
    "da8.encoding.pop(\"scale_factor\", None)\n",
    "da8.encoding.pop(\"add_offset\", None)\n",
    "\n",
    "# Set nodata appropriate for uint8 (0)\n",
    "da8 = da8.rio.write_nodata(0, encoded=False, inplace=False)\n",
    "\n",
    "# 7a) Write 8-bit GeoTIFF\n",
    "da8.rio.to_raster(\n",
    "    OUTPUT_TIF_8BIT,\n",
    "    dtype=\"uint8\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "# 7b) Optional: write float32 with native values\n",
    "daf = da.where(np.isfinite(da))\n",
    "# Clear encodings here too\n",
    "daf.encoding.pop(\"_FillValue\", None)\n",
    "daf.encoding.pop(\"missing_value\", None)\n",
    "daf.encoding.pop(\"scale_factor\", None)\n",
    "daf.encoding.pop(\"add_offset\", None)\n",
    "\n",
    "# Use NaN as nodata for float32 (supported by rasterio/GTiff)\n",
    "daf = daf.rio.write_nodata(np.nan, encoded=False, inplace=False)\n",
    "\n",
    "daf.rio.to_raster(\n",
    "    OUTPUT_TIF_FLOAT,\n",
    "    dtype=\"float32\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Wrote {OUTPUT_TIF_8BIT} (uint8; 0=NoData, 1–255=data) and {OUTPUT_TIF_FLOAT} (float32). \"\n",
    "    f\"Scale for 8-bit: vmin={vmin:.4f}, vmax={vmax:.4f} (units of sst_trends).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68417ca-9f9c-459f-8bd0-b826c93227bf",
   "metadata": {},
   "source": [
    "## Figure 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de1d70e-34b2-4859-a2ea-48a308b7ab9e",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_ice_9.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714fc796-134f-426c-bd4b-f95bbd7df3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.stats import linregress\n",
    "\n",
    "a = xr.open_mfdataset(\"../Data/CERES_EBAF_Edition4.2_200003-202407.nc\")\n",
    "\n",
    "diff = a[\"solar_mon\"] - a[\"toa_sw_all_mon\"]\n",
    "diff_ann = diff.resample(time=\"YS\").mean(\"time\")\n",
    "\n",
    "# Make time a single chunk (safe & cheap)\n",
    "diff_ann = diff_ann.chunk({\"time\": -1})\n",
    "\n",
    "years = (diff_ann.time.dt.year if hasattr(diff_ann.time, \"dt\")\n",
    "         else xr.DataArray([t.year for t in diff_ann.time.values], coords={\"time\": diff_ann.time}, dims=\"time\")).astype(float)\n",
    "x = xr.DataArray(years, coords={\"time\": diff_ann.time}, dims=\"time\").chunk({\"time\": -1})\n",
    "\n",
    "def linregress_1d(y, x):\n",
    "    m = np.isfinite(y) & np.isfinite(x)\n",
    "    if m.sum() < 2:\n",
    "        return np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    r = linregress(x[m], y[m])\n",
    "    return r.slope, r.intercept, r.rvalue, r.pvalue, r.stderr\n",
    "\n",
    "slope, intercept, r, p, stderr = xr.apply_ufunc(\n",
    "    linregress_1d, diff_ann, x,\n",
    "    input_core_dims=[[\"time\"], [\"time\"]],\n",
    "    output_core_dims=[[], [], [], [], []],\n",
    "    vectorize=True,\n",
    "    dask=\"parallelized\",\n",
    "    output_dtypes=[float, float, float, float, float],\n",
    "    # If you can’t rechunk time (above), enable this instead:\n",
    "    # dask_gufunc_kwargs={\"allow_rechunk\": True}\n",
    ")\n",
    "\n",
    "trend = xr.Dataset(\n",
    "    {\n",
    "        \"slope_per_year\": slope,\n",
    "        \"slope_per_decade\": slope * 10.0,\n",
    "        \"intercept\": intercept,\n",
    "        \"r\": r, \"p\": p, \"stderr\": stderr,\n",
    "        \"n_obs\": diff_ann.count(\"time\"),\n",
    "        \"y_mean\": diff_ann.mean(\"time\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "# --------------------\n",
    "# CONFIG\n",
    "# --------------------\n",
    "OUTPUT_TIF_8BIT   = \"../Data/Figure_9_sea_ice.tif\"\n",
    "OUTPUT_TIF_FLOAT  = \"../Data/slope_per_decade_float32.tif\"\n",
    "\n",
    "USE_PERCENTILES   = True\n",
    "P_LOW, P_HIGH     = 2, 98\n",
    "VMIN_FIXED, VMAX_FIXED = -0.2, 0.2  # adjust to your units/range if not using percentiles\n",
    "\n",
    "# If your data are dask-backed & huge, consider using xarray's quantile (DASK_SAFE_PERCENTILES=True)\n",
    "DASK_SAFE_PERCENTILES = True\n",
    "\n",
    "# 1) Select the dataarray\n",
    "da = trend[\"slope_per_decade\"]  # (lat, lon) or (latitude, longitude)\n",
    "if \"time\" in da.dims:\n",
    "    da = da.mean(dim=\"time\")\n",
    "\n",
    "# 2) Rename to x/y if needed\n",
    "rename_map = {}\n",
    "if \"lat\" in da.dims or \"lat\" in da.coords: rename_map[\"lat\"] = \"y\"\n",
    "if \"latitude\" in da.dims or \"latitude\" in da.coords: rename_map[\"latitude\"] = \"y\"\n",
    "if \"lon\" in da.dims or \"lon\" in da.coords: rename_map[\"lon\"] = \"x\"\n",
    "if \"longitude\" in da.dims or \"longitude\" in da.coords: rename_map[\"longitude\"] = \"x\"\n",
    "if rename_map:\n",
    "    da = da.rename(rename_map)\n",
    "\n",
    "# 2b) Wrap longitudes from [0, 360) to [-180, 180) and sort west→east\n",
    "if \"x\" in da.coords:\n",
    "    x_wrapped = ((da[\"x\"] + 180) % 360) - 180  # 0..360 -> -180..180\n",
    "    da = da.assign_coords(x=x_wrapped).sortby(\"x\")\n",
    "\n",
    "\n",
    "# 3) Ensure y is north→south (descending)\n",
    "if da[\"y\"].values[0] < da[\"y\"].values[-1]:\n",
    "    da = da.sortby(\"y\", ascending=False)\n",
    "\n",
    "# 4) Register spatial dims & CRS\n",
    "da = da.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "if da.rio.crs is None:\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "\n",
    "# 5) Determine scaling range\n",
    "if USE_PERCENTILES:\n",
    "    if DASK_SAFE_PERCENTILES:\n",
    "        vmin = float(da.quantile(P_LOW / 100.0, skipna=True).compute())\n",
    "        vmax = float(da.quantile(P_HIGH / 100.0, skipna=True).compute())\n",
    "    else:\n",
    "        # Pull into memory (fast/simple, but not great for very large rasters)\n",
    "        data_tmp = da.data\n",
    "        if hasattr(data_tmp, \"compute\"):  # dask array\n",
    "            data_tmp = data_tmp.compute()\n",
    "        data_tmp = data_tmp.astype(np.float32)\n",
    "        vmin = float(np.nanpercentile(data_tmp, P_LOW))\n",
    "        vmax = float(np.nanpercentile(data_tmp, P_HIGH))\n",
    "else:\n",
    "    vmin, vmax = float(VMIN_FIXED), float(VMAX_FIXED)\n",
    "\n",
    "if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "    raise ValueError(f\"Bad scaling range: vmin={vmin}, vmax={vmax}\")\n",
    "\n",
    "# 6) Build the 8-bit layer (0 = NoData, 1..255 = scaled data)\n",
    "data = da.data\n",
    "if hasattr(data, \"compute\"):  # dask -> numpy\n",
    "    data = data.compute()\n",
    "data = data.astype(np.float32)\n",
    "\n",
    "valid = np.isfinite(data)\n",
    "scaled = np.zeros_like(data, dtype=np.uint8)  # 0 = NoData\n",
    "scaled_valid = (np.clip((data[valid] - vmin) / (vmax - vmin), 0.0, 1.0) * 254 + 1).astype(np.uint8)\n",
    "scaled[valid] = scaled_valid\n",
    "\n",
    "da8 = da.copy(data=scaled)\n",
    "\n",
    "# Clear encodings that can cause huge _FillValue / scaling metadata\n",
    "for k in (\"_FillValue\", \"missing_value\", \"scale_factor\", \"add_offset\"):\n",
    "    da8.encoding.pop(k, None)\n",
    "\n",
    "# Set nodata for uint8\n",
    "da8 = da8.rio.write_nodata(0, encoded=False, inplace=False)\n",
    "\n",
    "# 7a) Write 8-bit GeoTIFF (Mapbox-friendly)\n",
    "da8.rio.to_raster(\n",
    "    OUTPUT_TIF_8BIT,\n",
    "    dtype=\"uint8\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "# 7b) Optional: write float32 with native values\n",
    "daf = da.where(np.isfinite(da))\n",
    "for k in (\"_FillValue\", \"missing_value\", \"scale_factor\", \"add_offset\"):\n",
    "    daf.encoding.pop(k, None)\n",
    "\n",
    "# Use NaN as nodata for float32\n",
    "daf = daf.rio.write_nodata(np.nan, encoded=False, inplace=False)\n",
    "\n",
    "daf.rio.to_raster(\n",
    "    OUTPUT_TIF_FLOAT,\n",
    "    dtype=\"float32\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Wrote {OUTPUT_TIF_8BIT} (uint8; 0=NoData, 1–255=data) and {OUTPUT_TIF_FLOAT} (float32). \"\n",
    "    f\"8-bit scale: vmin={vmin:.6g}, vmax={vmax:.6g} (units of slope_per_decade).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4650e9-87a6-4b53-9c50-f6b0b4a487d0",
   "metadata": {},
   "source": [
    "## Figure 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39737cd1-7720-4fe7-8e3a-403398131ea1",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_ice_10.png\" style=\"width:50%;\">\n",
    "</p>\n",
    "\n",
    "Data were retrieved from [IUCN SSC PBSG](https://www.iucn-pbsg.org/population-status/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246fcf2-5d40-4c45-9428-5b1f8fe83f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333777a0-ab9d-459a-afd0-34be2fef2894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792b589-c21c-47c6-bde1-d0db20b7ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file(\"../Data/Figure_3_acidity.geojson\")\n",
    "\n",
    "(df['Biodiversity_Area'].sum() / df['Sea_Area'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a9c0e-efee-49f9-a9b2-25710d769864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel",
   "language": "python",
   "name": "claymodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
