{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a68d863-6d59-4533-a5a1-4f616f8e6808",
   "metadata": {},
   "source": [
    "# Mitigate Climate Change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe067026-4fd8-4760-b32a-46c11dfd2b63",
   "metadata": {},
   "source": [
    "This notebook outlines the general workflow for the data within the [Mitigate Climate Change](https://oceancentral.org/track/mitigate-climate-change) page of the Ocean Central website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c137a64-8efa-4766-a781-d5f48faa57d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Utils functions and globals used for making all figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeacb4e-82f3-4b15-9fec-3551c390abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import box\n",
    "import rioxarray\n",
    "import re\n",
    "\n",
    "from rasterio.features import geometry_mask\n",
    "from scipy.stats import linregress\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Open the biodiversity priority areas based on Zhao et al. 2020 (https://www.sciencedirect.com/science/article/abs/pii/S0006320719312182?via%3Dihub)\n",
    "masked_data = rioxarray.open_rasterio('../Data/masked_top_30_percent_over_water.tif')\n",
    "\n",
    "# Set the CRS for masked_data if it's not already set\n",
    "if 'crs' not in masked_data.attrs:\n",
    "    masked_data.rio.write_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "# Load SST dataset and EEZ shapefile\n",
    "seas_shapefile_path = '../Data/World_Seas_IHO_v3/World_Seas_IHO_v3.shp'\n",
    "SEAS_DF = gpd.read_file(seas_shapefile_path)\n",
    "\n",
    "# Calculate linear trend and p-value for each grid point\n",
    "def calculate_trend_and_significance(x):\n",
    "    if np.isnan(x).all():\n",
    "        return np.nan, np.nan, np.nan\n",
    "    else:\n",
    "        slope, intercept, _, p_value, _ = stats.linregress(range(len(x)), x)\n",
    "        return slope, intercept, p_value\n",
    "\n",
    "# Calculate the trend and significance of the trend at each pixel in an xarray dataset\n",
    "def calculate_trend_df(climate_df):\n",
    "    df_mean = climate_df.groupby('time.year').mean()\n",
    "    \n",
    "    # Apply the trend and p-value calculation to the entire dataset\n",
    "    results = xr.apply_ufunc(\n",
    "        calculate_trend_and_significance,\n",
    "        df_mean,\n",
    "        input_core_dims=[['year']],\n",
    "        vectorize=True,\n",
    "        output_core_dims=[[], [], []],\n",
    "        output_dtypes=[float, float, float]\n",
    "    )\n",
    "    \n",
    "    # Extract the trend and p-value into separate DataArrays\n",
    "    trends_da = xr.DataArray(results[0], coords=df_mean.isel(year=0).coords, name='trend')\n",
    "    pvalues_da = xr.DataArray(results[2], coords=df_mean.isel(year=0).coords, name='p_value')\n",
    "    \n",
    "    # Create a significance mask where p-value < 0.05\n",
    "    significant_da = xr.DataArray((pvalues_da < 0.05), coords=pvalues_da.coords, name='significant')\n",
    "    \n",
    "    # Combine trend, p-value, and significance mask into a single dataset\n",
    "    trend_significance_ds = xr.Dataset({\n",
    "        'trend': trends_da,\n",
    "        'p_value': pvalues_da,\n",
    "        'significant': significant_da\n",
    "    })\n",
    "    \n",
    "    # Set the CRS for the trends dataset to match the EEZ CRS\n",
    "    trend_significance_ds = trend_significance_ds.rio.write_crs(\"epsg:4326\")\n",
    "    return trend_significance_ds\n",
    "\n",
    "# Calculate area-weighted trend, significance for each sea/ocean area\n",
    "def area_trend(trend_significance_ds, SEAS_DF=SEAS_DF):\n",
    "    # Iterate over each sea/ocean area and calculate the area-weighted trend and significant area percentage\n",
    "    area_weighted_trends = []\n",
    "    \n",
    "    # Check if 'lat' and 'lon' are in the dataset, otherwise check for 'latitude' and 'longitude'\n",
    "    if 'lat' in trend_significance_ds.dims and 'lon' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'lat': 'y', 'lon': 'x'})\n",
    "    elif 'latitude' in trend_significance_ds.dims and 'longitude' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "\n",
    "    # Interpolate biodiversity priority areas to the same resolution as the climate data\n",
    "    masked_data_interp = masked_data.interp(\n",
    "        x=trend_significance_ds['x'],\n",
    "        y=trend_significance_ds['y'],\n",
    "        method='nearest'\n",
    "    )\n",
    "\n",
    "    # Calculate the area for each grid cell (assumes lat/lon grid)\n",
    "    lat = trend_significance_ds['y'].values\n",
    "    lon = trend_significance_ds['x'].values\n",
    "    \n",
    "    # Calculate grid cell area using Haversine formula or by approximation\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    \n",
    "    # Earth radius in kilometers\n",
    "    R = 6371\n",
    "    dlat = np.gradient(lat_rad)\n",
    "    dlon = np.gradient(lon_rad)\n",
    "    \n",
    "    # Approximate area calculation\n",
    "    cell_areas = (R**2 * np.outer(np.sin(dlat), dlon)) * np.cos(lat_rad[:, None])\n",
    "    \n",
    "    for i, row in SEAS_DF.iterrows():\n",
    "        try:\n",
    "            region_name = row['NAME']\n",
    "            area = row['area']\n",
    "            geom = row['geometry']\n",
    "    \n",
    "            # Mask SST trends with the sea geometry\n",
    "            masked_trends = trend_significance_ds['trend'].rio.clip([geom], drop=True)\n",
    "            masked_significance = trend_significance_ds['significant'].rio.clip([geom], drop=True)\n",
    "    \n",
    "            # Clip cell_areas to the same extent as masked_trends\n",
    "            cell_areas_clipped = xr.DataArray(\n",
    "                cell_areas, \n",
    "                dims=['y', 'x'], \n",
    "                coords={'y': trend_significance_ds['y'], 'x': trend_significance_ds['x']}\n",
    "            )\n",
    "            \n",
    "            # Set CRS for cell_areas_clipped to match the CRS of trend_significance_ds\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.write_crs('EPSG:4326')\n",
    "    \n",
    "            # Clip cell_areas to the same geometry\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.clip([geom], drop=True)\n",
    "        \n",
    "            # Compute the area-weighted trend\n",
    "            weighted_trend = (masked_trends * cell_areas_clipped).sum(dim=('y', 'x')) / cell_areas_clipped.sum()\n",
    "    \n",
    "            # Compute the total area that is significant\n",
    "            significant_masked_areas = (masked_significance * cell_areas_clipped).where(masked_significance, 0)\n",
    "            total_significant_area = significant_masked_areas.sum(dim=('y', 'x')).item()\n",
    "    \n",
    "            # Calculate the percentage of the area that is significant\n",
    "            total_area = cell_areas_clipped.sum()\n",
    "            significant_area_percent = (total_significant_area / total_area) * 100\n",
    "    \n",
    "            # Calculate the area for biodiversity based on the mask\n",
    "            area_biodiversity = ((masked_significance * cell_areas_clipped) * masked_data_interp).sum(dim=['x', 'y']).values\n",
    "    \n",
    "            # Store the result\n",
    "            area_weighted_trends.append({\n",
    "                'Region_Name': region_name,\n",
    "                'geometry': geom,\n",
    "                'Weighted_Trend': weighted_trend.item(),\n",
    "                'Sea_Area': area,\n",
    "                'Significant_Area': area*total_significant_area/total_area.item(),\n",
    "                'Significant_Area_Percent': 100*total_significant_area/total_area.item(),\n",
    "                'Biodiversity_Area': area*area_biodiversity[0]/total_area.item(),\n",
    "                'Biodiversity_Area_Percent': 100*area_biodiversity[0]/total_area.item(),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Convert the results to a GeoDataFrame for easy viewing\n",
    "    area_weighted_trends_gdf = gpd.GeoDataFrame(area_weighted_trends, crs=SEAS_DF.crs)\n",
    "    return area_weighted_trends_gdf\n",
    "\n",
    "def area_heatwave(temp_df, SEAS_DF=SEAS_DF):\n",
    "    area_heatwave = []\n",
    "\n",
    "    # Set CRS and rename dimensions and coordinates\n",
    "    temp_df = temp_df.rio.write_crs(\"epsg:4326\")\n",
    "    temp_df = temp_df.rename({'latdim': 'y', 'londim': 'x'}).rename({'lat': 'y', 'lon': 'x'})  # Adjust based on your dimensions\n",
    "\n",
    "    # Select heatwave categories >= 3 and aggregate over time\n",
    "    temp_df = (temp_df['heatwave_category'] >= 3).any(dim='time')\n",
    "\n",
    "    # Interpolate biodiversity priority areas to the same resolution as the climate data\n",
    "    masked_data_interp = masked_data.interp(\n",
    "        x=temp_df['x'],\n",
    "        y=temp_df['y'],\n",
    "        method='nearest'\n",
    "    )\n",
    "\n",
    "    # Calculate the area for each grid cell (assumes lat/lon grid)\n",
    "    lat = temp_df['y'].values\n",
    "    lon = temp_df['x'].values\n",
    "    \n",
    "    # Calculate grid cell area using Haversine formula or by approximation\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    \n",
    "    # Earth radius in kilometers\n",
    "    R = 6371\n",
    "    dlat = np.gradient(lat_rad)\n",
    "    dlon = np.gradient(lon_rad)\n",
    "    \n",
    "    # Approximate area calculation\n",
    "    cell_areas = (R**2 * np.outer(np.sin(dlat), dlon)) * np.cos(lat_rad[:, None])\n",
    "    \n",
    "    # Use tqdm to track progress through SEAS_DF.iterrows()\n",
    "    for i, row in tqdm(SEAS_DF.iterrows(), total=len(SEAS_DF), desc=\"Processing Sea Areas\"):\n",
    "        try:\n",
    "            region_name = row['NAME']\n",
    "            area = row['area']\n",
    "            geom = row['geometry']\n",
    "    \n",
    "            # Mask SST trends with the sea geometry\n",
    "            masked_df = temp_df.rio.clip([geom], drop=True)\n",
    "    \n",
    "            # Clip cell_areas to the same extent as masked_df\n",
    "            cell_areas_clipped = xr.DataArray(\n",
    "                cell_areas, \n",
    "                dims=['y', 'x'], \n",
    "                coords={'y': temp_df['y'], 'x': temp_df['x']}\n",
    "            )\n",
    "            \n",
    "            # Set CRS for cell_areas_clipped to match the CRS of trend_significance_ds\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.write_crs('EPSG:4326')\n",
    "    \n",
    "            # Clip cell_areas to the same geometry\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.clip([geom], drop=True)\n",
    "        \n",
    "            # Compute the total area that is impacted by a severe heatwave\n",
    "            heatwave_area = (masked_df * cell_areas_clipped).sum(dim=('y', 'x')).compute()  # Compute to convert from Dask array\n",
    "    \n",
    "            # Calculate the area for biodiversity based on the mask\n",
    "            area_biodiversity = ((masked_df * cell_areas_clipped) * masked_data_interp).sum(dim=['x', 'y']).compute()\n",
    "\n",
    "            total_area = cell_areas_clipped.sum(dim=('y', 'x')).compute()  # Ensure computation\n",
    "    \n",
    "            # Extract values after computing\n",
    "            heatwave_value = heatwave_area.item() if heatwave_area.size == 1 else heatwave_area.values[0]\n",
    "            total_area_value = total_area.item() if total_area.size == 1 else total_area.values[0]\n",
    "            area_biodiversity = area_biodiversity.item() if area_biodiversity.size == 1 else area_biodiversity.values[0]\n",
    "    \n",
    "            # Store the result\n",
    "            area_heatwave.append({\n",
    "                'Region_Name': region_name,\n",
    "                'geometry': geom,\n",
    "                'Heatwave_Area': area*heatwave_value/total_area_value,\n",
    "                'Heatwave_Area_Percent': 100*heatwave_value/total_area_value,\n",
    "                'Sea_Area': area,\n",
    "                'Biodiversity_Area': area*area_biodiversity/total_area_value,\n",
    "                'Biodiversity_Area_Percent': 100*area_biodiversity/total_area_value,\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {region_name}: {e}\")\n",
    "\n",
    "    # Convert the results to a GeoDataFrame for easy viewing\n",
    "    area_heatwave_gdf = gpd.GeoDataFrame(area_heatwave, crs=SEAS_DF.crs)\n",
    "    return area_heatwave_gdf\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5164ba-c570-424c-a810-d7f172f3de88",
   "metadata": {},
   "source": [
    "# Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c00d4-77cc-4782-a181-9f13bf4e6a9f",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca8ca4-21b0-469a-aefe-266348bb6091",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_1.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c41e63-e685-4d74-9e18-70249b5c02b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_data = pd.read_csv(\"../Data/GISTEMP_SST.csv\") # Data downloaded from GISS Surface Temperature Analysis (v4)\n",
    "gmst_data = pd.read_csv(\"../Data/GMST_GISTEMP4.csv\") # Data downloaded from GISS Surface Temperature Analysis (v4)\n",
    "\n",
    "ocean_data['Ocean_Annual'] = ocean_data['Lowess(5)']\n",
    "gmst_data['GMST_Annual'] = gmst_data['Lowess(5)']\n",
    "\n",
    "temp_data = ocean_data.merge(gmst_data,on='Year')\n",
    "\n",
    "# Calculate the mean of the 'No_Smoothing' column for the period 1880-1900\n",
    "base_period = temp_data[(temp_data['Year'] >= 1880) & (temp_data['Year'] <= 1900)]\n",
    "mean_sst_base_period = base_period['Ocean_Annual'].mean()\n",
    "mean_gmst_base_period = base_period['GMST_Annual'].mean()\n",
    "\n",
    "# Update the 'No_Smoothing' column to be anomalies relative to the period 1880-1900\n",
    "temp_data['Ocean_Annual'] = temp_data['Ocean_Annual'] - mean_sst_base_period\n",
    "temp_data['GMST_Annual'] = temp_data['GMST_Annual'] - mean_gmst_base_period\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(temp_data['Year'], temp_data['Ocean_Annual'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "temp_data['ocean_trend'] = intercept + slope * temp_data['Year']\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(temp_data['Year'], temp_data['GMST_Annual'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "temp_data['gmst_trend'] = intercept + slope * temp_data['Year']\n",
    "temp_data['paris_goal'] = 1.5\n",
    "\n",
    "# Save out as JSON\n",
    "temp_data[['Year','Ocean_Annual','ocean_trend','GMST_Annual','gmst_trend','paris_goal']].to_json(\n",
    "    \"../Data/Figure_1_temperature.json\", \n",
    "    orient=\"records\",  # list of dicts (one per row)\n",
    "    indent=2           # pretty print\n",
    ")\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "temp_data[['Year','Ocean_Annual','ocean_trend','GMST_Annual','gmst_trend','paris_goal']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126ca5c-8d7f-47e6-bf6e-d56ffaa63fe4",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea8702-867b-43ed-b85c-f0289d80a964",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_2.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5f7f6-5fd2-4431-a9ba-f0866c389e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "\n",
    "trends_df = xr.open_dataset(\"../Data/global_omi_tempsal_sst_trend_19932021_P20220331.nc\")\n",
    "\n",
    "# --------------------\n",
    "# CONFIG\n",
    "# --------------------\n",
    "OUTPUT_TIF_8BIT   = \"../Data/Figure_2_temperature.tif\"\n",
    "OUTPUT_TIF_FLOAT  = \"../Data/sst_trend_float32.tif\"\n",
    "USE_PERCENTILES   = True\n",
    "P_LOW, P_HIGH     = 2, 98\n",
    "VMIN_FIXED, VMAX_FIXED = -0.5, 0.5  # if USE_PERCENTILES=False\n",
    "\n",
    "# 1) Select the dataarray\n",
    "da = trends_df[\"sst_trends\"]\n",
    "if \"time\" in da.dims:\n",
    "    da = da.mean(dim=\"time\")\n",
    "\n",
    "# 2) Rename to x/y if needed\n",
    "rename_map = {}\n",
    "if \"lat\" in da.dims or \"lat\" in da.coords: rename_map[\"lat\"] = \"y\"\n",
    "if \"latitude\" in da.dims or \"latitude\" in da.coords: rename_map[\"latitude\"] = \"y\"\n",
    "if \"lon\" in da.dims or \"lon\" in da.coords: rename_map[\"lon\"] = \"x\"\n",
    "if \"longitude\" in da.dims or \"longitude\" in da.coords: rename_map[\"longitude\"] = \"x\"\n",
    "if rename_map:\n",
    "    da = da.rename(rename_map)\n",
    "\n",
    "# 3) Ensure y is north→south (descending)\n",
    "if da[\"y\"].values[0] < da[\"y\"].values[-1]:\n",
    "    da = da.sortby(\"y\", ascending=False)\n",
    "\n",
    "# 4) Register spatial dims & CRS\n",
    "da = da.rio.set_spatial_dims(x_dim=\"x\", y_dim=\"y\", inplace=False)\n",
    "if da.rio.crs is None:\n",
    "    da = da.rio.write_crs(\"EPSG:4326\", inplace=False)\n",
    "\n",
    "# 5) Prepare data and scaling\n",
    "data = da.data.astype(np.float32)\n",
    "valid = np.isfinite(data)\n",
    "\n",
    "if USE_PERCENTILES:\n",
    "    vmin = float(np.nanpercentile(data, P_LOW))\n",
    "    vmax = float(np.nanpercentile(data, P_HIGH))\n",
    "else:\n",
    "    vmin, vmax = float(VMIN_FIXED), float(VMAX_FIXED)\n",
    "\n",
    "if not np.isfinite(vmin) or not np.isfinite(vmax) or vmin >= vmax:\n",
    "    raise ValueError(f\"Bad scaling range: vmin={vmin}, vmax={vmax}\")\n",
    "\n",
    "# 6) Build the 8-bit layer (reserve 0 for NoData)\n",
    "scaled = np.zeros_like(data, dtype=np.uint8)  # 0 = NoData\n",
    "scaled_valid = (np.clip((data[valid] - vmin) / (vmax - vmin), 0.0, 1.0) * 254 + 1).astype(np.uint8)\n",
    "scaled[valid] = scaled_valid\n",
    "\n",
    "da8 = da.copy(data=scaled)\n",
    "\n",
    "# --- CRITICAL: clear CF encodings that carry a massive _FillValue ---\n",
    "da8.encoding.pop(\"_FillValue\", None)\n",
    "da8.encoding.pop(\"missing_value\", None)\n",
    "# (optional) also clear scale/offset if present\n",
    "da8.encoding.pop(\"scale_factor\", None)\n",
    "da8.encoding.pop(\"add_offset\", None)\n",
    "\n",
    "# Set nodata appropriate for uint8 (0)\n",
    "da8 = da8.rio.write_nodata(0, encoded=False, inplace=False)\n",
    "\n",
    "# 7a) Write 8-bit GeoTIFF\n",
    "da8.rio.to_raster(\n",
    "    OUTPUT_TIF_8BIT,\n",
    "    dtype=\"uint8\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "# 7b) Optional: write float32 with native values\n",
    "daf = da.where(np.isfinite(da))\n",
    "# Clear encodings here too\n",
    "daf.encoding.pop(\"_FillValue\", None)\n",
    "daf.encoding.pop(\"missing_value\", None)\n",
    "daf.encoding.pop(\"scale_factor\", None)\n",
    "daf.encoding.pop(\"add_offset\", None)\n",
    "\n",
    "# Use NaN as nodata for float32 (supported by rasterio/GTiff)\n",
    "daf = daf.rio.write_nodata(np.nan, encoded=False, inplace=False)\n",
    "\n",
    "daf.rio.to_raster(\n",
    "    OUTPUT_TIF_FLOAT,\n",
    "    dtype=\"float32\",\n",
    "    compress=\"LZW\",\n",
    "    tiled=True,\n",
    "    blockxsize=256,\n",
    "    blockysize=256,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Wrote {OUTPUT_TIF_8BIT} (uint8; 0=NoData, 1–255=data) and {OUTPUT_TIF_FLOAT} (float32). \"\n",
    "    f\"Scale for 8-bit: vmin={vmin:.4f}, vmax={vmax:.4f} (units of sst_trends).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a6928-80c1-41f0-8931-f136ed10e05f",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a55a1-632f-44d0-8e46-1897367aefa7",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_3.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c1715-7776-4190-9862-0da292a3576f",
   "metadata": {},
   "source": [
    "**This figure calculates the total area for each major sea region globally. It also calculates 1) the area for each sea region impacted by a severe marine heatwave in the year 2023 as well as 2) the area for each sea that both has priority biodiversity areas AND is impacted by a severe marine heatwave.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7329b-be8f-44d0-ba51-8efb83291da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2024 Heatwave data downloaded from NOAA's Coral Reef Watch https://coralreefwatch.noaa.gov/product/marine_heatwave/\n",
    "import xarray as xr\n",
    "import glob\n",
    "\n",
    "files = sorted(glob.glob(\"../Data/2023/*.nc\"))\n",
    "\n",
    "temp_df = xr.open_mfdataset(\"../Data/2023/*.nc\")\n",
    "\n",
    "area_df = area_heatwave(temp_df)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/Figure_3_temperature.geojson\",driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8102994c-1985-4949-a649-f91af99fdaad",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d2692-6a48-4d4e-bb91-66f9922df628",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_4.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f751c1c-b9d9-4d33-8808-06e75751cd34",
   "metadata": {},
   "source": [
    "$CO_2$ data retrieved from [Lan, X., P. Tans, & K.W. Thoning (2025). Trends in globally-averaged CO₂ determined from NOAA Global Monitoring Laboratory measurements (Version 2025-11) NOAA Global Monitoring Laboratory. https://doi.org/10.15138/9N0H-ZH07](https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.txt). Temperature data retrieved from GISS Surface Temperature Analysis (v4)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ec2a1-6206-4554-82a1-3481eba4ff91",
   "metadata": {},
   "source": [
    "## Figure 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94da805-3110-4b81-983f-81d7d337f510",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_5.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc173db-e178-4e15-adf9-e93113e8bd23",
   "metadata": {},
   "source": [
    "Data were retrieved from [Hughes, T. P., et al. (2018). Spatial and temporal patterns of mass bleaching of corals in the Anthropocene. Science. – processed by Our World in Data](https://ourworldindata.org/grapher/coral-bleaching-events)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e83a16-7adb-4cc2-9e13-c146d714186c",
   "metadata": {},
   "source": [
    "## Figure 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c497b-be92-4549-b3b6-54eb7d8a4112",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_6.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b62bdc-ee90-488c-9957-35ea1cb62707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import xarray as xr\n",
    "import rioxarray  # pip install rioxarray rasterio\n",
    "\n",
    "# --- Load and compute heatwave days ---\n",
    "ds = xr.open_mfdataset(\"../Data/2023/*.nc\", combine=\"by_coords\")\n",
    "\n",
    "# If you already have temp_df, you can instead do:\n",
    "# hw_count = temp_df\n",
    "\n",
    "hw_count = (ds[\"heatwave_category\"] >= 1).sum(dim=\"time\")\n",
    "hw_count = hw_count.astype(\"float32\")\n",
    "\n",
    "# --- Rechunk along spatial dims for quantile ---\n",
    "# Identify spatial dimensions (everything except 'time')\n",
    "spatial_dims = [d for d in hw_count.dims if d != \"time\"]\n",
    "\n",
    "# Make each spatial dimension a single chunk\n",
    "hw_q = hw_count.chunk({d: -1 for d in spatial_dims})\n",
    "\n",
    "# --- Compute 2nd and 98th percentiles over space ---\n",
    "q = hw_q.quantile([0.02, 0.98], dim=spatial_dims, skipna=True).compute()\n",
    "\n",
    "p2 = float(q.sel(quantile=0.02).values)\n",
    "p98 = float(q.sel(quantile=0.98).values)\n",
    "\n",
    "print(f\"2nd percentile: {p2}, 98th percentile: {p98}\")\n",
    "\n",
    "if p98 <= p2:\n",
    "    raise ValueError(\"98th percentile is not greater than 2nd percentile; cannot scale safely.\")\n",
    "\n",
    "# --- Scale to 0–255 using the 2nd and 98th percentiles ---\n",
    "scaled = (hw_count - p2) / (p98 - p2) * 255.0\n",
    "scaled = scaled.clip(0, 255)\n",
    "scaled = scaled.fillna(0)\n",
    "\n",
    "# Round and cast to uint8\n",
    "scaled_uint8 = scaled.round().astype(\"uint8\")\n",
    "\n",
    "# --- Set spatial dims and CRS for Mapbox ---\n",
    "# Try to guess your x/y dimension names\n",
    "cands_x = [\"lon\", \"longitude\", \"x\", \"londim\"]\n",
    "cands_y = [\"lat\", \"latitude\", \"y\", \"latdim\"]\n",
    "\n",
    "x_dim = next(d for d in cands_x if d in scaled_uint8.dims)\n",
    "y_dim = next(d for d in cands_y if d in scaled_uint8.dims)\n",
    "\n",
    "scaled_uint8 = scaled_uint8.rio.set_spatial_dims(x_dim=x_dim, y_dim=y_dim, inplace=False)\n",
    "\n",
    "# Ensure WGS84\n",
    "if not scaled_uint8.rio.crs:\n",
    "    scaled_uint8 = scaled_uint8.rio.write_crs(\"EPSG:4326\")\n",
    "\n",
    "# Optional: use 0 as nodata for transparency in Mapbox\n",
    "scaled_uint8 = scaled_uint8.rio.write_nodata(0)\n",
    "\n",
    "# --- Save as GeoTIFF ---\n",
    "out_path = \"Figure_6_temperature.tif\"\n",
    "scaled_uint8.rio.to_raster(out_path, dtype=\"uint8\")\n",
    "\n",
    "print(f\"Saved GeoTIFF: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54339b03-3b65-417a-b87d-2347c4a693f7",
   "metadata": {},
   "source": [
    "## Figure 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a29b4b-602e-477f-a151-d1c6f0b88748",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Figs/climate_temperature_7.png\" style=\"width:50%;\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ffe68-b87c-4c87-b449-d6d27da859aa",
   "metadata": {},
   "source": [
    "Data were retrieved from [Jones et al. (2024) – with major processing by Our World in Data](https://ourworldindata.org/co2-and-greenhouse-gas-emissions#explore-data-on-co2-and-greenhouse-gas-emissions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ecfb0e-b3be-4da6-b26f-5055a618f76d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel",
   "language": "python",
   "name": "claymodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
