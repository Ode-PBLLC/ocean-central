{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a3a59f-1d21-4f4a-b5ee-8bbac98634cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils functions and globals \n",
    "\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import box\n",
    "import rioxarray\n",
    "import re\n",
    "\n",
    "from rasterio.features import geometry_mask\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Open the biodiversity priority areas based on Zhao et al. 2020 (https://www.sciencedirect.com/science/article/abs/pii/S0006320719312182?via%3Dihub)\n",
    "masked_data = rioxarray.open_rasterio('masked_top_30_percent_over_water.tif')\n",
    "\n",
    "# Set the CRS for masked_data if it's not already set\n",
    "if 'crs' not in masked_data.attrs:\n",
    "    masked_data.rio.write_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "# Load SST dataset and EEZ shapefile\n",
    "seas_shapefile_path = '../Data/World_Seas_IHO_v3/World_Seas_IHO_v3.shp'\n",
    "SEAS_DF = gpd.read_file(seas_shapefile_path)\n",
    "\n",
    "# Calculate linear trend and p-value for each grid point\n",
    "def calculate_trend_and_significance(x):\n",
    "    if np.isnan(x).all():\n",
    "        return np.nan, np.nan, np.nan\n",
    "    else:\n",
    "        slope, intercept, _, p_value, _ = stats.linregress(range(len(x)), x)\n",
    "        return slope, intercept, p_value\n",
    "\n",
    "# Calculate the trend and significance of the trend at each pixel in an xarray dataset\n",
    "def calculate_trend_df(climate_df):\n",
    "    df_mean = climate_df.groupby('time.year').mean()\n",
    "    \n",
    "    # Apply the trend and p-value calculation to the entire dataset\n",
    "    results = xr.apply_ufunc(\n",
    "        calculate_trend_and_significance,\n",
    "        df_mean,\n",
    "        input_core_dims=[['year']],\n",
    "        vectorize=True,\n",
    "        output_core_dims=[[], [], []],\n",
    "        output_dtypes=[float, float, float]\n",
    "    )\n",
    "    \n",
    "    # Extract the trend and p-value into separate DataArrays\n",
    "    trends_da = xr.DataArray(results[0], coords=df_mean.isel(year=0).coords, name='trend')\n",
    "    pvalues_da = xr.DataArray(results[2], coords=df_mean.isel(year=0).coords, name='p_value')\n",
    "    \n",
    "    # Create a significance mask where p-value < 0.05\n",
    "    significant_da = xr.DataArray((pvalues_da < 0.05), coords=pvalues_da.coords, name='significant')\n",
    "    \n",
    "    # Combine trend, p-value, and significance mask into a single dataset\n",
    "    trend_significance_ds = xr.Dataset({\n",
    "        'trend': trends_da,\n",
    "        'p_value': pvalues_da,\n",
    "        'significant': significant_da\n",
    "    })\n",
    "    \n",
    "    # Set the CRS for the trends dataset to match the EEZ CRS\n",
    "    trend_significance_ds = trend_significance_ds.rio.write_crs(\"epsg:4326\")\n",
    "    return trend_significance_ds\n",
    "\n",
    "# Calculate area-weighted trend, significance for each sea/ocean area\n",
    "def area_trend(trend_significance_ds, SEAS_DF=SEAS_DF):\n",
    "    # Iterate over each sea/ocean area and calculate the area-weighted trend and significant area percentage\n",
    "    area_weighted_trends = []\n",
    "    \n",
    "    # Check if 'lat' and 'lon' are in the dataset, otherwise check for 'latitude' and 'longitude'\n",
    "    if 'lat' in trend_significance_ds.dims and 'lon' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'lat': 'y', 'lon': 'x'})\n",
    "    elif 'latitude' in trend_significance_ds.dims and 'longitude' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "\n",
    "\n",
    "    # Interpolate biodiversity priority areas to the same resolution as the climate data\n",
    "    masked_data_interp = masked_data.interp(\n",
    "        x=trend_significance_ds['x'],\n",
    "        y=trend_significance_ds['y'],\n",
    "        method='nearest'\n",
    "    )\n",
    "\n",
    "    # Calculate the area for each grid cell (assumes lat/lon grid)\n",
    "    lat = trend_significance_ds['y'].values\n",
    "    lon = trend_significance_ds['x'].values\n",
    "    \n",
    "    # Calculate grid cell area using Haversine formula or by approximation\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    \n",
    "    # Earth radius in kilometers\n",
    "    R = 6371\n",
    "    dlat = np.gradient(lat_rad)\n",
    "    dlon = np.gradient(lon_rad)\n",
    "    \n",
    "    # Approximate area calculation\n",
    "    cell_areas = (R**2 * np.outer(np.sin(dlat), dlon)) * np.cos(lat_rad[:, None])\n",
    "    \n",
    "    for i, row in SEAS_DF.iterrows():\n",
    "        try:\n",
    "            region_name = row['NAME']\n",
    "            area = row['area']\n",
    "            geom = row['geometry']\n",
    "    \n",
    "            # Mask SST trends with the sea geometry\n",
    "            masked_trends = trend_significance_ds['trend'].rio.clip([geom], drop=True)\n",
    "            masked_significance = trend_significance_ds['significant'].rio.clip([geom], drop=True)\n",
    "    \n",
    "            # Clip cell_areas to the same extent as masked_trends\n",
    "            cell_areas_clipped = xr.DataArray(\n",
    "                cell_areas, \n",
    "                dims=['y', 'x'], \n",
    "                coords={'y': trend_significance_ds['y'], 'x': trend_significance_ds['x']}\n",
    "            )\n",
    "            \n",
    "            # Set CRS for cell_areas_clipped to match the CRS of trend_significance_ds\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.write_crs('EPSG:4326')\n",
    "    \n",
    "            # Clip cell_areas to the same geometry\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.clip([geom], drop=True)\n",
    "        \n",
    "            # Compute the area-weighted trend\n",
    "            weighted_trend = (masked_trends * cell_areas_clipped).sum(dim=('y', 'x')) / cell_areas_clipped.sum()\n",
    "    \n",
    "            # Compute the total area that is significant\n",
    "            significant_masked_areas = (masked_significance * cell_areas_clipped).where(masked_significance, 0)\n",
    "            total_significant_area = significant_masked_areas.sum(dim=('y', 'x')).item()\n",
    "    \n",
    "            # Calculate the percentage of the area that is significant\n",
    "            total_area = cell_areas_clipped.sum()\n",
    "            significant_area_percent = (total_significant_area / total_area) * 100\n",
    "    \n",
    "            # Calculate the area for biodiversity based on the mask\n",
    "            area_biodiversity = ((masked_significance * cell_areas_clipped) * masked_data_interp).sum(dim=['x', 'y']).values\n",
    "    \n",
    "            # Store the result\n",
    "            area_weighted_trends.append({\n",
    "                'Region_Name': region_name,\n",
    "                'geometry': geom,\n",
    "                'Weighted_Trend': weighted_trend.item(),\n",
    "                'Sea_Area': total_area.item(),\n",
    "                'Significant_Area': total_significant_area,\n",
    "                'Significant_Area_Percent': significant_area_percent.item(),\n",
    "                'Biodiversity_Area': area_biodiversity[0]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Convert the results to a GeoDataFrame for easy viewing\n",
    "    area_weighted_trends_gdf = gpd.GeoDataFrame(area_weighted_trends, crs=SEAS_DF.crs)\n",
    "    return area_weighted_trends_gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9764e0c4-231e-4959-b242-a1329590e0c6",
   "metadata": {},
   "source": [
    "# Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a6cd5a-88f8-4934-acea-3a68346b7f41",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce6b288a-4493-44eb-a58c-5745788d191c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Ocean_Annual</th>\n",
       "      <th>ocean_trend</th>\n",
       "      <th>GMST_Annual</th>\n",
       "      <th>gmst_trend</th>\n",
       "      <th>paris_goal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1880</td>\n",
       "      <td>0.080476</td>\n",
       "      <td>-0.271844</td>\n",
       "      <td>0.038571</td>\n",
       "      <td>-0.280867</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1881</td>\n",
       "      <td>0.140476</td>\n",
       "      <td>-0.265884</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>-0.272933</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1882</td>\n",
       "      <td>0.130476</td>\n",
       "      <td>-0.259924</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>-0.264999</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1883</td>\n",
       "      <td>0.070476</td>\n",
       "      <td>-0.253964</td>\n",
       "      <td>0.038571</td>\n",
       "      <td>-0.257065</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1884</td>\n",
       "      <td>-0.019524</td>\n",
       "      <td>-0.248004</td>\n",
       "      <td>-0.061429</td>\n",
       "      <td>-0.249131</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2019</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.556595</td>\n",
       "      <td>1.198571</td>\n",
       "      <td>0.821968</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.800476</td>\n",
       "      <td>0.562555</td>\n",
       "      <td>1.228571</td>\n",
       "      <td>0.829902</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2021</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.568515</td>\n",
       "      <td>1.068571</td>\n",
       "      <td>0.837836</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2022</td>\n",
       "      <td>0.740476</td>\n",
       "      <td>0.574475</td>\n",
       "      <td>1.108571</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.980476</td>\n",
       "      <td>0.580435</td>\n",
       "      <td>1.388571</td>\n",
       "      <td>0.853705</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Ocean_Annual  ocean_trend  GMST_Annual  gmst_trend  paris_goal\n",
       "0    1880      0.080476    -0.271844     0.038571   -0.280867         1.5\n",
       "1    1881      0.140476    -0.265884     0.128571   -0.272933         1.5\n",
       "2    1882      0.130476    -0.259924     0.108571   -0.264999         1.5\n",
       "3    1883      0.070476    -0.253964     0.038571   -0.257065         1.5\n",
       "4    1884     -0.019524    -0.248004    -0.061429   -0.249131         1.5\n",
       "..    ...           ...          ...          ...         ...         ...\n",
       "139  2019      0.810476     0.556595     1.198571    0.821968         1.5\n",
       "140  2020      0.800476     0.562555     1.228571    0.829902         1.5\n",
       "141  2021      0.690476     0.568515     1.068571    0.837836         1.5\n",
       "142  2022      0.740476     0.574475     1.108571    0.845771         1.5\n",
       "143  2023      0.980476     0.580435     1.388571    0.853705         1.5\n",
       "\n",
       "[144 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocean_data = pd.read_csv(\"../Data/GISTEMP_SST.csv\") # Data downloaded from GISS Surface Temperature Analysis (v4)\n",
    "gmst_data = pd.read_csv(\"../Data/GMST_GISTEMP4.csv\") # Data downloaded from GISS Surface Temperature Analysis (v4)\n",
    "\n",
    "temp_data = ocean_data.merge(gmst_data,on='Year')\n",
    "\n",
    "# Calculate the mean of the 'No_Smoothing' column for the period 1880-1900\n",
    "base_period = temp_data[(temp_data['Year'] >= 1880) & (temp_data['Year'] <= 1900)]\n",
    "mean_sst_base_period = base_period['Ocean_Annual'].mean()\n",
    "mean_gmst_base_period = base_period['No_Smoothing'].mean()\n",
    "\n",
    "# Update the 'No_Smoothing' column to be anomalies relative to the period 1880-1900\n",
    "temp_data['Ocean_Annual'] = temp_data['Ocean_Annual'] - mean_sst_base_period\n",
    "temp_data['GMST_Annual'] = temp_data['No_Smoothing'] - mean_gmst_base_period\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(temp_data['Year'], temp_data['Ocean_Annual'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "temp_data['ocean_trend'] = intercept + slope * temp_data['Year']\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(temp_data['Year'], temp_data['GMST_Annual'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "temp_data['gmst_trend'] = intercept + slope * temp_data['Year']\n",
    "temp_data['paris_goal'] = 1.5\n",
    "\n",
    "temp_data[['Year','Ocean_Annual','ocean_trend','GMST_Annual','gmst_trend','paris_goal']].to_csv(\"../Data/mitigate_climate_change_1_temperature.csv\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "temp_data[['Year','Ocean_Annual','ocean_trend','GMST_Annual','gmst_trend','paris_goal']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bc4cf-7618-4bae-959e-589459d3a9c6",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "584f0d9f-2bcf-4bb7-a8e8-1aa9d14a0677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n"
     ]
    }
   ],
   "source": [
    "temp_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(temp_df['temperature'])\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/mitigate_climate_change_3_temperature.geojson\",driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5994a99-d325-4fee-b054-301a4f33f619",
   "metadata": {},
   "source": [
    "# Salinity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b6f60-d38f-4f84-aae0-a6438e930fcb",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d0a8fd-8e89-4a79-a44b-e8cdb6ff5001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pjs156/miniforge3/envs/claymodel/lib/python3.11/site-packages/xarray/core/groupby.py:668: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  index_grouper = pd.Grouper(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>salinity</th>\n",
       "      <th>linear_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1994-12-31</td>\n",
       "      <td>33.936993</td>\n",
       "      <td>33.933002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1995-12-31</td>\n",
       "      <td>33.936226</td>\n",
       "      <td>33.933941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-12-31</td>\n",
       "      <td>33.936901</td>\n",
       "      <td>33.934882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-12-31</td>\n",
       "      <td>33.936794</td>\n",
       "      <td>33.935820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998-12-31</td>\n",
       "      <td>33.934608</td>\n",
       "      <td>33.936759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time   salinity  linear_trend\n",
       "0 1994-12-31  33.936993     33.933002\n",
       "1 1995-12-31  33.936226     33.933941\n",
       "2 1996-12-31  33.936901     33.934882\n",
       "3 1997-12-31  33.936794     33.935820\n",
       "4 1998-12-31  33.934608     33.936759"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "salt_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "salt_df = salt_df['salinity'].mean(dim=['lat','lon']).resample(time='Y').mean()\n",
    "\n",
    "final_subset = salt_df.sel(time=slice('1994-01-01', None))\n",
    "\n",
    "# Create a pandas DataFrame with these columns\n",
    "df = pd.DataFrame({\n",
    "    'time': final_subset['time'].values,\n",
    "    'salinity': final_subset.values,\n",
    "})\n",
    "\n",
    "# Convert 'time' to datetime\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Convert datetime to a numerical value for linear regression (using ordinal format)\n",
    "df['time_ordinal'] = df['time'].map(pd.Timestamp.toordinal)\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(df['time_ordinal'], df['salinity'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "df['linear_trend'] = intercept + slope * df['time_ordinal']\n",
    "\n",
    "df[['time','salinity','linear_trend']].to_csv(\"../Data/mitigate_climate_change_1_salinity.csv\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df[['time','salinity','linear_trend']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d2e11-decc-4a10-a955-1c08807c7bcf",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f686b761-19b1-4790-a542-b368532e2e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_76839/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>SSP119_Mean</th>\n",
       "      <th>SSP119_CI_Lower</th>\n",
       "      <th>SSP119_CI_Upper</th>\n",
       "      <th>SSP245_Mean</th>\n",
       "      <th>SSP245_CI_Lower</th>\n",
       "      <th>SSP245_CI_Upper</th>\n",
       "      <th>SSP534_Mean</th>\n",
       "      <th>SSP534_CI_Lower</th>\n",
       "      <th>SSP534_CI_Upper</th>\n",
       "      <th>SSP585_Mean</th>\n",
       "      <th>SSP585_CI_Lower</th>\n",
       "      <th>SSP585_CI_Upper</th>\n",
       "      <th>NATURAL_Mean</th>\n",
       "      <th>NATURAL_CI_Lower</th>\n",
       "      <th>NATURAL_CI_Upper</th>\n",
       "      <th>Historical_Mean</th>\n",
       "      <th>Historical_CI_Lower</th>\n",
       "      <th>Historical_CI_Upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.598387</td>\n",
       "      <td>15.100</td>\n",
       "      <td>20.290</td>\n",
       "      <td>17.283548</td>\n",
       "      <td>14.730</td>\n",
       "      <td>20.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.514516</td>\n",
       "      <td>14.615</td>\n",
       "      <td>20.065</td>\n",
       "      <td>17.611290</td>\n",
       "      <td>14.710</td>\n",
       "      <td>21.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.754194</td>\n",
       "      <td>15.045</td>\n",
       "      <td>19.770</td>\n",
       "      <td>17.857742</td>\n",
       "      <td>14.585</td>\n",
       "      <td>20.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.815806</td>\n",
       "      <td>15.455</td>\n",
       "      <td>20.655</td>\n",
       "      <td>17.917419</td>\n",
       "      <td>14.910</td>\n",
       "      <td>20.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.055806</td>\n",
       "      <td>14.940</td>\n",
       "      <td>20.620</td>\n",
       "      <td>17.737097</td>\n",
       "      <td>14.715</td>\n",
       "      <td>20.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  SSP119_Mean  SSP119_CI_Lower  SSP119_CI_Upper  SSP245_Mean  \\\n",
       "0  1921          NaN              NaN              NaN          NaN   \n",
       "1  1922          NaN              NaN              NaN          NaN   \n",
       "2  1923          NaN              NaN              NaN          NaN   \n",
       "3  1924          NaN              NaN              NaN          NaN   \n",
       "4  1925          NaN              NaN              NaN          NaN   \n",
       "\n",
       "   SSP245_CI_Lower  SSP245_CI_Upper  SSP534_Mean  SSP534_CI_Lower  \\\n",
       "0              NaN              NaN          NaN              NaN   \n",
       "1              NaN              NaN          NaN              NaN   \n",
       "2              NaN              NaN          NaN              NaN   \n",
       "3              NaN              NaN          NaN              NaN   \n",
       "4              NaN              NaN          NaN              NaN   \n",
       "\n",
       "   SSP534_CI_Upper  SSP585_Mean  SSP585_CI_Lower  SSP585_CI_Upper  \\\n",
       "0              NaN          NaN              NaN              NaN   \n",
       "1              NaN          NaN              NaN              NaN   \n",
       "2              NaN          NaN              NaN              NaN   \n",
       "3              NaN          NaN              NaN              NaN   \n",
       "4              NaN          NaN              NaN              NaN   \n",
       "\n",
       "   NATURAL_Mean  NATURAL_CI_Lower  NATURAL_CI_Upper  Historical_Mean  \\\n",
       "0     17.598387            15.100            20.290        17.283548   \n",
       "1     17.514516            14.615            20.065        17.611290   \n",
       "2     17.754194            15.045            19.770        17.857742   \n",
       "3     17.815806            15.455            20.655        17.917419   \n",
       "4     18.055806            14.940            20.620        17.737097   \n",
       "\n",
       "   Historical_CI_Lower  Historical_CI_Upper  \n",
       "0               14.730                20.09  \n",
       "1               14.710                21.13  \n",
       "2               14.585                20.49  \n",
       "3               14.910                20.90  \n",
       "4               14.715                20.95  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load the file with more flexibility in handling its format to try and correctly parse it.\n",
    "with open('../Data/TAR_FIGURE_3_AMOC_45N', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Adjust the column naming logic to include the full scenario name as requested\n",
    "scenario_data = {}\n",
    "current_scenario = None\n",
    "time_range = range(1921, 2100)\n",
    "\n",
    "for line in lines:\n",
    "    # Check if line indicates a new scenario/ensemble using the SPEAR_c192_o1 pattern\n",
    "    scenario_match = re.search(r'SPEAR_c192_o1_(.+)_ENS_(\\d+)', line)\n",
    "    if scenario_match:\n",
    "        # Preserve the full scenario name (like HIST_SSP585_ALLForc) and ensemble number\n",
    "        scenario_name = scenario_match.group(1) + \"_\" + scenario_match.group(2)\n",
    "        current_scenario = scenario_name\n",
    "        scenario_data[current_scenario] = []\n",
    "    else:\n",
    "        # If the line contains numerical data, extract and add it to the current scenario\n",
    "        match = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line.strip())\n",
    "        if match and current_scenario:\n",
    "            scenario_data[current_scenario].extend([float(value) for value in match])\n",
    "\n",
    "# Creating a DataFrame where the first column is the time and each subsequent column is a scenario/ensemble\n",
    "df = pd.DataFrame({'Year': list(time_range)})\n",
    "\n",
    "for scenario, values in scenario_data.items():\n",
    "    df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
    "\n",
    "# Define scenario prefixes to filter the columns\n",
    "scenarios = ['SSP119', 'SSP245', 'SSP534', 'SSP585', 'NATURAL']\n",
    "\n",
    "# Initialize a result DataFrame\n",
    "result = pd.DataFrame()\n",
    "result['Year'] = df['Year']\n",
    "\n",
    "# Loop through each scenario to calculate the mean and confidence intervals\n",
    "for scenario in scenarios:\n",
    "    # Identify relevant columns for the current scenario\n",
    "    scenario_columns = [col for col in df.columns if scenario in col]\n",
    "    \n",
    "    if scenario_columns:  # Ensure there are columns for this scenario\n",
    "        # Calculate mean and confidence intervals\n",
    "        result[f'{scenario}_Mean'] = df[scenario_columns].mean(axis=1)\n",
    "        result[f'{scenario}_CI_Lower'] = df[scenario_columns].quantile(0.05, axis=1)\n",
    "        result[f'{scenario}_CI_Upper'] = df[scenario_columns].quantile(0.95, axis=1)\n",
    "\n",
    "# Create historical scenario columns by aggregating the SSP scenarios up to the year 2014\n",
    "historical_columns = ['SSP119', 'SSP245', 'SSP534', 'SSP585']\n",
    "\n",
    "# Create a mask for years up to 2014\n",
    "mask_historical = result['Year'] <= 2014\n",
    "\n",
    "# Calculate mean and confidence intervals for historical scenarios\n",
    "result['Historical_Mean'] = result.loc[mask_historical, [f'{scenario}_Mean' for scenario in historical_columns]].mean(axis=1)\n",
    "result['Historical_CI_Lower'] = result.loc[mask_historical, [f'{scenario}_CI_Lower' for scenario in historical_columns]].mean(axis=1)\n",
    "result['Historical_CI_Upper'] = result.loc[mask_historical, [f'{scenario}_CI_Upper' for scenario in historical_columns]].mean(axis=1)\n",
    "\n",
    "# Set SSP columns to NaN for years up to 2014\n",
    "for scenario in historical_columns:\n",
    "    result.loc[mask_historical, f'{scenario}_Mean'] = np.nan\n",
    "    result.loc[mask_historical, f'{scenario}_CI_Lower'] = np.nan\n",
    "    result.loc[mask_historical, f'{scenario}_CI_Upper'] = np.nan\n",
    "\n",
    "# Set Historical columns to NaN for years after 2014\n",
    "result.loc[~mask_historical, 'Historical_Mean'] = np.nan\n",
    "result.loc[~mask_historical, 'Historical_CI_Lower'] = np.nan\n",
    "result.loc[~mask_historical, 'Historical_CI_Upper'] = np.nan\n",
    "\n",
    "result.to_csv(\"mitigating_climate_change_3_salinity.csv\")\n",
    "\n",
    "result.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a4da73-28a4-4c3e-803b-3bbc28e595bd",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f8969-6a59-434a-9e72-36f8a229409f",
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(salt_df['salinity'])\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/mitigate_climate_change_4_salinity.geojson\",driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed662d8-0bad-4726-8938-3e9de16b615a",
   "metadata": {},
   "source": [
    "# Acidity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86eb980-e92b-4195-972e-c268f3457613",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa71227-ec8a-4fd0-b2bc-bc19db3831ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "acid_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "acid_df = acid_df['ph_total'].mean(dim=['lat','lon']).resample(time='Y').mean()\n",
    "\n",
    "# Create a pandas DataFrame with these columns\n",
    "df = pd.DataFrame({\n",
    "    'time': acid_df['time'].values,\n",
    "    'ph_total': acid_df.values,\n",
    "})\n",
    "\n",
    "# Convert 'time' to datetime\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Convert datetime to a numerical value for linear regression (using ordinal format)\n",
    "df['time_ordinal'] = df['time'].map(pd.Timestamp.toordinal)\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(df['time_ordinal'], df['ph_total'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "df['linear_trend'] = intercept + slope * df['time_ordinal']\n",
    "\n",
    "df[['time','ph_total','linear_trend']].to_csv(\"../Data/mitigate_climate_change_1_pH.csv\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df[['time','ph_total','linear_trend']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1aebed-15f3-4118-a455-bf0ecbcb882d",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e8cd5-eb40-4482-bcd3-6e29d3ed479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acid_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(acid_df['ph_total'])\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/mitigate_climate_change_3_pH.geojson\",driver=\"GeoJSON\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f8725-7e7b-4997-9380-8bd70981ccd2",
   "metadata": {},
   "source": [
    "# Sea Level Rise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7811bec-082d-4108-93f8-ac1f9e21b163",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf61201-1e3d-4b89-a0ba-e61660df1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load the data from the file, obtain unique hyperlink from https://sealevel.nasa.gov/\n",
    "url = 'https://deotb6e7tfubr.cloudfront.net/s3-edaf5da92e0ce48fb61175c28b67e95d/podaac-ops-cumulus-protected.s3.us-west-2.amazonaws.com/MERGED_TP_J1_OSTM_OST_GMSL_ASCII_V51/GMSL_TPJAOS_5.1.txt?A-userid=ps4813&Expires=1728253818&Signature=K3xewYQ~8a6Nl7xIoZ2wmsqgO4DR7~33lQLuhlH7uKH65PXRuAbddjFtW6riKHrOu8En20JKpku-56oBaaNG7asukhyorbhtOtDYKoFE5AiNe5gbaLN8bINf1RHym25W6vnBl76izBTI6FFy3CGCd74TpRRLwButl~M42cR1xANQ8SNa2A1xxTdBYgdnC5QkySZztz04VSgzqprotyDV88wq8MZ1PduSOALV-8PSgAUXX9Y74xvQNFMMcvuMpByfl7oXdRWmNIXRgRPUj5KnTch27wiuHwRWcAGry-vOwh9uShM6g0~drgwX2JFxntbqZobfTTkMcd2uFLWs3qq3pg__&Key-Pair-Id=K2ECMKQ3SIJ8HS'\n",
    "# Fetch the content\n",
    "response = requests.get(url)\n",
    "content = response.text\n",
    "\n",
    "# Split the content into lines\n",
    "lines = content.split('\\n')\n",
    "\n",
    "# Find the index of the line containing \"Header_End\"\n",
    "header_end_index = next(i for i, line in enumerate(lines) if \"Header_End\" in line)\n",
    "\n",
    "# Read the data, skipping the header rows\n",
    "raw_data = pd.read_csv(io.StringIO('\\n'.join(lines[header_end_index + 1:])), \n",
    "                       sep='\\s+', \n",
    "                       header=None)\n",
    "\n",
    "# Create a new DataFrame with 'date' and 'SLR' columns\n",
    "df = pd.DataFrame({\n",
    "    'date': raw_data[2],\n",
    "    'SLR': raw_data[5] - raw_data[5].iloc[0]  # Shifting SLR so that the first value is 0\n",
    "})\n",
    "\n",
    "# Function to convert fractional year to datetime (year, month, day only)\n",
    "def fractional_year_to_datetime(year):\n",
    "    year_int = int(year)  # Extract the integer part\n",
    "    remainder = year - year_int  # Get the fractional part\n",
    "    beginning_of_year = datetime(year_int, 1, 1)\n",
    "    days_in_year = (datetime(year_int + 1, 1, 1) - beginning_of_year).days\n",
    "    return (beginning_of_year + timedelta(days=remainder * days_in_year)).date()\n",
    "\n",
    "# Convert the fractional years in 'date' column to datetime (year-month-day)\n",
    "df['date'] = df['date'].apply(fractional_year_to_datetime)\n",
    "\n",
    "# Extract the year from the 'date' column and create a new 'year' column\n",
    "df['year'] = df['date'].apply(lambda x: x.year)\n",
    "\n",
    "# Group by the 'year' column and calculate the mean for the 'SLR' column\n",
    "df_grouped = df.groupby('year').mean(numeric_only=True)\n",
    "\n",
    "# Fit a linear trend\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df_grouped.index, df_grouped['SLR'])\n",
    "\n",
    "# Add the linear trend to the DataFrame\n",
    "df_grouped['linear_trend'] = slope * df_grouped.index + intercept\n",
    "\n",
    "# Save the grouped data to a CSV file\n",
    "df_grouped.to_csv(\"../Data/mitigate_climate_change_1_SLR.csv\")\n",
    "\n",
    "# Display the first few rows of the grouped DataFrame\n",
    "df_grouped.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09d1d7-394a-42bc-aed5-9f39dc73ee98",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e2f23-384c-4bcf-b6cd-909c0f8a1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copernicus Climate Change Service, Climate Data Store, (2018): Sea level daily gridded data from satellite observations for the global ocean from 1993 to present. Copernicus Climate Change Service (C3S) Climate Data Store (CDS)\n",
    "SLR_df = xr.open_mfdataset(\"../Data/dataset-satellite-sea-level-global-dc7f92ea-2d3e-4fc6-b767-836a5b8c0bff/*.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(SLR_df['sla'].load())\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/mitigate_climate_change_3_SLR.geojson\",driver=\"GeoJSON\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f50d8d-4c9c-42d3-b722-afd620b88942",
   "metadata": {},
   "source": [
    "# Sea Ice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf4dff-2e4d-4e3e-a6b5-76effa3fe881",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da60d964-0134-4e68-bcb4-46f0c994e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Base URLs for the NSIDC Sea Ice Index monthly data (North and South)\n",
    "base_urls = {\n",
    "    \"north\": \"https://noaadata.apps.nsidc.org/NOAA/G02135/north/monthly/data/\",\n",
    "    \"south\": \"https://noaadata.apps.nsidc.org/NOAA/G02135/south/monthly/data/\"\n",
    "}\n",
    "\n",
    "# List of file names for North and South\n",
    "file_names = {\n",
    "    \"north\": [f\"N_{month:02d}_extent_v3.0.csv\" for month in range(1, 13)],\n",
    "    \"south\": [f\"S_{month:02d}_extent_v3.0.csv\" for month in range(1, 13)]\n",
    "}\n",
    "\n",
    "# Function to download and load a single file\n",
    "def download_and_load(base_url, file_name):\n",
    "    url = base_url + file_name\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = StringIO(response.text)\n",
    "        df = pd.read_csv(data)\n",
    "        df['mo'] = int(file_name.split('_')[1])  # Extract month from filename\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed to download {file_name}\")\n",
    "        return None\n",
    "\n",
    "# Download and load all files for North and South\n",
    "dataframes = {}\n",
    "for region in base_urls:\n",
    "    dataframes[region] = [download_and_load(base_urls[region], file) for file in file_names[region]]\n",
    "\n",
    "# Remove any None values (failed downloads) and concatenate dataframes\n",
    "for region in dataframes:\n",
    "    dataframes[region] = [df for df in dataframes[region] if df is not None]\n",
    "    dataframes[region] = pd.concat(dataframes[region], ignore_index=True)\n",
    "    dataframes[region] = dataframes[region].sort_values(['year', 'mo']).reset_index(drop=True)\n",
    "\n",
    "# Add north and south data together for corresponding year-month pairs\n",
    "combined_data = pd.merge(\n",
    "    dataframes['north'], \n",
    "    dataframes['south'], \n",
    "    on=['year', 'mo'], \n",
    "    suffixes=('_north', '_south')\n",
    ")\n",
    "\n",
    "# Calculate total extent (this assumes 'extent' column exists in both north and south data)\n",
    "combined_data['total_extent'] = combined_data[' extent_north'] + combined_data[' extent_south']\n",
    "\n",
    "combined_data = combined_data.query(\"` extent_north` != -9999\")\n",
    "\n",
    "# Calculate the annual average for extent_north and extent_south\n",
    "annual_avg = combined_data.groupby('year').mean(numeric_only=True)[[' extent_north', ' extent_south']]\n",
    "\n",
    "# Calculate the linear trend for extent_north\n",
    "slope_north, intercept_north, r_value_north, p_value_north, std_err_north = stats.linregress(\n",
    "    annual_avg.index, annual_avg[' extent_north']\n",
    ")\n",
    "\n",
    "# Calculate the linear trend for extent_south\n",
    "slope_south, intercept_south, r_value_south, p_value_south, std_err_south = stats.linregress(\n",
    "    annual_avg.index, annual_avg[' extent_south']\n",
    ")\n",
    "\n",
    "# Add the linear trend values as new columns to the DataFrame\n",
    "annual_avg['linear_trend_north'] = slope_north * annual_avg.index + intercept_north\n",
    "annual_avg['linear_trend_south'] = slope_south * annual_avg.index + intercept_south\n",
    "\n",
    "annual_avg.to_csv(\"../Data/mitigate_climate_change_1_sea_ice.csv\")\n",
    "\n",
    "# Display the first few rows of the annual averages with trends\n",
    "annual_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1651556-af8b-4e01-92ae-dc4c53d12dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel",
   "language": "python",
   "name": "claymodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
