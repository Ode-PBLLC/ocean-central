{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20a3a59f-1d21-4f4a-b5ee-8bbac98634cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils functions and globals \n",
    "\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import box\n",
    "import rioxarray\n",
    "from rasterio.features import geometry_mask\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Open the biodiversity priority areas based on Zhao et al. 2020 (https://www.sciencedirect.com/science/article/abs/pii/S0006320719312182?via%3Dihub)\n",
    "masked_data = rioxarray.open_rasterio('masked_top_30_percent_over_water.tif')\n",
    "\n",
    "# Set the CRS for masked_data if it's not already set\n",
    "if 'crs' not in masked_data.attrs:\n",
    "    masked_data.rio.write_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "# Load SST dataset and EEZ shapefile\n",
    "seas_shapefile_path = '../Data/World_Seas_IHO_v3/World_Seas_IHO_v3.shp'\n",
    "SEAS_DF = gpd.read_file(seas_shapefile_path)\n",
    "\n",
    "# Calculate linear trend and p-value for each grid point\n",
    "def calculate_trend_and_significance(x):\n",
    "    if np.isnan(x).all():\n",
    "        return np.nan, np.nan, np.nan\n",
    "    else:\n",
    "        slope, intercept, _, p_value, _ = stats.linregress(range(len(x)), x)\n",
    "        return slope, intercept, p_value\n",
    "\n",
    "# Calculate the trend and significance of the trend at each pixel in an xarray dataset\n",
    "def calculate_trend_df(climate_df):\n",
    "    df_mean = climate_df.groupby('time.year').mean()\n",
    "    \n",
    "    # Apply the trend and p-value calculation to the entire dataset\n",
    "    results = xr.apply_ufunc(\n",
    "        calculate_trend_and_significance,\n",
    "        df_mean,\n",
    "        input_core_dims=[['year']],\n",
    "        vectorize=True,\n",
    "        output_core_dims=[[], [], []],\n",
    "        output_dtypes=[float, float, float]\n",
    "    )\n",
    "    \n",
    "    # Extract the trend and p-value into separate DataArrays\n",
    "    trends_da = xr.DataArray(results[0], coords=df_mean.isel(year=0).coords, name='trend')\n",
    "    pvalues_da = xr.DataArray(results[2], coords=df_mean.isel(year=0).coords, name='p_value')\n",
    "    \n",
    "    # Create a significance mask where p-value < 0.05\n",
    "    significant_da = xr.DataArray((pvalues_da < 0.05), coords=pvalues_da.coords, name='significant')\n",
    "    \n",
    "    # Combine trend, p-value, and significance mask into a single dataset\n",
    "    trend_significance_ds = xr.Dataset({\n",
    "        'trend': trends_da,\n",
    "        'p_value': pvalues_da,\n",
    "        'significant': significant_da\n",
    "    })\n",
    "    \n",
    "    # Set the CRS for the trends dataset to match the EEZ CRS\n",
    "    trend_significance_ds = trend_significance_ds.rio.write_crs(\"epsg:4326\")\n",
    "    return trend_significance_ds\n",
    "\n",
    "# Calculate area-weighted trend, significance for each sea/ocean area\n",
    "def area_trend(trend_significance_ds, SEAS_DF=SEAS_DF):\n",
    "    # Iterate over each sea/ocean area and calculate the area-weighted trend and significant area percentage\n",
    "    area_weighted_trends = []\n",
    "    \n",
    "    # Check if 'lat' and 'lon' are in the dataset, otherwise check for 'latitude' and 'longitude'\n",
    "    if 'lat' in trend_significance_ds.dims and 'lon' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'lat': 'y', 'lon': 'x'})\n",
    "    elif 'latitude' in trend_significance_ds.dims and 'longitude' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "\n",
    "\n",
    "    # Interpolate biodiversity priority areas to the same resolution as the climate data\n",
    "    masked_data_interp = masked_data.interp(\n",
    "        x=trend_significance_ds['x'],\n",
    "        y=trend_significance_ds['y'],\n",
    "        method='nearest'\n",
    "    )\n",
    "\n",
    "    # Calculate the area for each grid cell (assumes lat/lon grid)\n",
    "    lat = trend_significance_ds['y'].values\n",
    "    lon = trend_significance_ds['x'].values\n",
    "    \n",
    "    # Calculate grid cell area using Haversine formula or by approximation\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    \n",
    "    # Earth radius in kilometers\n",
    "    R = 6371\n",
    "    dlat = np.gradient(lat_rad)\n",
    "    dlon = np.gradient(lon_rad)\n",
    "    \n",
    "    # Approximate area calculation\n",
    "    cell_areas = (R**2 * np.outer(np.sin(dlat), dlon)) * np.cos(lat_rad[:, None])\n",
    "    \n",
    "    for i, row in SEAS_DF.iterrows():\n",
    "        try:\n",
    "            region_name = row['NAME']\n",
    "            area = row['area']\n",
    "            geom = row['geometry']\n",
    "    \n",
    "            # Mask SST trends with the sea geometry\n",
    "            masked_trends = trend_significance_ds['trend'].rio.clip([geom], drop=True)\n",
    "            masked_significance = trend_significance_ds['significant'].rio.clip([geom], drop=True)\n",
    "    \n",
    "            # Clip cell_areas to the same extent as masked_trends\n",
    "            cell_areas_clipped = xr.DataArray(\n",
    "                cell_areas, \n",
    "                dims=['y', 'x'], \n",
    "                coords={'y': trend_significance_ds['y'], 'x': trend_significance_ds['x']}\n",
    "            )\n",
    "            \n",
    "            # Set CRS for cell_areas_clipped to match the CRS of trend_significance_ds\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.write_crs('EPSG:4326')\n",
    "    \n",
    "            # Clip cell_areas to the same geometry\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.clip([geom], drop=True)\n",
    "        \n",
    "            # Compute the area-weighted trend\n",
    "            weighted_trend = (masked_trends * cell_areas_clipped).sum(dim=('y', 'x')) / cell_areas_clipped.sum()\n",
    "    \n",
    "            # Compute the total area that is significant\n",
    "            significant_masked_areas = (masked_significance * cell_areas_clipped).where(masked_significance, 0)\n",
    "            total_significant_area = significant_masked_areas.sum(dim=('y', 'x')).item()\n",
    "    \n",
    "            # Calculate the percentage of the area that is significant\n",
    "            total_area = cell_areas_clipped.sum()\n",
    "            significant_area_percent = (total_significant_area / total_area) * 100\n",
    "    \n",
    "            # Calculate the area for biodiversity based on the mask\n",
    "            area_biodiversity = ((masked_significance * cell_areas_clipped) * masked_data_interp).sum(dim=['x', 'y']).values\n",
    "    \n",
    "            # Store the result\n",
    "            area_weighted_trends.append({\n",
    "                'Region_Name': region_name,\n",
    "                'geometry': geom,\n",
    "                'Weighted_Trend': weighted_trend.item(),\n",
    "                'Sea_Area': total_area.values,\n",
    "                'Significant_Area': total_significant_area,\n",
    "                'Significant_Area_Percent': significant_area_percent.values,\n",
    "                'Biodiversity_Area': area_biodiversity[0]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Convert the results to a GeoDataFrame for easy viewing\n",
    "    area_weighted_trends_gdf = gpd.GeoDataFrame(area_weighted_trends, crs=SEAS_DF.crs)\n",
    "    return area_weighted_trends_gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9764e0c4-231e-4959-b242-a1329590e0c6",
   "metadata": {},
   "source": [
    "# Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a6cd5a-88f8-4934-acea-3a68346b7f41",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce6b288a-4493-44eb-a58c-5745788d191c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Ocean_Annual</th>\n",
       "      <th>ocean_trend</th>\n",
       "      <th>GMST_Annual</th>\n",
       "      <th>gmst_trend</th>\n",
       "      <th>paris_goal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1880</td>\n",
       "      <td>0.080476</td>\n",
       "      <td>-0.271844</td>\n",
       "      <td>0.038571</td>\n",
       "      <td>-0.280867</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1881</td>\n",
       "      <td>0.140476</td>\n",
       "      <td>-0.265884</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>-0.272933</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1882</td>\n",
       "      <td>0.130476</td>\n",
       "      <td>-0.259924</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>-0.264999</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1883</td>\n",
       "      <td>0.070476</td>\n",
       "      <td>-0.253964</td>\n",
       "      <td>0.038571</td>\n",
       "      <td>-0.257065</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1884</td>\n",
       "      <td>-0.019524</td>\n",
       "      <td>-0.248004</td>\n",
       "      <td>-0.061429</td>\n",
       "      <td>-0.249131</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2019</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.556595</td>\n",
       "      <td>1.198571</td>\n",
       "      <td>0.821968</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.800476</td>\n",
       "      <td>0.562555</td>\n",
       "      <td>1.228571</td>\n",
       "      <td>0.829902</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2021</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.568515</td>\n",
       "      <td>1.068571</td>\n",
       "      <td>0.837836</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2022</td>\n",
       "      <td>0.740476</td>\n",
       "      <td>0.574475</td>\n",
       "      <td>1.108571</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.980476</td>\n",
       "      <td>0.580435</td>\n",
       "      <td>1.388571</td>\n",
       "      <td>0.853705</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Ocean_Annual  ocean_trend  GMST_Annual  gmst_trend  paris_goal\n",
       "0    1880      0.080476    -0.271844     0.038571   -0.280867         1.5\n",
       "1    1881      0.140476    -0.265884     0.128571   -0.272933         1.5\n",
       "2    1882      0.130476    -0.259924     0.108571   -0.264999         1.5\n",
       "3    1883      0.070476    -0.253964     0.038571   -0.257065         1.5\n",
       "4    1884     -0.019524    -0.248004    -0.061429   -0.249131         1.5\n",
       "..    ...           ...          ...          ...         ...         ...\n",
       "139  2019      0.810476     0.556595     1.198571    0.821968         1.5\n",
       "140  2020      0.800476     0.562555     1.228571    0.829902         1.5\n",
       "141  2021      0.690476     0.568515     1.068571    0.837836         1.5\n",
       "142  2022      0.740476     0.574475     1.108571    0.845771         1.5\n",
       "143  2023      0.980476     0.580435     1.388571    0.853705         1.5\n",
       "\n",
       "[144 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocean_data = pd.read_csv(\"../Data/GISTEMP_SST.csv\") # Data downloaded from GISS Surface Temperature Analysis (v4)\n",
    "gmst_data = pd.read_csv(\"../Data/GMST_GISTEMP4.csv\") # Data downloaded from GISS Surface Temperature Analysis (v4)\n",
    "\n",
    "temp_data = ocean_data.merge(gmst_data,on='Year')\n",
    "\n",
    "# Calculate the mean of the 'No_Smoothing' column for the period 1880-1900\n",
    "base_period = temp_data[(temp_data['Year'] >= 1880) & (temp_data['Year'] <= 1900)]\n",
    "mean_sst_base_period = base_period['Ocean_Annual'].mean()\n",
    "mean_gmst_base_period = base_period['No_Smoothing'].mean()\n",
    "\n",
    "# Update the 'No_Smoothing' column to be anomalies relative to the period 1880-1900\n",
    "temp_data['Ocean_Annual'] = temp_data['Ocean_Annual'] - mean_sst_base_period\n",
    "temp_data['GMST_Annual'] = temp_data['No_Smoothing'] - mean_gmst_base_period\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(temp_data['Year'], temp_data['Ocean_Annual'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "temp_data['ocean_trend'] = intercept + slope * temp_data['Year']\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(temp_data['Year'], temp_data['GMST_Annual'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "temp_data['gmst_trend'] = intercept + slope * temp_data['Year']\n",
    "temp_data['paris_goal'] = 1.5\n",
    "\n",
    "temp_data[['Year','Ocean_Annual','ocean_trend','GMST_Annual','gmst_trend','paris_goal']].to_csv(\"../Data/mitigate_climate_change_1_temperature.csv\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "temp_data[['Year','Ocean_Annual','ocean_trend','GMST_Annual','gmst_trend','paris_goal']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bc4cf-7618-4bae-959e-589459d3a9c6",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "584f0d9f-2bcf-4bb7-a8e8-1aa9d14a0677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n"
     ]
    }
   ],
   "source": [
    "temp_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(temp_df['temperature'])\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_csv(\"../Data/mitigate_climate_change_3_temperature.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5994a99-d325-4fee-b054-301a4f33f619",
   "metadata": {},
   "source": [
    "# Salinity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b6f60-d38f-4f84-aae0-a6438e930fcb",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93d0a8fd-8e89-4a79-a44b-e8cdb6ff5001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pjs156/miniforge3/envs/claymodel/lib/python3.11/site-packages/xarray/core/groupby.py:668: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  index_grouper = pd.Grouper(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>salinity</th>\n",
       "      <th>linear_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1994-12-31</td>\n",
       "      <td>33.936993</td>\n",
       "      <td>33.933002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1995-12-31</td>\n",
       "      <td>33.936226</td>\n",
       "      <td>33.933941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-12-31</td>\n",
       "      <td>33.936901</td>\n",
       "      <td>33.934882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-12-31</td>\n",
       "      <td>33.936794</td>\n",
       "      <td>33.935820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998-12-31</td>\n",
       "      <td>33.934608</td>\n",
       "      <td>33.936759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time   salinity  linear_trend\n",
       "0 1994-12-31  33.936993     33.933002\n",
       "1 1995-12-31  33.936226     33.933941\n",
       "2 1996-12-31  33.936901     33.934882\n",
       "3 1997-12-31  33.936794     33.935820\n",
       "4 1998-12-31  33.934608     33.936759"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "salt_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "salt_df = salt_df['salinity'].mean(dim=['lat','lon']).resample(time='Y').mean()\n",
    "\n",
    "final_subset = salt_df.sel(time=slice('1994-01-01', None))\n",
    "\n",
    "# Create a pandas DataFrame with these columns\n",
    "df = pd.DataFrame({\n",
    "    'time': final_subset['time'].values,\n",
    "    'salinity': final_subset.values,\n",
    "})\n",
    "\n",
    "# Convert 'time' to datetime\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Convert datetime to a numerical value for linear regression (using ordinal format)\n",
    "df['time_ordinal'] = df['time'].map(pd.Timestamp.toordinal)\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(df['time_ordinal'], df['salinity'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "df['linear_trend'] = intercept + slope * df['time_ordinal']\n",
    "\n",
    "df[['time','salinity','linear_trend']].to_csv(\"../Data/mitigate_climate_change_1_salinity.csv\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df[['time','salinity','linear_trend']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a4da73-28a4-4c3e-803b-3bbc28e595bd",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1c0f8969-6a59-434a-9e72-36f8a229409f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n"
     ]
    }
   ],
   "source": [
    "salt_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(salt_df['salinity'])\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_csv(\"../Data/mitigate_climate_change_4_salinity.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed662d8-0bad-4726-8938-3e9de16b615a",
   "metadata": {},
   "source": [
    "# Acidity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86eb980-e92b-4195-972e-c268f3457613",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa71227-ec8a-4fd0-b2bc-bc19db3831ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pjs156/miniforge3/envs/claymodel/lib/python3.11/site-packages/xarray/core/groupby.py:668: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  index_grouper = pd.Grouper(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>ph_total</th>\n",
       "      <th>linear_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1982-12-31</td>\n",
       "      <td>8.131546</td>\n",
       "      <td>8.131885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1983-12-31</td>\n",
       "      <td>8.129746</td>\n",
       "      <td>8.130157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1984-12-31</td>\n",
       "      <td>8.126836</td>\n",
       "      <td>8.128425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1985-12-31</td>\n",
       "      <td>8.125478</td>\n",
       "      <td>8.126697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1986-12-31</td>\n",
       "      <td>8.124204</td>\n",
       "      <td>8.124968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1987-12-31</td>\n",
       "      <td>8.122771</td>\n",
       "      <td>8.123240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1988-12-31</td>\n",
       "      <td>8.119001</td>\n",
       "      <td>8.121508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1989-12-31</td>\n",
       "      <td>8.117866</td>\n",
       "      <td>8.119780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1990-12-31</td>\n",
       "      <td>8.116906</td>\n",
       "      <td>8.118052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1991-12-31</td>\n",
       "      <td>8.115874</td>\n",
       "      <td>8.116324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1992-12-31</td>\n",
       "      <td>8.115781</td>\n",
       "      <td>8.114591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1993-12-31</td>\n",
       "      <td>8.114245</td>\n",
       "      <td>8.112863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1994-12-31</td>\n",
       "      <td>8.112067</td>\n",
       "      <td>8.111135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1995-12-31</td>\n",
       "      <td>8.109838</td>\n",
       "      <td>8.109407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1996-12-31</td>\n",
       "      <td>8.108363</td>\n",
       "      <td>8.107674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1997-12-31</td>\n",
       "      <td>8.108090</td>\n",
       "      <td>8.105946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1998-12-31</td>\n",
       "      <td>8.104975</td>\n",
       "      <td>8.104218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>8.102454</td>\n",
       "      <td>8.102490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-12-31</td>\n",
       "      <td>8.101127</td>\n",
       "      <td>8.100757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2001-12-31</td>\n",
       "      <td>8.099681</td>\n",
       "      <td>8.099029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2002-12-31</td>\n",
       "      <td>8.098517</td>\n",
       "      <td>8.097301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2003-12-31</td>\n",
       "      <td>8.096164</td>\n",
       "      <td>8.095573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>8.094796</td>\n",
       "      <td>8.093840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>8.092931</td>\n",
       "      <td>8.092112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2006-12-31</td>\n",
       "      <td>8.091172</td>\n",
       "      <td>8.090384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2007-12-31</td>\n",
       "      <td>8.089204</td>\n",
       "      <td>8.088656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>8.087272</td>\n",
       "      <td>8.086923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>8.086955</td>\n",
       "      <td>8.085195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>8.084702</td>\n",
       "      <td>8.083467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>8.082982</td>\n",
       "      <td>8.081739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>8.081233</td>\n",
       "      <td>8.080007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>8.078918</td>\n",
       "      <td>8.078279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>8.077369</td>\n",
       "      <td>8.076551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>8.075502</td>\n",
       "      <td>8.074823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>8.072486</td>\n",
       "      <td>8.073090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>8.070603</td>\n",
       "      <td>8.071362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>8.068720</td>\n",
       "      <td>8.069634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>8.066983</td>\n",
       "      <td>8.067906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>8.064296</td>\n",
       "      <td>8.066173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>8.062463</td>\n",
       "      <td>8.064445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>8.059190</td>\n",
       "      <td>8.062717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  ph_total  linear_trend\n",
       "0  1982-12-31  8.131546      8.131885\n",
       "1  1983-12-31  8.129746      8.130157\n",
       "2  1984-12-31  8.126836      8.128425\n",
       "3  1985-12-31  8.125478      8.126697\n",
       "4  1986-12-31  8.124204      8.124968\n",
       "5  1987-12-31  8.122771      8.123240\n",
       "6  1988-12-31  8.119001      8.121508\n",
       "7  1989-12-31  8.117866      8.119780\n",
       "8  1990-12-31  8.116906      8.118052\n",
       "9  1991-12-31  8.115874      8.116324\n",
       "10 1992-12-31  8.115781      8.114591\n",
       "11 1993-12-31  8.114245      8.112863\n",
       "12 1994-12-31  8.112067      8.111135\n",
       "13 1995-12-31  8.109838      8.109407\n",
       "14 1996-12-31  8.108363      8.107674\n",
       "15 1997-12-31  8.108090      8.105946\n",
       "16 1998-12-31  8.104975      8.104218\n",
       "17 1999-12-31  8.102454      8.102490\n",
       "18 2000-12-31  8.101127      8.100757\n",
       "19 2001-12-31  8.099681      8.099029\n",
       "20 2002-12-31  8.098517      8.097301\n",
       "21 2003-12-31  8.096164      8.095573\n",
       "22 2004-12-31  8.094796      8.093840\n",
       "23 2005-12-31  8.092931      8.092112\n",
       "24 2006-12-31  8.091172      8.090384\n",
       "25 2007-12-31  8.089204      8.088656\n",
       "26 2008-12-31  8.087272      8.086923\n",
       "27 2009-12-31  8.086955      8.085195\n",
       "28 2010-12-31  8.084702      8.083467\n",
       "29 2011-12-31  8.082982      8.081739\n",
       "30 2012-12-31  8.081233      8.080007\n",
       "31 2013-12-31  8.078918      8.078279\n",
       "32 2014-12-31  8.077369      8.076551\n",
       "33 2015-12-31  8.075502      8.074823\n",
       "34 2016-12-31  8.072486      8.073090\n",
       "35 2017-12-31  8.070603      8.071362\n",
       "36 2018-12-31  8.068720      8.069634\n",
       "37 2019-12-31  8.066983      8.067906\n",
       "38 2020-12-31  8.064296      8.066173\n",
       "39 2021-12-31  8.062463      8.064445\n",
       "40 2022-12-31  8.059190      8.062717"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "acid_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "acid_df = acid_df['ph_total'].mean(dim=['lat','lon']).resample(time='Y').mean()\n",
    "\n",
    "# Create a pandas DataFrame with these columns\n",
    "df = pd.DataFrame({\n",
    "    'time': acid_df['time'].values,\n",
    "    'ph_total': acid_df.values,\n",
    "})\n",
    "\n",
    "# Convert 'time' to datetime\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Convert datetime to a numerical value for linear regression (using ordinal format)\n",
    "df['time_ordinal'] = df['time'].map(pd.Timestamp.toordinal)\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(df['time_ordinal'], df['ph_total'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "df['linear_trend'] = intercept + slope * df['time_ordinal']\n",
    "\n",
    "df[['time','ph_total','linear_trend']].to_csv(\"../Data/mitigate_climate_change_1_pH.csv\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df[['time','ph_total','linear_trend']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1aebed-15f3-4118-a455-bf0ecbcb882d",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d05e8cd5-eb40-4482-bcd3-6e29d3ed479d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n"
     ]
    }
   ],
   "source": [
    "acid_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(acid_df['ph_total'])\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_csv(\"../Data/mitigate_climate_change_3_pH.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f8725-7e7b-4997-9380-8bd70981ccd2",
   "metadata": {},
   "source": [
    "# Sea Level Rise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7811bec-082d-4108-93f8-ac1f9e21b163",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bf61201-1e3d-4b89-a0ba-e61660df1db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SLR</th>\n",
       "      <th>linear_trend</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>4.838919</td>\n",
       "      <td>1.617056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>9.133514</td>\n",
       "      <td>4.672681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>12.806757</td>\n",
       "      <td>7.728305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>13.429722</td>\n",
       "      <td>10.783929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>17.778108</td>\n",
       "      <td>13.839553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            SLR  linear_trend\n",
       "year                         \n",
       "1993   4.838919      1.617056\n",
       "1994   9.133514      4.672681\n",
       "1995  12.806757      7.728305\n",
       "1996  13.429722     10.783929\n",
       "1997  17.778108     13.839553"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load the data from the file, obtain unique hyperlink from https://sealevel.nasa.gov/\n",
    "url = 'https://deotb6e7tfubr.cloudfront.net/s3-edaf5da92e0ce48fb61175c28b67e95d/podaac-ops-cumulus-protected.s3.us-west-2.amazonaws.com/MERGED_TP_J1_OSTM_OST_GMSL_ASCII_V51/GMSL_TPJAOS_5.1.txt?A-userid=ps4813&Expires=1728253818&Signature=K3xewYQ~8a6Nl7xIoZ2wmsqgO4DR7~33lQLuhlH7uKH65PXRuAbddjFtW6riKHrOu8En20JKpku-56oBaaNG7asukhyorbhtOtDYKoFE5AiNe5gbaLN8bINf1RHym25W6vnBl76izBTI6FFy3CGCd74TpRRLwButl~M42cR1xANQ8SNa2A1xxTdBYgdnC5QkySZztz04VSgzqprotyDV88wq8MZ1PduSOALV-8PSgAUXX9Y74xvQNFMMcvuMpByfl7oXdRWmNIXRgRPUj5KnTch27wiuHwRWcAGry-vOwh9uShM6g0~drgwX2JFxntbqZobfTTkMcd2uFLWs3qq3pg__&Key-Pair-Id=K2ECMKQ3SIJ8HS'\n",
    "# Fetch the content\n",
    "response = requests.get(url)\n",
    "content = response.text\n",
    "\n",
    "# Split the content into lines\n",
    "lines = content.split('\\n')\n",
    "\n",
    "# Find the index of the line containing \"Header_End\"\n",
    "header_end_index = next(i for i, line in enumerate(lines) if \"Header_End\" in line)\n",
    "\n",
    "# Read the data, skipping the header rows\n",
    "raw_data = pd.read_csv(io.StringIO('\\n'.join(lines[header_end_index + 1:])), \n",
    "                       sep='\\s+', \n",
    "                       header=None)\n",
    "\n",
    "# Create a new DataFrame with 'date' and 'SLR' columns\n",
    "df = pd.DataFrame({\n",
    "    'date': raw_data[2],\n",
    "    'SLR': raw_data[5] - raw_data[5].iloc[0]  # Shifting SLR so that the first value is 0\n",
    "})\n",
    "\n",
    "# Function to convert fractional year to datetime (year, month, day only)\n",
    "def fractional_year_to_datetime(year):\n",
    "    year_int = int(year)  # Extract the integer part\n",
    "    remainder = year - year_int  # Get the fractional part\n",
    "    beginning_of_year = datetime(year_int, 1, 1)\n",
    "    days_in_year = (datetime(year_int + 1, 1, 1) - beginning_of_year).days\n",
    "    return (beginning_of_year + timedelta(days=remainder * days_in_year)).date()\n",
    "\n",
    "# Convert the fractional years in 'date' column to datetime (year-month-day)\n",
    "df['date'] = df['date'].apply(fractional_year_to_datetime)\n",
    "\n",
    "# Extract the year from the 'date' column and create a new 'year' column\n",
    "df['year'] = df['date'].apply(lambda x: x.year)\n",
    "\n",
    "# Group by the 'year' column and calculate the mean for the 'SLR' column\n",
    "df_grouped = df.groupby('year').mean(numeric_only=True)\n",
    "\n",
    "# Fit a linear trend\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df_grouped.index, df_grouped['SLR'])\n",
    "\n",
    "# Add the linear trend to the DataFrame\n",
    "df_grouped['linear_trend'] = slope * df_grouped.index + intercept\n",
    "\n",
    "# Save the grouped data to a CSV file\n",
    "df_grouped.to_csv(\"../Data/mitigate_climate_change_1_SLR.csv\")\n",
    "\n",
    "# Display the first few rows of the grouped DataFrame\n",
    "df_grouped.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09d1d7-394a-42bc-aed5-9f39dc73ee98",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d7e2f23-384c-4bcf-b6cd-909c0f8a1a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n"
     ]
    }
   ],
   "source": [
    "# Copernicus Climate Change Service, Climate Data Store, (2018): Sea level daily gridded data from satellite observations for the global ocean from 1993 to present. Copernicus Climate Change Service (C3S) Climate Data Store (CDS)\n",
    "SLR_df = xr.open_mfdataset(\"../Data/dataset-satellite-sea-level-global-dc7f92ea-2d3e-4fc6-b767-836a5b8c0bff/*.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(SLR_df['sla'].load())\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_csv(\"../Data/mitigate_climate_change_3_SLR.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f50d8d-4c9c-42d3-b722-afd620b88942",
   "metadata": {},
   "source": [
    "# Sea Ice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf4dff-2e4d-4e3e-a6b5-76effa3fe881",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da60d964-0134-4e68-bcb4-46f0c994e77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extent_north</th>\n",
       "      <th>extent_south</th>\n",
       "      <th>linear_trend_north</th>\n",
       "      <th>linear_trend_south</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>12.660000</td>\n",
       "      <td>13.150000</td>\n",
       "      <td>12.509701</td>\n",
       "      <td>11.827650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>12.350000</td>\n",
       "      <td>11.655833</td>\n",
       "      <td>12.459839</td>\n",
       "      <td>11.814673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>12.348333</td>\n",
       "      <td>11.205833</td>\n",
       "      <td>12.409976</td>\n",
       "      <td>11.801696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>12.146667</td>\n",
       "      <td>11.386667</td>\n",
       "      <td>12.360113</td>\n",
       "      <td>11.788719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>12.467500</td>\n",
       "      <td>11.595000</td>\n",
       "      <td>12.310251</td>\n",
       "      <td>11.775742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       extent_north   extent_south  linear_trend_north  linear_trend_south\n",
       "year                                                                      \n",
       "1978      12.660000      13.150000           12.509701           11.827650\n",
       "1979      12.350000      11.655833           12.459839           11.814673\n",
       "1980      12.348333      11.205833           12.409976           11.801696\n",
       "1981      12.146667      11.386667           12.360113           11.788719\n",
       "1982      12.467500      11.595000           12.310251           11.775742"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Base URLs for the NSIDC Sea Ice Index monthly data (North and South)\n",
    "base_urls = {\n",
    "    \"north\": \"https://noaadata.apps.nsidc.org/NOAA/G02135/north/monthly/data/\",\n",
    "    \"south\": \"https://noaadata.apps.nsidc.org/NOAA/G02135/south/monthly/data/\"\n",
    "}\n",
    "\n",
    "# List of file names for North and South\n",
    "file_names = {\n",
    "    \"north\": [f\"N_{month:02d}_extent_v3.0.csv\" for month in range(1, 13)],\n",
    "    \"south\": [f\"S_{month:02d}_extent_v3.0.csv\" for month in range(1, 13)]\n",
    "}\n",
    "\n",
    "# Function to download and load a single file\n",
    "def download_and_load(base_url, file_name):\n",
    "    url = base_url + file_name\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = StringIO(response.text)\n",
    "        df = pd.read_csv(data)\n",
    "        df['mo'] = int(file_name.split('_')[1])  # Extract month from filename\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed to download {file_name}\")\n",
    "        return None\n",
    "\n",
    "# Download and load all files for North and South\n",
    "dataframes = {}\n",
    "for region in base_urls:\n",
    "    dataframes[region] = [download_and_load(base_urls[region], file) for file in file_names[region]]\n",
    "\n",
    "# Remove any None values (failed downloads) and concatenate dataframes\n",
    "for region in dataframes:\n",
    "    dataframes[region] = [df for df in dataframes[region] if df is not None]\n",
    "    dataframes[region] = pd.concat(dataframes[region], ignore_index=True)\n",
    "    dataframes[region] = dataframes[region].sort_values(['year', 'mo']).reset_index(drop=True)\n",
    "\n",
    "# Add north and south data together for corresponding year-month pairs\n",
    "combined_data = pd.merge(\n",
    "    dataframes['north'], \n",
    "    dataframes['south'], \n",
    "    on=['year', 'mo'], \n",
    "    suffixes=('_north', '_south')\n",
    ")\n",
    "\n",
    "# Calculate total extent (this assumes 'extent' column exists in both north and south data)\n",
    "combined_data['total_extent'] = combined_data[' extent_north'] + combined_data[' extent_south']\n",
    "\n",
    "combined_data = combined_data.query(\"` extent_north` != -9999\")\n",
    "\n",
    "# Calculate the annual average for extent_north and extent_south\n",
    "annual_avg = combined_data.groupby('year').mean(numeric_only=True)[[' extent_north', ' extent_south']]\n",
    "\n",
    "# Calculate the linear trend for extent_north\n",
    "slope_north, intercept_north, r_value_north, p_value_north, std_err_north = stats.linregress(\n",
    "    annual_avg.index, annual_avg[' extent_north']\n",
    ")\n",
    "\n",
    "# Calculate the linear trend for extent_south\n",
    "slope_south, intercept_south, r_value_south, p_value_south, std_err_south = stats.linregress(\n",
    "    annual_avg.index, annual_avg[' extent_south']\n",
    ")\n",
    "\n",
    "# Add the linear trend values as new columns to the DataFrame\n",
    "annual_avg['linear_trend_north'] = slope_north * annual_avg.index + intercept_north\n",
    "annual_avg['linear_trend_south'] = slope_south * annual_avg.index + intercept_south\n",
    "\n",
    "annual_avg.to_csv(\"../Data/mitigate_climate_change_1_sea_ice.csv\")\n",
    "\n",
    "# Display the first few rows of the annual averages with trends\n",
    "annual_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1651556-af8b-4e01-92ae-dc4c53d12dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel",
   "language": "python",
   "name": "claymodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
