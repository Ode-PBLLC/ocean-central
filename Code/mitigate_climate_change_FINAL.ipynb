{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "20a3a59f-1d21-4f4a-b5ee-8bbac98634cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils functions and globals \n",
    "\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import cartopy.crs as ccrs\n",
    "from shapely.geometry import box\n",
    "import rioxarray\n",
    "import re\n",
    "\n",
    "from rasterio.features import geometry_mask\n",
    "from scipy.stats import linregress\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Open the biodiversity priority areas based on Zhao et al. 2020 (https://www.sciencedirect.com/science/article/abs/pii/S0006320719312182?via%3Dihub)\n",
    "masked_data = rioxarray.open_rasterio('masked_top_30_percent_over_water.tif')\n",
    "\n",
    "# Set the CRS for masked_data if it's not already set\n",
    "if 'crs' not in masked_data.attrs:\n",
    "    masked_data.rio.write_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "# Load SST dataset and EEZ shapefile\n",
    "seas_shapefile_path = '../Data/World_Seas_IHO_v3/World_Seas_IHO_v3.shp'\n",
    "SEAS_DF = gpd.read_file(seas_shapefile_path)\n",
    "\n",
    "# Calculate linear trend and p-value for each grid point\n",
    "def calculate_trend_and_significance(x):\n",
    "    if np.isnan(x).all():\n",
    "        return np.nan, np.nan, np.nan\n",
    "    else:\n",
    "        slope, intercept, _, p_value, _ = stats.linregress(range(len(x)), x)\n",
    "        return slope, intercept, p_value\n",
    "\n",
    "# Calculate the trend and significance of the trend at each pixel in an xarray dataset\n",
    "def calculate_trend_df(climate_df):\n",
    "    df_mean = climate_df.groupby('time.year').mean()\n",
    "    \n",
    "    # Apply the trend and p-value calculation to the entire dataset\n",
    "    results = xr.apply_ufunc(\n",
    "        calculate_trend_and_significance,\n",
    "        df_mean,\n",
    "        input_core_dims=[['year']],\n",
    "        vectorize=True,\n",
    "        output_core_dims=[[], [], []],\n",
    "        output_dtypes=[float, float, float]\n",
    "    )\n",
    "    \n",
    "    # Extract the trend and p-value into separate DataArrays\n",
    "    trends_da = xr.DataArray(results[0], coords=df_mean.isel(year=0).coords, name='trend')\n",
    "    pvalues_da = xr.DataArray(results[2], coords=df_mean.isel(year=0).coords, name='p_value')\n",
    "    \n",
    "    # Create a significance mask where p-value < 0.05\n",
    "    significant_da = xr.DataArray((pvalues_da < 0.05), coords=pvalues_da.coords, name='significant')\n",
    "    \n",
    "    # Combine trend, p-value, and significance mask into a single dataset\n",
    "    trend_significance_ds = xr.Dataset({\n",
    "        'trend': trends_da,\n",
    "        'p_value': pvalues_da,\n",
    "        'significant': significant_da\n",
    "    })\n",
    "    \n",
    "    # Set the CRS for the trends dataset to match the EEZ CRS\n",
    "    trend_significance_ds = trend_significance_ds.rio.write_crs(\"epsg:4326\")\n",
    "    return trend_significance_ds\n",
    "\n",
    "# Calculate area-weighted trend, significance for each sea/ocean area\n",
    "def area_trend(trend_significance_ds, SEAS_DF=SEAS_DF):\n",
    "    # Iterate over each sea/ocean area and calculate the area-weighted trend and significant area percentage\n",
    "    area_weighted_trends = []\n",
    "    \n",
    "    # Check if 'lat' and 'lon' are in the dataset, otherwise check for 'latitude' and 'longitude'\n",
    "    if 'lat' in trend_significance_ds.dims and 'lon' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'lat': 'y', 'lon': 'x'})\n",
    "    elif 'latitude' in trend_significance_ds.dims and 'longitude' in trend_significance_ds.dims:\n",
    "        trend_significance_ds = trend_significance_ds.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "\n",
    "    # Interpolate biodiversity priority areas to the same resolution as the climate data\n",
    "    masked_data_interp = masked_data.interp(\n",
    "        x=trend_significance_ds['x'],\n",
    "        y=trend_significance_ds['y'],\n",
    "        method='nearest'\n",
    "    )\n",
    "\n",
    "    # Calculate the area for each grid cell (assumes lat/lon grid)\n",
    "    lat = trend_significance_ds['y'].values\n",
    "    lon = trend_significance_ds['x'].values\n",
    "    \n",
    "    # Calculate grid cell area using Haversine formula or by approximation\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    \n",
    "    # Earth radius in kilometers\n",
    "    R = 6371\n",
    "    dlat = np.gradient(lat_rad)\n",
    "    dlon = np.gradient(lon_rad)\n",
    "    \n",
    "    # Approximate area calculation\n",
    "    cell_areas = (R**2 * np.outer(np.sin(dlat), dlon)) * np.cos(lat_rad[:, None])\n",
    "    \n",
    "    for i, row in SEAS_DF.iterrows():\n",
    "        try:\n",
    "            region_name = row['NAME']\n",
    "            area = row['area']\n",
    "            geom = row['geometry']\n",
    "    \n",
    "            # Mask SST trends with the sea geometry\n",
    "            masked_trends = trend_significance_ds['trend'].rio.clip([geom], drop=True)\n",
    "            masked_significance = trend_significance_ds['significant'].rio.clip([geom], drop=True)\n",
    "    \n",
    "            # Clip cell_areas to the same extent as masked_trends\n",
    "            cell_areas_clipped = xr.DataArray(\n",
    "                cell_areas, \n",
    "                dims=['y', 'x'], \n",
    "                coords={'y': trend_significance_ds['y'], 'x': trend_significance_ds['x']}\n",
    "            )\n",
    "            \n",
    "            # Set CRS for cell_areas_clipped to match the CRS of trend_significance_ds\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.write_crs('EPSG:4326')\n",
    "    \n",
    "            # Clip cell_areas to the same geometry\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.clip([geom], drop=True)\n",
    "        \n",
    "            # Compute the area-weighted trend\n",
    "            weighted_trend = (masked_trends * cell_areas_clipped).sum(dim=('y', 'x')) / cell_areas_clipped.sum()\n",
    "    \n",
    "            # Compute the total area that is significant\n",
    "            significant_masked_areas = (masked_significance * cell_areas_clipped).where(masked_significance, 0)\n",
    "            total_significant_area = significant_masked_areas.sum(dim=('y', 'x')).item()\n",
    "    \n",
    "            # Calculate the percentage of the area that is significant\n",
    "            total_area = cell_areas_clipped.sum()\n",
    "            significant_area_percent = (total_significant_area / total_area) * 100\n",
    "    \n",
    "            # Calculate the area for biodiversity based on the mask\n",
    "            area_biodiversity = ((masked_significance * cell_areas_clipped) * masked_data_interp).sum(dim=['x', 'y']).values\n",
    "    \n",
    "            # Store the result\n",
    "            area_weighted_trends.append({\n",
    "                'Region_Name': region_name,\n",
    "                'geometry': geom,\n",
    "                'Weighted_Trend': weighted_trend.item(),\n",
    "                'Sea_Area': total_area.item(),\n",
    "                'Significant_Area': total_significant_area,\n",
    "                'Significant_Area_Percent': significant_area_percent.item(),\n",
    "                'Biodiversity_Area': area_biodiversity[0]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Convert the results to a GeoDataFrame for easy viewing\n",
    "    area_weighted_trends_gdf = gpd.GeoDataFrame(area_weighted_trends, crs=SEAS_DF.crs)\n",
    "    return area_weighted_trends_gdf\n",
    "\n",
    "def area_heatwave(temp_df, SEAS_DF=SEAS_DF):\n",
    "    area_heatwave = []\n",
    "\n",
    "    # Set CRS and rename dimensions and coordinates\n",
    "    temp_df = temp_df.rio.write_crs(\"epsg:4326\")\n",
    "    temp_df = temp_df.rename({'latdim': 'y', 'londim': 'x'}).rename({'lat': 'y', 'lon': 'x'})  # Adjust based on your dimensions\n",
    "\n",
    "    # Select heatwave categories >= 3 and aggregate over time\n",
    "    temp_df = (temp_df['heatwave_category'] >= 3).any(dim='time')\n",
    "\n",
    "    # Interpolate biodiversity priority areas to the same resolution as the climate data\n",
    "    masked_data_interp = masked_data.interp(\n",
    "        x=temp_df['x'],\n",
    "        y=temp_df['y'],\n",
    "        method='nearest'\n",
    "    )\n",
    "\n",
    "    # Calculate the area for each grid cell (assumes lat/lon grid)\n",
    "    lat = temp_df['y'].values\n",
    "    lon = temp_df['x'].values\n",
    "    \n",
    "    # Calculate grid cell area using Haversine formula or by approximation\n",
    "    lat_rad = np.deg2rad(lat)\n",
    "    lon_rad = np.deg2rad(lon)\n",
    "    \n",
    "    # Earth radius in kilometers\n",
    "    R = 6371\n",
    "    dlat = np.gradient(lat_rad)\n",
    "    dlon = np.gradient(lon_rad)\n",
    "    \n",
    "    # Approximate area calculation\n",
    "    cell_areas = (R**2 * np.outer(np.sin(dlat), dlon)) * np.cos(lat_rad[:, None])\n",
    "    \n",
    "    # Use tqdm to track progress through SEAS_DF.iterrows()\n",
    "    for i, row in tqdm(SEAS_DF.iterrows(), total=len(SEAS_DF), desc=\"Processing Sea Areas\"):\n",
    "        try:\n",
    "            region_name = row['NAME']\n",
    "            geom = row['geometry']\n",
    "    \n",
    "            # Mask SST trends with the sea geometry\n",
    "            masked_df = temp_df.rio.clip([geom], drop=True)\n",
    "    \n",
    "            # Clip cell_areas to the same extent as masked_df\n",
    "            cell_areas_clipped = xr.DataArray(\n",
    "                cell_areas, \n",
    "                dims=['y', 'x'], \n",
    "                coords={'y': temp_df['y'], 'x': temp_df['x']}\n",
    "            )\n",
    "            \n",
    "            # Set CRS for cell_areas_clipped to match the CRS of trend_significance_ds\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.write_crs('EPSG:4326')\n",
    "    \n",
    "            # Clip cell_areas to the same geometry\n",
    "            cell_areas_clipped = cell_areas_clipped.rio.clip([geom], drop=True)\n",
    "        \n",
    "            # Compute the total area that is impacted by a severe heatwave\n",
    "            heatwave_area = (masked_df * cell_areas_clipped).sum(dim=('y', 'x')).compute()  # Compute to convert from Dask array\n",
    "    \n",
    "            # Calculate the area for biodiversity based on the mask\n",
    "            area_biodiversity = ((masked_df * cell_areas_clipped) * masked_data_interp).sum(dim=['x', 'y']).compute()\n",
    "\n",
    "            total_area = cell_areas_clipped.sum(dim=('y', 'x')).compute()  # Ensure computation\n",
    "    \n",
    "            # Extract values after computing\n",
    "            heatwave_value = heatwave_area.item() if heatwave_area.size == 1 else heatwave_area.values[0]\n",
    "            total_area_value = total_area.item() if total_area.size == 1 else total_area.values[0]\n",
    "            area_biodiversity = area_biodiversity.item() if area_biodiversity.size == 1 else area_biodiversity.values[0]\n",
    "    \n",
    "            # Store the result\n",
    "            area_heatwave.append({\n",
    "                'Region_Name': region_name,\n",
    "                'geometry': geom,\n",
    "                'Heatwave_Area': heatwave_value,\n",
    "                'Sea_Area': total_area_value,\n",
    "                'Biodiversity_Area': area_biodiversity\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {region_name}: {e}\")\n",
    "\n",
    "    # Convert the results to a GeoDataFrame for easy viewing\n",
    "    area_heatwave_gdf = gpd.GeoDataFrame(area_heatwave, crs=SEAS_DF.crs)\n",
    "    return area_heatwave_gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9764e0c4-231e-4959-b242-a1329590e0c6",
   "metadata": {},
   "source": [
    "# Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a6cd5a-88f8-4934-acea-3a68346b7f41",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce6b288a-4493-44eb-a58c-5745788d191c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Ocean_Annual</th>\n",
       "      <th>ocean_trend</th>\n",
       "      <th>GMST_Annual</th>\n",
       "      <th>gmst_trend</th>\n",
       "      <th>paris_goal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1880</td>\n",
       "      <td>0.080476</td>\n",
       "      <td>-0.271844</td>\n",
       "      <td>0.038571</td>\n",
       "      <td>-0.280867</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1881</td>\n",
       "      <td>0.140476</td>\n",
       "      <td>-0.265884</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>-0.272933</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1882</td>\n",
       "      <td>0.130476</td>\n",
       "      <td>-0.259924</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>-0.264999</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1883</td>\n",
       "      <td>0.070476</td>\n",
       "      <td>-0.253964</td>\n",
       "      <td>0.038571</td>\n",
       "      <td>-0.257065</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1884</td>\n",
       "      <td>-0.019524</td>\n",
       "      <td>-0.248004</td>\n",
       "      <td>-0.061429</td>\n",
       "      <td>-0.249131</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2019</td>\n",
       "      <td>0.810476</td>\n",
       "      <td>0.556595</td>\n",
       "      <td>1.198571</td>\n",
       "      <td>0.821968</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2020</td>\n",
       "      <td>0.800476</td>\n",
       "      <td>0.562555</td>\n",
       "      <td>1.228571</td>\n",
       "      <td>0.829902</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2021</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.568515</td>\n",
       "      <td>1.068571</td>\n",
       "      <td>0.837836</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2022</td>\n",
       "      <td>0.740476</td>\n",
       "      <td>0.574475</td>\n",
       "      <td>1.108571</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2023</td>\n",
       "      <td>0.980476</td>\n",
       "      <td>0.580435</td>\n",
       "      <td>1.388571</td>\n",
       "      <td>0.853705</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Ocean_Annual  ocean_trend  GMST_Annual  gmst_trend  paris_goal\n",
       "0    1880      0.080476    -0.271844     0.038571   -0.280867         1.5\n",
       "1    1881      0.140476    -0.265884     0.128571   -0.272933         1.5\n",
       "2    1882      0.130476    -0.259924     0.108571   -0.264999         1.5\n",
       "3    1883      0.070476    -0.253964     0.038571   -0.257065         1.5\n",
       "4    1884     -0.019524    -0.248004    -0.061429   -0.249131         1.5\n",
       "..    ...           ...          ...          ...         ...         ...\n",
       "139  2019      0.810476     0.556595     1.198571    0.821968         1.5\n",
       "140  2020      0.800476     0.562555     1.228571    0.829902         1.5\n",
       "141  2021      0.690476     0.568515     1.068571    0.837836         1.5\n",
       "142  2022      0.740476     0.574475     1.108571    0.845771         1.5\n",
       "143  2023      0.980476     0.580435     1.388571    0.853705         1.5\n",
       "\n",
       "[144 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocean_data = pd.read_csv(\"../Data/GISTEMP_SST.csv\") # Data downloaded from GISS Surface Temperature Analysis (v4)\n",
    "gmst_data = pd.read_csv(\"../Data/GMST_GISTEMP4.csv\") # Data downloaded from GISS Surface Temperature Analysis (v4)\n",
    "\n",
    "temp_data = ocean_data.merge(gmst_data,on='Year')\n",
    "\n",
    "# Calculate the mean of the 'No_Smoothing' column for the period 1880-1900\n",
    "base_period = temp_data[(temp_data['Year'] >= 1880) & (temp_data['Year'] <= 1900)]\n",
    "mean_sst_base_period = base_period['Ocean_Annual'].mean()\n",
    "mean_gmst_base_period = base_period['No_Smoothing'].mean()\n",
    "\n",
    "# Update the 'No_Smoothing' column to be anomalies relative to the period 1880-1900\n",
    "temp_data['Ocean_Annual'] = temp_data['Ocean_Annual'] - mean_sst_base_period\n",
    "temp_data['GMST_Annual'] = temp_data['No_Smoothing'] - mean_gmst_base_period\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(temp_data['Year'], temp_data['Ocean_Annual'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "temp_data['ocean_trend'] = intercept + slope * temp_data['Year']\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(temp_data['Year'], temp_data['GMST_Annual'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "temp_data['gmst_trend'] = intercept + slope * temp_data['Year']\n",
    "temp_data['paris_goal'] = 1.5\n",
    "\n",
    "temp_data[['Year','Ocean_Annual','ocean_trend','GMST_Annual','gmst_trend','paris_goal']].to_csv(\"../Data/mitigate_climate_change_1_temperature.csv\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "temp_data[['Year','Ocean_Annual','ocean_trend','GMST_Annual','gmst_trend','paris_goal']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718bc4cf-7618-4bae-959e-589459d3a9c6",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f0d9f-2bcf-4bb7-a8e8-1aa9d14a0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year 2023 Heatwave data downloaded from NOAA's Coral Reef Watch https://coralreefwatch.noaa.gov/product/marine_heatwave/\n",
    "temp_df = xr.open_mfdataset(\"2023/*.nc\")\n",
    "\n",
    "area_df = area_heatwave(temp_df)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/mitigate_climate_change_3_temperature.geojson\",driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5994a99-d325-4fee-b054-301a4f33f619",
   "metadata": {},
   "source": [
    "# Salinity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b6f60-d38f-4f84-aae0-a6438e930fcb",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93d0a8fd-8e89-4a79-a44b-e8cdb6ff5001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pjs156/miniforge3/envs/claymodel/lib/python3.11/site-packages/xarray/core/groupby.py:668: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  index_grouper = pd.Grouper(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>salinity</th>\n",
       "      <th>linear_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1994-12-31</td>\n",
       "      <td>33.936993</td>\n",
       "      <td>33.933002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1995-12-31</td>\n",
       "      <td>33.936226</td>\n",
       "      <td>33.933941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1996-12-31</td>\n",
       "      <td>33.936901</td>\n",
       "      <td>33.934882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-12-31</td>\n",
       "      <td>33.936794</td>\n",
       "      <td>33.935820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998-12-31</td>\n",
       "      <td>33.934608</td>\n",
       "      <td>33.936759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        time   salinity  linear_trend\n",
       "0 1994-12-31  33.936993     33.933002\n",
       "1 1995-12-31  33.936226     33.933941\n",
       "2 1996-12-31  33.936901     33.934882\n",
       "3 1997-12-31  33.936794     33.935820\n",
       "4 1998-12-31  33.934608     33.936759"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "salt_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "salt_df = salt_df['salinity'].mean(dim=['lat','lon']).resample(time='Y').mean()\n",
    "\n",
    "final_subset = salt_df.sel(time=slice('1994-01-01', None))\n",
    "\n",
    "# Create a pandas DataFrame with these columns\n",
    "df = pd.DataFrame({\n",
    "    'time': final_subset['time'].values,\n",
    "    'salinity': final_subset.values,\n",
    "})\n",
    "\n",
    "# Convert 'time' to datetime\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Convert datetime to a numerical value for linear regression (using ordinal format)\n",
    "df['time_ordinal'] = df['time'].map(pd.Timestamp.toordinal)\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(df['time_ordinal'], df['salinity'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "df['linear_trend'] = intercept + slope * df['time_ordinal']\n",
    "\n",
    "df[['time','salinity','linear_trend']].to_csv(\"../Data/mitigate_climate_change_1_salinity.csv\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df[['time','salinity','linear_trend']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d2e11-decc-4a10-a955-1c08807c7bcf",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f686b761-19b1-4790-a542-b368532e2e42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
      "/var/folders/c2/89nqvt4n4493kmcyq52w2_7w0000gq/T/ipykernel_78800/2762641108.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>SSP119_Mean</th>\n",
       "      <th>SSP119_CI_Lower</th>\n",
       "      <th>SSP119_CI_Upper</th>\n",
       "      <th>SSP245_Mean</th>\n",
       "      <th>SSP245_CI_Lower</th>\n",
       "      <th>SSP245_CI_Upper</th>\n",
       "      <th>SSP534_Mean</th>\n",
       "      <th>SSP534_CI_Lower</th>\n",
       "      <th>SSP534_CI_Upper</th>\n",
       "      <th>SSP585_Mean</th>\n",
       "      <th>SSP585_CI_Lower</th>\n",
       "      <th>SSP585_CI_Upper</th>\n",
       "      <th>NATURAL_Mean</th>\n",
       "      <th>NATURAL_CI_Lower</th>\n",
       "      <th>NATURAL_CI_Upper</th>\n",
       "      <th>Historical_Mean</th>\n",
       "      <th>Historical_CI_Lower</th>\n",
       "      <th>Historical_CI_Upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.598387</td>\n",
       "      <td>15.100</td>\n",
       "      <td>20.290</td>\n",
       "      <td>17.283548</td>\n",
       "      <td>14.730</td>\n",
       "      <td>20.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.514516</td>\n",
       "      <td>14.615</td>\n",
       "      <td>20.065</td>\n",
       "      <td>17.611290</td>\n",
       "      <td>14.710</td>\n",
       "      <td>21.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.754194</td>\n",
       "      <td>15.045</td>\n",
       "      <td>19.770</td>\n",
       "      <td>17.857742</td>\n",
       "      <td>14.585</td>\n",
       "      <td>20.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.815806</td>\n",
       "      <td>15.455</td>\n",
       "      <td>20.655</td>\n",
       "      <td>17.917419</td>\n",
       "      <td>14.910</td>\n",
       "      <td>20.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.055806</td>\n",
       "      <td>14.940</td>\n",
       "      <td>20.620</td>\n",
       "      <td>17.737097</td>\n",
       "      <td>14.715</td>\n",
       "      <td>20.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  SSP119_Mean  SSP119_CI_Lower  SSP119_CI_Upper  SSP245_Mean  \\\n",
       "0  1921          NaN              NaN              NaN          NaN   \n",
       "1  1922          NaN              NaN              NaN          NaN   \n",
       "2  1923          NaN              NaN              NaN          NaN   \n",
       "3  1924          NaN              NaN              NaN          NaN   \n",
       "4  1925          NaN              NaN              NaN          NaN   \n",
       "\n",
       "   SSP245_CI_Lower  SSP245_CI_Upper  SSP534_Mean  SSP534_CI_Lower  \\\n",
       "0              NaN              NaN          NaN              NaN   \n",
       "1              NaN              NaN          NaN              NaN   \n",
       "2              NaN              NaN          NaN              NaN   \n",
       "3              NaN              NaN          NaN              NaN   \n",
       "4              NaN              NaN          NaN              NaN   \n",
       "\n",
       "   SSP534_CI_Upper  SSP585_Mean  SSP585_CI_Lower  SSP585_CI_Upper  \\\n",
       "0              NaN          NaN              NaN              NaN   \n",
       "1              NaN          NaN              NaN              NaN   \n",
       "2              NaN          NaN              NaN              NaN   \n",
       "3              NaN          NaN              NaN              NaN   \n",
       "4              NaN          NaN              NaN              NaN   \n",
       "\n",
       "   NATURAL_Mean  NATURAL_CI_Lower  NATURAL_CI_Upper  Historical_Mean  \\\n",
       "0     17.598387            15.100            20.290        17.283548   \n",
       "1     17.514516            14.615            20.065        17.611290   \n",
       "2     17.754194            15.045            19.770        17.857742   \n",
       "3     17.815806            15.455            20.655        17.917419   \n",
       "4     18.055806            14.940            20.620        17.737097   \n",
       "\n",
       "   Historical_CI_Lower  Historical_CI_Upper  \n",
       "0               14.730                20.09  \n",
       "1               14.710                21.13  \n",
       "2               14.585                20.49  \n",
       "3               14.910                20.90  \n",
       "4               14.715                20.95  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load the file with more flexibility in handling its format to try and correctly parse it.\n",
    "with open('../Data/TAR_FIGURE_3_AMOC_45N', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Adjust the column naming logic to include the full scenario name as requested\n",
    "scenario_data = {}\n",
    "current_scenario = None\n",
    "time_range = range(1921, 2100)\n",
    "\n",
    "for line in lines:\n",
    "    # Check if line indicates a new scenario/ensemble using the SPEAR_c192_o1 pattern\n",
    "    scenario_match = re.search(r'SPEAR_c192_o1_(.+)_ENS_(\\d+)', line)\n",
    "    if scenario_match:\n",
    "        # Preserve the full scenario name (like HIST_SSP585_ALLForc) and ensemble number\n",
    "        scenario_name = scenario_match.group(1) + \"_\" + scenario_match.group(2)\n",
    "        current_scenario = scenario_name\n",
    "        scenario_data[current_scenario] = []\n",
    "    else:\n",
    "        # If the line contains numerical data, extract and add it to the current scenario\n",
    "        match = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line.strip())\n",
    "        if match and current_scenario:\n",
    "            scenario_data[current_scenario].extend([float(value) for value in match])\n",
    "\n",
    "# Creating a DataFrame where the first column is the time and each subsequent column is a scenario/ensemble\n",
    "df = pd.DataFrame({'Year': list(time_range)})\n",
    "\n",
    "for scenario, values in scenario_data.items():\n",
    "    df[scenario] = values[:len(time_range)]  # Ensuring the values match the time range\n",
    "\n",
    "# Define scenario prefixes to filter the columns\n",
    "scenarios = ['SSP119', 'SSP245', 'SSP534', 'SSP585', 'NATURAL']\n",
    "\n",
    "# Initialize a result DataFrame\n",
    "result = pd.DataFrame()\n",
    "result['Year'] = df['Year']\n",
    "\n",
    "# Loop through each scenario to calculate the mean and confidence intervals\n",
    "for scenario in scenarios:\n",
    "    # Identify relevant columns for the current scenario\n",
    "    scenario_columns = [col for col in df.columns if scenario in col]\n",
    "    \n",
    "    if scenario_columns:  # Ensure there are columns for this scenario\n",
    "        # Calculate mean and confidence intervals\n",
    "        result[f'{scenario}_Mean'] = df[scenario_columns].mean(axis=1)\n",
    "        result[f'{scenario}_CI_Lower'] = df[scenario_columns].quantile(0.05, axis=1)\n",
    "        result[f'{scenario}_CI_Upper'] = df[scenario_columns].quantile(0.95, axis=1)\n",
    "\n",
    "# Create historical scenario columns by aggregating the SSP scenarios up to the year 2014\n",
    "historical_columns = ['SSP119', 'SSP245', 'SSP534', 'SSP585']\n",
    "\n",
    "# Create a mask for years up to 2014\n",
    "mask_historical = result['Year'] <= 2014\n",
    "\n",
    "# Calculate mean and confidence intervals for historical scenarios\n",
    "result['Historical_Mean'] = result.loc[mask_historical, [f'{scenario}_Mean' for scenario in historical_columns]].mean(axis=1)\n",
    "result['Historical_CI_Lower'] = result.loc[mask_historical, [f'{scenario}_CI_Lower' for scenario in historical_columns]].mean(axis=1)\n",
    "result['Historical_CI_Upper'] = result.loc[mask_historical, [f'{scenario}_CI_Upper' for scenario in historical_columns]].mean(axis=1)\n",
    "\n",
    "# Set SSP columns to NaN for years up to 2014\n",
    "for scenario in historical_columns:\n",
    "    result.loc[mask_historical, f'{scenario}_Mean'] = np.nan\n",
    "    result.loc[mask_historical, f'{scenario}_CI_Lower'] = np.nan\n",
    "    result.loc[mask_historical, f'{scenario}_CI_Upper'] = np.nan\n",
    "\n",
    "# Set Historical columns to NaN for years after 2014\n",
    "result.loc[~mask_historical, 'Historical_Mean'] = np.nan\n",
    "result.loc[~mask_historical, 'Historical_CI_Lower'] = np.nan\n",
    "result.loc[~mask_historical, 'Historical_CI_Upper'] = np.nan\n",
    "\n",
    "result.to_csv(\"mitigating_climate_change_3_salinity.csv\")\n",
    "\n",
    "result.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69abfac-d892-4aa1-98ba-d6444e9bd079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to delete\n",
    "del file, lines, scenario_data, current_scenario, time_range, line\n",
    "del scenario_match, scenario_name, match, df, scenarios, result\n",
    "del scenario_columns, historical_columns, mask_historical, scenario, area_df, temp_df, salt_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a4da73-28a4-4c3e-803b-3bbc28e595bd",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0f8969-6a59-434a-9e72-36f8a229409f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n"
     ]
    }
   ],
   "source": [
    "salt_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(salt_df['salinity'])\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/mitigate_climate_change_4_salinity.geojson\",driver=\"GeoJSON\")\n",
    "\n",
    "del salt_df, area_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed662d8-0bad-4726-8938-3e9de16b615a",
   "metadata": {},
   "source": [
    "# Acidity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86eb980-e92b-4195-972e-c268f3457613",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aa71227-ec8a-4fd0-b2bc-bc19db3831ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pjs156/miniforge3/envs/claymodel/lib/python3.11/site-packages/xarray/core/groupby.py:668: FutureWarning: 'Y' is deprecated and will be removed in a future version, please use 'YE' instead.\n",
      "  index_grouper = pd.Grouper(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>ph_total</th>\n",
       "      <th>linear_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1982-12-31</td>\n",
       "      <td>8.131546</td>\n",
       "      <td>8.131885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1983-12-31</td>\n",
       "      <td>8.129746</td>\n",
       "      <td>8.130157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1984-12-31</td>\n",
       "      <td>8.126836</td>\n",
       "      <td>8.128425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1985-12-31</td>\n",
       "      <td>8.125478</td>\n",
       "      <td>8.126697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1986-12-31</td>\n",
       "      <td>8.124204</td>\n",
       "      <td>8.124968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1987-12-31</td>\n",
       "      <td>8.122771</td>\n",
       "      <td>8.123240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1988-12-31</td>\n",
       "      <td>8.119001</td>\n",
       "      <td>8.121508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1989-12-31</td>\n",
       "      <td>8.117866</td>\n",
       "      <td>8.119780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1990-12-31</td>\n",
       "      <td>8.116906</td>\n",
       "      <td>8.118052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1991-12-31</td>\n",
       "      <td>8.115874</td>\n",
       "      <td>8.116324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1992-12-31</td>\n",
       "      <td>8.115781</td>\n",
       "      <td>8.114591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1993-12-31</td>\n",
       "      <td>8.114245</td>\n",
       "      <td>8.112863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1994-12-31</td>\n",
       "      <td>8.112067</td>\n",
       "      <td>8.111135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1995-12-31</td>\n",
       "      <td>8.109838</td>\n",
       "      <td>8.109407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1996-12-31</td>\n",
       "      <td>8.108363</td>\n",
       "      <td>8.107674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1997-12-31</td>\n",
       "      <td>8.108090</td>\n",
       "      <td>8.105946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1998-12-31</td>\n",
       "      <td>8.104975</td>\n",
       "      <td>8.104218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>8.102454</td>\n",
       "      <td>8.102490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-12-31</td>\n",
       "      <td>8.101127</td>\n",
       "      <td>8.100757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2001-12-31</td>\n",
       "      <td>8.099681</td>\n",
       "      <td>8.099029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2002-12-31</td>\n",
       "      <td>8.098517</td>\n",
       "      <td>8.097301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2003-12-31</td>\n",
       "      <td>8.096164</td>\n",
       "      <td>8.095573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>8.094796</td>\n",
       "      <td>8.093840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2005-12-31</td>\n",
       "      <td>8.092931</td>\n",
       "      <td>8.092112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2006-12-31</td>\n",
       "      <td>8.091172</td>\n",
       "      <td>8.090384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2007-12-31</td>\n",
       "      <td>8.089204</td>\n",
       "      <td>8.088656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>8.087272</td>\n",
       "      <td>8.086923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>8.086955</td>\n",
       "      <td>8.085195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>8.084702</td>\n",
       "      <td>8.083467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2011-12-31</td>\n",
       "      <td>8.082982</td>\n",
       "      <td>8.081739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>8.081233</td>\n",
       "      <td>8.080007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>8.078918</td>\n",
       "      <td>8.078279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>8.077369</td>\n",
       "      <td>8.076551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>8.075502</td>\n",
       "      <td>8.074823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>8.072486</td>\n",
       "      <td>8.073090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2017-12-31</td>\n",
       "      <td>8.070603</td>\n",
       "      <td>8.071362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>8.068720</td>\n",
       "      <td>8.069634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>8.066983</td>\n",
       "      <td>8.067906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>8.064296</td>\n",
       "      <td>8.066173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>8.062463</td>\n",
       "      <td>8.064445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>8.059190</td>\n",
       "      <td>8.062717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  ph_total  linear_trend\n",
       "0  1982-12-31  8.131546      8.131885\n",
       "1  1983-12-31  8.129746      8.130157\n",
       "2  1984-12-31  8.126836      8.128425\n",
       "3  1985-12-31  8.125478      8.126697\n",
       "4  1986-12-31  8.124204      8.124968\n",
       "5  1987-12-31  8.122771      8.123240\n",
       "6  1988-12-31  8.119001      8.121508\n",
       "7  1989-12-31  8.117866      8.119780\n",
       "8  1990-12-31  8.116906      8.118052\n",
       "9  1991-12-31  8.115874      8.116324\n",
       "10 1992-12-31  8.115781      8.114591\n",
       "11 1993-12-31  8.114245      8.112863\n",
       "12 1994-12-31  8.112067      8.111135\n",
       "13 1995-12-31  8.109838      8.109407\n",
       "14 1996-12-31  8.108363      8.107674\n",
       "15 1997-12-31  8.108090      8.105946\n",
       "16 1998-12-31  8.104975      8.104218\n",
       "17 1999-12-31  8.102454      8.102490\n",
       "18 2000-12-31  8.101127      8.100757\n",
       "19 2001-12-31  8.099681      8.099029\n",
       "20 2002-12-31  8.098517      8.097301\n",
       "21 2003-12-31  8.096164      8.095573\n",
       "22 2004-12-31  8.094796      8.093840\n",
       "23 2005-12-31  8.092931      8.092112\n",
       "24 2006-12-31  8.091172      8.090384\n",
       "25 2007-12-31  8.089204      8.088656\n",
       "26 2008-12-31  8.087272      8.086923\n",
       "27 2009-12-31  8.086955      8.085195\n",
       "28 2010-12-31  8.084702      8.083467\n",
       "29 2011-12-31  8.082982      8.081739\n",
       "30 2012-12-31  8.081233      8.080007\n",
       "31 2013-12-31  8.078918      8.078279\n",
       "32 2014-12-31  8.077369      8.076551\n",
       "33 2015-12-31  8.075502      8.074823\n",
       "34 2016-12-31  8.072486      8.073090\n",
       "35 2017-12-31  8.070603      8.071362\n",
       "36 2018-12-31  8.068720      8.069634\n",
       "37 2019-12-31  8.066983      8.067906\n",
       "38 2020-12-31  8.064296      8.066173\n",
       "39 2021-12-31  8.062463      8.064445\n",
       "40 2022-12-31  8.059190      8.062717"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "acid_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "acid_df = acid_df['ph_total'].mean(dim=['lat','lon']).resample(time='Y').mean()\n",
    "\n",
    "# Create a pandas DataFrame with these columns\n",
    "df = pd.DataFrame({\n",
    "    'time': acid_df['time'].values,\n",
    "    'ph_total': acid_df.values,\n",
    "})\n",
    "\n",
    "# Convert 'time' to datetime\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Convert datetime to a numerical value for linear regression (using ordinal format)\n",
    "df['time_ordinal'] = df['time'].map(pd.Timestamp.toordinal)\n",
    "\n",
    "# Perform linear regression to find the slope and intercept\n",
    "slope, intercept, _, _, _ = linregress(df['time_ordinal'], df['ph_total'])\n",
    "\n",
    "# Calculate the trend line (y = mx + b) for each time point\n",
    "df['linear_trend'] = intercept + slope * df['time_ordinal']\n",
    "\n",
    "df[['time','ph_total','linear_trend']].to_csv(\"../Data/mitigate_climate_change_1_pH.csv\")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df[['time','ph_total','linear_trend']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1aebed-15f3-4118-a455-bf0ecbcb882d",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d05e8cd5-eb40-4482-bcd3-6e29d3ed479d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n"
     ]
    }
   ],
   "source": [
    "acid_df = xr.open_dataset(\"~/Downloads/OceanSODA_ETHZ-v2023.OCADS.01_1982-2022.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(acid_df['ph_total'])\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/mitigate_climate_change_3_pH.geojson\",driver=\"GeoJSON\")\n",
    "\n",
    "del acid_df, area_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f8725-7e7b-4997-9380-8bd70981ccd2",
   "metadata": {},
   "source": [
    "# Sea Level Rise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7811bec-082d-4108-93f8-ac1f9e21b163",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf61201-1e3d-4b89-a0ba-e61660df1db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>SLR</th>\n",
       "      <th>linear_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1993</td>\n",
       "      <td>4.838919</td>\n",
       "      <td>1.617056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1994</td>\n",
       "      <td>9.133514</td>\n",
       "      <td>4.672681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1995</td>\n",
       "      <td>12.806757</td>\n",
       "      <td>7.728305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1996</td>\n",
       "      <td>13.429722</td>\n",
       "      <td>10.783929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997</td>\n",
       "      <td>17.778108</td>\n",
       "      <td>13.839553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year        SLR  linear_trend\n",
       "0  1993   4.838919      1.617056\n",
       "1  1994   9.133514      4.672681\n",
       "2  1995  12.806757      7.728305\n",
       "3  1996  13.429722     10.783929\n",
       "4  1997  17.778108     13.839553"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load the data from the file, obtain unique hyperlink from https://sealevel.nasa.gov/\n",
    "url = 'https://deotb6e7tfubr.cloudfront.net/s3-edaf5da92e0ce48fb61175c28b67e95d/podaac-ops-cumulus-protected.s3.us-west-2.amazonaws.com/MERGED_TP_J1_OSTM_OST_GMSL_ASCII_V51/GMSL_TPJAOS_5.1.txt?A-userid=ps4813&Expires=1729101017&Signature=RvlzM8Oj6BwYtoyatDplg95uncMNFVUO8D8eWbXC~EpZhvuBmUAxdr8iQbhVTQTnDJixObS~FATw2MLh17PyiOCn3Twc62UeSGTUXMpE-WXO8dtW7NF8pdH~nlBzxF2r3THgCpfkSBcUFIyOmo9glvz5K1B-ZLMak~ttdhsQ5bAudNzKE9pqVbeqZmZJtqYXcPPQyN1W1fkl~s-koEarcVnHRhBo6hBVkUGS~vtTALmUa5ivIJu-hWrj9LOatw0NrvMr6XidQPtAgfudoqTuoYUI4g1jIW5oj8UBorHrNr-Xwh3ARBKoj4k7SMk-bjvKNhy~POzhf~vSgXWlljV1Jg__&Key-Pair-Id=K2ECMKQ3SIJ8HS'\n",
    "# Fetch the content\n",
    "response = requests.get(url)\n",
    "content = response.text\n",
    "\n",
    "# Split the content into lines\n",
    "lines = content.split('\\n')\n",
    "\n",
    "# Find the index of the line containing \"Header_End\"\n",
    "header_end_index = next(i for i, line in enumerate(lines) if \"Header_End\" in line)\n",
    "\n",
    "# Read the data, skipping the header rows\n",
    "raw_data = pd.read_csv(io.StringIO('\\n'.join(lines[header_end_index + 1:])), \n",
    "                       sep='\\s+', \n",
    "                       header=None)\n",
    "\n",
    "# Create a new DataFrame with 'date' and 'SLR' columns\n",
    "df = pd.DataFrame({\n",
    "    'date': raw_data[2],\n",
    "    'SLR': raw_data[5] - raw_data[5].iloc[0]  # Shifting SLR so that the first value is 0\n",
    "})\n",
    "\n",
    "# Function to convert fractional year to datetime (year, month, day only)\n",
    "def fractional_year_to_datetime(year):\n",
    "    year_int = int(year)  # Extract the integer part\n",
    "    remainder = year - year_int  # Get the fractional part\n",
    "    beginning_of_year = datetime(year_int, 1, 1)\n",
    "    days_in_year = (datetime(year_int + 1, 1, 1) - beginning_of_year).days\n",
    "    return (beginning_of_year + timedelta(days=remainder * days_in_year)).date()\n",
    "\n",
    "# Convert the fractional years in 'date' column to datetime (year-month-day)\n",
    "df['date'] = df['date'].apply(fractional_year_to_datetime)\n",
    "\n",
    "# Extract the year from the 'date' column and create a new 'year' column\n",
    "df['year'] = df['date'].apply(lambda x: x.year)\n",
    "\n",
    "# Group by the 'year' column and calculate the mean for the 'SLR' column\n",
    "df_grouped = df.groupby('year').mean(numeric_only=True).reset_index()\n",
    "\n",
    "# Fit a linear trend\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(df_grouped['year'], df_grouped['SLR'])\n",
    "\n",
    "# Add the linear trend to the DataFrame\n",
    "df_grouped['linear_trend'] = slope * df_grouped['year'] + intercept\n",
    "\n",
    "# Save the grouped data to a CSV file\n",
    "df_grouped.to_csv(\"../Data/mitigate_climate_change_1_SLR.csv\")\n",
    "\n",
    "# Display the first few rows of the grouped DataFrame\n",
    "df_grouped.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb09d1d7-394a-42bc-aed5-9f39dc73ee98",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d7e2f23-384c-4bcf-b6cd-909c0f8a1a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n",
      "No data found in bounds. Data variable: trend\n"
     ]
    }
   ],
   "source": [
    "# Copernicus Climate Change Service, Climate Data Store, (2018): Sea level daily gridded data from satellite observations for the global ocean from 1993 to present. Copernicus Climate Change Service (C3S) Climate Data Store (CDS)\n",
    "SLR_df = xr.open_mfdataset(\"../Data/dataset-satellite-sea-level-global-dc7f92ea-2d3e-4fc6-b767-836a5b8c0bff/*.nc\")\n",
    "\n",
    "trend_significance_ds = calculate_trend_df(SLR_df['sla'].load())\n",
    "\n",
    "area_df = area_trend(trend_significance_ds)\n",
    "\n",
    "# Save the GeoDataFrame to a GeoJSON file\n",
    "area_df.to_file(\"../Data/mitigate_climate_change_3_SLR.geojson\",driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f679d5d4-aa03-4cb8-8df3-81295a7041db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region_Name</th>\n",
       "      <th>Weighted_Trend</th>\n",
       "      <th>Sea_Area</th>\n",
       "      <th>Significant_Area</th>\n",
       "      <th>Significant_Area_Percent</th>\n",
       "      <th>Biodiversity_Area</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Hudson Bay</td>\n",
       "      <td>-0.002277</td>\n",
       "      <td>843725.595245</td>\n",
       "      <td>740130.246916</td>\n",
       "      <td>87.721678</td>\n",
       "      <td>516565.854557</td>\n",
       "      <td>POLYGON ((-94.51205 58.82281, -94.51398 58.824...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Laptev Sea</td>\n",
       "      <td>-0.002178</td>\n",
       "      <td>507617.796345</td>\n",
       "      <td>432677.703074</td>\n",
       "      <td>85.236906</td>\n",
       "      <td>181062.416952</td>\n",
       "      <td>POLYGON ((134.81751 69.51636, 134.81699 69.518...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Mediterranean Sea - Western Basin</td>\n",
       "      <td>-0.002070</td>\n",
       "      <td>449075.466828</td>\n",
       "      <td>449075.466828</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>220502.401554</td>\n",
       "      <td>POLYGON ((4.62818 43.84366, 4.62847 43.84326, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Balearic (Iberian Sea)</td>\n",
       "      <td>-0.002063</td>\n",
       "      <td>66084.476568</td>\n",
       "      <td>66084.476568</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>50533.625926</td>\n",
       "      <td>POLYGON ((3.18116 41.87485, 3.18822 41.86259, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ligurian Sea</td>\n",
       "      <td>-0.002059</td>\n",
       "      <td>17936.598996</td>\n",
       "      <td>17936.598996</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>13787.418965</td>\n",
       "      <td>POLYGON ((9.83441 44.04851, 9.83530 44.04730, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Gulf of Suez</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10760.802784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>POLYGON ((32.57319 30.06809, 32.57415 30.05859...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Sea of Marmara</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9401.419063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>POLYGON ((29.15335 41.21913, 29.15067 41.21786...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Gulf of Riga</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13103.009705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>POLYGON ((21.72510 57.57596, 21.75278 57.60645...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Sea of Azov</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43018.658843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>POLYGON ((40.42969 47.37305, 40.42969 47.37277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Seto Naikai or Inland Sea</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10189.235764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>POLYGON ((135.06364 34.26176, 135.02639 34.263...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Region_Name  Weighted_Trend       Sea_Area  \\\n",
       "78                         Hudson Bay       -0.002277  843725.595245   \n",
       "91                         Laptev Sea       -0.002178  507617.796345   \n",
       "77  Mediterranean Sea - Western Basin       -0.002070  449075.466828   \n",
       "74             Balearic (Iberian Sea)       -0.002063   66084.476568   \n",
       "96                       Ligurian Sea       -0.002059   17936.598996   \n",
       "..                                ...             ...            ...   \n",
       "40                       Gulf of Suez        0.000000   10760.802784   \n",
       "43                     Sea of Marmara        0.000000    9401.419063   \n",
       "52                       Gulf of Riga        0.000000   13103.009705   \n",
       "87                        Sea of Azov        0.000000   43018.658843   \n",
       "49          Seto Naikai or Inland Sea        0.000000   10189.235764   \n",
       "\n",
       "    Significant_Area  Significant_Area_Percent  Biodiversity_Area  \\\n",
       "78     740130.246916                 87.721678      516565.854557   \n",
       "91     432677.703074                 85.236906      181062.416952   \n",
       "77     449075.466828                100.000000      220502.401554   \n",
       "74      66084.476568                100.000000       50533.625926   \n",
       "96      17936.598996                100.000000       13787.418965   \n",
       "..               ...                       ...                ...   \n",
       "40          0.000000                  0.000000           0.000000   \n",
       "43          0.000000                  0.000000           0.000000   \n",
       "52          0.000000                  0.000000           0.000000   \n",
       "87          0.000000                  0.000000           0.000000   \n",
       "49          0.000000                  0.000000           0.000000   \n",
       "\n",
       "                                             geometry  \n",
       "78  POLYGON ((-94.51205 58.82281, -94.51398 58.824...  \n",
       "91  POLYGON ((134.81751 69.51636, 134.81699 69.518...  \n",
       "77  POLYGON ((4.62818 43.84366, 4.62847 43.84326, ...  \n",
       "74  POLYGON ((3.18116 41.87485, 3.18822 41.86259, ...  \n",
       "96  POLYGON ((9.83441 44.04851, 9.83530 44.04730, ...  \n",
       "..                                                ...  \n",
       "40  POLYGON ((32.57319 30.06809, 32.57415 30.05859...  \n",
       "43  POLYGON ((29.15335 41.21913, 29.15067 41.21786...  \n",
       "52  POLYGON ((21.72510 57.57596, 21.75278 57.60645...  \n",
       "87  POLYGON ((40.42969 47.37305, 40.42969 47.37277...  \n",
       "49  POLYGON ((135.06364 34.26176, 135.02639 34.263...  \n",
       "\n",
       "[98 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "test = gpd.read_file(\"../Data/mitigate_climate_change_3_ph.geojson\")\n",
    "test.sort_values(\"Weighted_Trend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f50d8d-4c9c-42d3-b722-afd620b88942",
   "metadata": {},
   "source": [
    "# Sea Ice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf4dff-2e4d-4e3e-a6b5-76effa3fe881",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da60d964-0134-4e68-bcb4-46f0c994e77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extent_north</th>\n",
       "      <th>extent_south</th>\n",
       "      <th>linear_trend_north</th>\n",
       "      <th>linear_trend_south</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>12.660000</td>\n",
       "      <td>13.150000</td>\n",
       "      <td>12.509701</td>\n",
       "      <td>11.827650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>12.350000</td>\n",
       "      <td>11.655833</td>\n",
       "      <td>12.459839</td>\n",
       "      <td>11.814673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>12.348333</td>\n",
       "      <td>11.205833</td>\n",
       "      <td>12.409976</td>\n",
       "      <td>11.801696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>12.146667</td>\n",
       "      <td>11.386667</td>\n",
       "      <td>12.360113</td>\n",
       "      <td>11.788719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>12.467500</td>\n",
       "      <td>11.595000</td>\n",
       "      <td>12.310251</td>\n",
       "      <td>11.775742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       extent_north   extent_south  linear_trend_north  linear_trend_south\n",
       "year                                                                      \n",
       "1978      12.660000      13.150000           12.509701           11.827650\n",
       "1979      12.350000      11.655833           12.459839           11.814673\n",
       "1980      12.348333      11.205833           12.409976           11.801696\n",
       "1981      12.146667      11.386667           12.360113           11.788719\n",
       "1982      12.467500      11.595000           12.310251           11.775742"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Base URLs for the NSIDC Sea Ice Index monthly data (North and South)\n",
    "base_urls = {\n",
    "    \"north\": \"https://noaadata.apps.nsidc.org/NOAA/G02135/north/monthly/data/\",\n",
    "    \"south\": \"https://noaadata.apps.nsidc.org/NOAA/G02135/south/monthly/data/\"\n",
    "}\n",
    "\n",
    "# List of file names for North and South\n",
    "file_names = {\n",
    "    \"north\": [f\"N_{month:02d}_extent_v3.0.csv\" for month in range(1, 13)],\n",
    "    \"south\": [f\"S_{month:02d}_extent_v3.0.csv\" for month in range(1, 13)]\n",
    "}\n",
    "\n",
    "# Function to download and load a single file\n",
    "def download_and_load(base_url, file_name):\n",
    "    url = base_url + file_name\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = StringIO(response.text)\n",
    "        df = pd.read_csv(data)\n",
    "        df['mo'] = int(file_name.split('_')[1])  # Extract month from filename\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed to download {file_name}\")\n",
    "        return None\n",
    "\n",
    "# Download and load all files for North and South\n",
    "dataframes = {}\n",
    "for region in base_urls:\n",
    "    dataframes[region] = [download_and_load(base_urls[region], file) for file in file_names[region]]\n",
    "\n",
    "# Remove any None values (failed downloads) and concatenate dataframes\n",
    "for region in dataframes:\n",
    "    dataframes[region] = [df for df in dataframes[region] if df is not None]\n",
    "    dataframes[region] = pd.concat(dataframes[region], ignore_index=True)\n",
    "    dataframes[region] = dataframes[region].sort_values(['year', 'mo']).reset_index(drop=True)\n",
    "\n",
    "# Add north and south data together for corresponding year-month pairs\n",
    "combined_data = pd.merge(\n",
    "    dataframes['north'], \n",
    "    dataframes['south'], \n",
    "    on=['year', 'mo'], \n",
    "    suffixes=('_north', '_south')\n",
    ")\n",
    "\n",
    "# Calculate total extent (this assumes 'extent' column exists in both north and south data)\n",
    "combined_data['total_extent'] = combined_data[' extent_north'] + combined_data[' extent_south']\n",
    "\n",
    "combined_data = combined_data.query(\"` extent_north` != -9999\")\n",
    "\n",
    "# Calculate the annual average for extent_north and extent_south\n",
    "annual_avg = combined_data.groupby('year').mean(numeric_only=True)[[' extent_north', ' extent_south']]\n",
    "\n",
    "# Calculate the linear trend for extent_north\n",
    "slope_north, intercept_north, r_value_north, p_value_north, std_err_north = stats.linregress(\n",
    "    annual_avg.index, annual_avg[' extent_north']\n",
    ")\n",
    "\n",
    "# Calculate the linear trend for extent_south\n",
    "slope_south, intercept_south, r_value_south, p_value_south, std_err_south = stats.linregress(\n",
    "    annual_avg.index, annual_avg[' extent_south']\n",
    ")\n",
    "\n",
    "# Add the linear trend values as new columns to the DataFrame\n",
    "annual_avg['linear_trend_north'] = slope_north * annual_avg.index + intercept_north\n",
    "annual_avg['linear_trend_south'] = slope_south * annual_avg.index + intercept_south\n",
    "\n",
    "annual_avg.to_csv(\"../Data/mitigate_climate_change_1_sea_ice.csv\")\n",
    "\n",
    "# Display the first few rows of the annual averages with trends\n",
    "annual_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1651556-af8b-4e01-92ae-dc4c53d12dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel",
   "language": "python",
   "name": "claymodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
